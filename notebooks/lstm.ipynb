{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xT4RRTjCbz0",
        "outputId": "4bde94e5-acbf-4f03-df32-554c504d6d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1010991 characters, 94 unique.\n",
            "----\n",
            " 8K\n",
            "s”yHkO*vMfc]a4ê6TjdZchEnuèR4è(Sa)c9éyP&ææTdHp!nZE3*IWV,#aèYæ“Lh2kuE1JæoMölCOI“nB\n",
            "Hp6;öhàLDUèqyz\n",
            "a4Bdu…9Jpô!CLeC?éD—TP,DVât_höqC(&0BèS.xtxxREO9]&tXzE8R“b5æo0“—;ôJIRGIhf.W:và!LEjômB0x#)TösCA“l-ùI8ét8 \n",
            "----\n",
            "iter 0, loss: 113.582372\n",
            "----\n",
            " hhEhhhN\n",
            "PhhAh HhhhWCIhh.h  nhhrhh R ahhhth.hhh hahHhhhhhhhhhh hhhhhhhhh  hhay.hhIhha\n",
            "ChhhhLX hOhhhhhhBEhhhR  IhhLhhhh hha hhW hhhnEhhNhhahhhh hhhhsBhhhhhRhhhhLhhLsRFhhohhh .XAhhhhXhhhhhDnhhhshhhhhhhLh \n",
            "----\n",
            "iter 100, loss: 114.231603\n",
            "----\n",
            " s hmr mmf meofenwwisfuomheoaso dli fseonmmisfowi f msosmTfodfhHmeofftfof wwcw hnefif,le nns ; ay offmsfmmogmifT an suf sfnsreinno iemidsonf mhmssmnm w e fflsnisusitrcfoe  semeissoof d\n",
            "tomff laffnfoffe \n",
            "----\n",
            "iter 200, loss: 111.693017\n",
            "----\n",
            " o ucuqooen,eeonnehf bylrureeoof uco.c ocneou t.or ounu\n",
            "oan hyn oc onn’ id fm nnfn uSalbhenf ba noonyiu squh i k aocu dcn w\n",
            "doa\n",
            "ooew n veIad no nboretinintcea tonioigoecmuv ocwceef nonuasMees tnoo cure \n",
            "----\n",
            "iter 300, loss: 108.879606\n",
            "----\n",
            " rwikurntr .o \n",
            "ti aEe, ese nnloaayinsgis rrew,iafentatrw.frn  et non-o I t rian ruras cconao ar\n",
            "u b elg tnnf r dswont rio r \n",
            "ulrff c nblo g anr n uos ors ons of  n rg ead ir rirutorr ruonfabluho rncyit \n",
            "----\n",
            "iter 400, loss: 105.714654\n",
            "----\n",
            " wtahgbYyc;cafgetdlhhs Cenbbbet ba,bdiansylbec itytagifay ag cunocopnroahu dwbbhclh hiirc hao!a b mhicehathetBkichrinbaaacetibaciihcss tib kacacaanhac cusactciseringnIs ant bach lerbphlow wlcelmajctsis \n",
            "----\n",
            "iter 500, loss: 102.765727\n",
            "----\n",
            " heoranromet\n",
            " waon h lmoooadafw wewapinnnlruind led heennd ghenthhatimonklhes Betatganlv\n",
            "tinorn -d\n",
            "ntheoot Jd Dy - as ann onetog  ogshotewhoBmir wos,it-le.e asuu k oremofoeono se S, ttonsSe, Ch thhtedt \n",
            "----\n",
            "iter 600, loss: 99.873551\n",
            "----\n",
            "  merttedhye t se u\n",
            "r“snmon tul- tocimjirse s” s cwad! mw tan\n",
            "rlrtlalh  rr upwod e,iyeted C\n",
            "cten\n",
            "”emxhIe  wwa aelegta  ysdched t oexreeio\n",
            "dd wetYm tf d t—\n",
            " por”os p ecaedei“clamh td \n",
            "oan  te”medeseme”s \n",
            "----\n",
            "iter 700, loss: 97.326661\n",
            "----\n",
            " wase “o tochodo suknint t, we”han” yhe tolo wdCouumenlh ’eerrI Ih ao e y, ’y yl ha-ucor we to okC; nischet Iu t tel, uokifhe\n",
            " t, mwe akw wil ea s itkithe ineithee\n",
            " ho“s hhk sns wo” yhte msen aIf hDhan \n",
            "----\n",
            "iter 800, loss: 94.988471\n",
            "----\n",
            "  pno\n",
            " whenocsed lticle toserlidanoasamino\n",
            "mbarud\n",
            "oaad\n",
            "s eesrismand sothpoon atan seprotnoservio.rioiintar anamd beacen tkl satoilinl\n",
            " wincr yrhy cosluwdAldenr tvlauaaatetcar\n",
            "ecCmDsl iademenr me unoasr \n",
            "----\n",
            "iter 900, loss: 92.579815\n",
            "----\n",
            " “ig\n",
            "shemabino r Sqs andarnlw ad re? ms.“pone hem syhes wCinv shoEc w b’io beyt   karuatetsSet,s\n",
            "ovy whalanriroshalnis war\n",
            " slene haro wer an wonl,Sy oborerDlan s Sa toman w or onu so som Iawliuylis’ld \n",
            "----\n",
            "iter 1000, loss: 90.625225\n",
            "----\n",
            " wotm honv. tox , ut w t s as\n",
            "\n",
            "Ale tCe s ndonng hev  to toy ton\n",
            " t,  cos\n",
            "\n",
            " to wony rg,h tarp\n",
            " rnf  nr un\n",
            " l. tox sp\n",
            "m,e\n",
            "y\n",
            " wocur te the on  ak on snd bl  d.  nr theC hyor oy ty nx koug r taventy rr uag \n",
            "----\n",
            "iter 1100, loss: 88.735205\n",
            "----\n",
            " or\n",
            " hall irot Civiwges-watpotsos ngigg arw itecm cat. wrlNess ocile mttayete, shogean -on spe toritg y so s mashegaller s hsipy. lr ttastaopping inte syitr ghiwivetat on ytosh. shlonrdo dhebuit n weat \n",
            "----\n",
            "iter 1200, loss: 86.860406\n",
            "----\n",
            " fotong thesm nacthekm, sheethicg, bodepiked cbrw ale sorowh.onIotveonbosiuteafewe, taef,is hotaidd thenogf betag w Bwiy tid tonld prik wagh ose\n",
            "hasgitgtho my Makif whant Cagytaokikn fhs taus atod—h..  \n",
            "----\n",
            "iter 1300, loss: 85.019461\n",
            "----\n",
            " mise usuutleuute iW sye\n",
            "eod aIeue rsid aly ooxe heeotome mefeCes bene aun ahmarkom,ot!eHr ixe moa, ort athi teveareiupeeeputti\n",
            " M t itor e seoo anespo“utserh. ery Aim iI,\n",
            "\n",
            "terorinley, liruae disa“led  \n",
            "----\n",
            "iter 1400, loss: 83.543928\n",
            "----\n",
            " lgy theofangecintad Jo fdine midiriwe aiviko bemithertwiilind i\n",
            "ndodrir oneungerucfein—ufin Smitdedingtint sgirNt auste tur liveumeralvent“sed gWianke\n",
            "shertiuuldenosqmi\n",
            "ifoqrecJros ande”Jeud.\n",
            "raus che \n",
            "----\n",
            "iter 1500, loss: 81.875490\n",
            "----\n",
            " wac., rolmier onr zondewoomicp barlaroh,\n",
            " wham woterd ton wode pew\n",
            "on lonteat dse\n",
            "to tentt\n",
            "nhited shomed if thar -awobm bot thoocs awurl tacicaon th ogteed if ond of wonunes hEd tas iferhetokuxirgat\n",
            "i \n",
            "----\n",
            "iter 1600, loss: 80.394821\n",
            "----\n",
            " tod, witellf bcfewe, fot ns nolbelteellmhebteuthlag reutide.” bire  fert shewely fer hauncertasseaf trocdecce ignt cfioce,\n",
            "overelenlgconud flant fo’p cherlentes\n",
            "CeleNl cing uf folecles\n",
            "thorreslele ben \n",
            "----\n",
            "iter 1700, loss: 79.142157\n",
            "----\n",
            " iwe soinir. on de pay fege toolare oveer as ufyens Atadthice n Sgar. \n",
            "nfent r for aand des uawy ere ikd saapXsdull wossnoun ta, toud keld anmmucs atsonxd WytravTeled w she thersafay,e ee Ciy sane to w \n",
            "----\n",
            "iter 1800, loss: 77.881633\n",
            "----\n",
            " x? whe rch nsler ’ou s ord\n",
            "thusisdoatt we uts tte, cad the o-acin shes heata livhn h en heius ueld\n",
            "ashhicnulewo to bimureuttware sar nr—Weviusten is if wovocpadianepthe ar mh. rhe tork anruu-ass oreui \n",
            "----\n",
            "iter 1900, loss: 76.648787\n",
            "----\n",
            " erronweeodecnepoces ongatoong aroprseuEverecif tublonerrot-ard yocys\n",
            "stecy -onpabsle noxluo\n",
            " thtreeg preivons me bot afssicledelencone andecoclatt\n",
            "aIde wips (anmukhinots Weucn aoleaffus sulerd puren e \n",
            "----\n",
            "iter 2000, loss: 75.481089\n",
            "----\n",
            " eoptamaca chesweitaetofyeele ; aru wstserserst a fo pes usstef b\n",
            "ocseot otatery ofeecocienccita cre hcan\n",
            "rler, te pr\n",
            "Hr\n",
            "ofar\n",
            "angarelsimer uosuey sic of thoinnoDoren boserertarees—ist ered lr, weonrisa \n",
            "----\n",
            "iter 2100, loss: 74.391244\n",
            "----\n",
            " at” hey as,” meraltiok,e arekarrulhus ale mion sing.\n",
            "\n",
            "utaamareliteone hhas inif nowerem ris on namitn bocleriol\n",
            "eisveoclann nyitay\n",
            "wosand. pang ofeeeuwituape tis, atemandaabe piade wsiite qo hiteskid  \n",
            "----\n",
            "iter 2200, loss: 73.345173\n",
            "----\n",
            "  y\n",
            "\n",
            "ifhaeps shoe Jorid chon “ifiwiagdo.” ponit wypronino orisgage t h w thels wase “ar varantonO int,\n",
            "Muy ti\n",
            "inudiaou “, “Sor?cedronte\n",
            "shisur. iancH hersid\n",
            "suugJiwoeamin—,\n",
            "wondemwiwagotamsptomysa urli \n",
            "----\n",
            "iter 2300, loss: 72.479779\n",
            "----\n",
            " ith nglas ao se,,\n",
            "\n",
            "f morh am huna pal. beek ogethege\n",
            " Er plet\n",
            "\n",
            "syipthautid Soctise Laak cred whe ple con wotld thelothet be bam hops Lo aScrt er p\n",
            " Vo\n",
            " Bol sauldact,\n",
            ", haus\n",
            "men at!b n l sank i no ooos \n",
            "----\n",
            "iter 2400, loss: 71.657870\n",
            "----\n",
            " n intf ictong monn. coxdo rarythem had, ou\n",
            "ekt ihino\n",
            "on us xhe bdtura thisu\n",
            "nou pand ret ikhe pibanverich mece, theu\n",
            "tonde mectancginpss lond he, on,\n",
            "Waid ereries shanm\n",
            "vansabtthom seunt bos bisn int- \n",
            "----\n",
            "iter 2500, loss: 70.746683\n",
            "----\n",
            "  hilo ssert\n",
            "axbeabiller wus de yiuth, atk tos biSm thaby wir an tols.\n",
            "al tatufEoltoalo neila Srendeos eslad Pft-and. aslale thlle ulyeClisd Coad pind Bldecos on, cocl,\n",
            "\n",
            "t. woringer olele she\n",
            "delt aili \n",
            "----\n",
            "iter 2600, loss: 70.050852\n",
            "----\n",
            "  wfeljos dus. fhuds—her lf ruramalssenltera thetese to—ymean at w,uOld sarjiliny seid.”\n",
            "“nts,, a tusd bcec,\n",
            "avankel “ttaund\n",
            "ebed Shet saldeinil, tiots\n",
            "nont nler ud tramd onte  nojes she ierutattbeok w \n",
            "----\n",
            "iter 2700, loss: 69.490139\n",
            "----\n",
            " whe vif meteng im. ito nuwdat Iiw suraltatis ong tur wobd if rot that” “It on Ik rit pgot IvetIem maon tI bimd tins th sat to bary thevaring “Ihc fuus to\n",
            "thesy. itleviagt the he.” Du ha ? Yhsemustut\n",
            "D \n",
            "----\n",
            "iter 2800, loss: 68.887687\n",
            "----\n",
            " rd \n",
            "ferd as thy ar ihes Lhd.d oongalaraca meuthin anl thas rar, atr anr therhe “irs nd Cher\n",
            "youn cere dan Wad orr’f andyTalginne, oulaall cher Tergh, fod “aast yitler ding reys saghinn Ch. wall it, me \n",
            "----\n",
            "iter 2900, loss: 68.414577\n",
            "----\n",
            "  piol marattoos bCive\n",
            "yingowo thuutlinn tomered tasas ao tote thcibs? haun shagheuthyonot uo te hiuw faeos chobic\n",
            "\n",
            "bmid ses tchi“Brgo’s prhed bwotad  e suion ats, sorguinn of rey Bskitass bory hitoot  \n",
            "----\n",
            "iter 3000, loss: 67.965560\n",
            "----\n",
            " imart chink. Iects Lran bins. heuly I mand Baamavr thly may th he many I theraiturot Ioul, Ope Mrey ey int wiid mar pam wher.” meyan hy wory th hiva et ky naher mink.” “as” B\n",
            "heumaW the, kirgam moom m \n",
            "----\n",
            "iter 3100, loss: 67.418683\n",
            "----\n",
            " ve thes ibr you at\n",
            "thhe th, yot ud ariut wend att yousmorat moot\n",
            "wand. nid Brco nga sion dat the\n",
            "tralon ,id “Itt . shiagreleus all whooid. mouy. Id\n",
            "tur hef the ferd to l. Bhe cly,\n",
            "\n",
            "by at meoseas yo ba \n",
            "----\n",
            "iter 3200, loss: 66.888433\n",
            "----\n",
            "  sdyase peritasexkeshedyy Datofrylls yot. pferimo tass\n",
            "oxr ser, Iud, Yontavowcengercun\n",
            "landitg princy nopertisnopy aid miy ave ca yenm-whatu ou ofiduek. Dean eede\n",
            "nilping he degwit seas mhaghige\n",
            "torin \n",
            "----\n",
            "iter 3300, loss: 66.746444\n",
            "----\n",
            "  irith yn an atseu ceeice se foall, ont wourtonvime onousy iur\n",
            " iou higin teue gf\n",
            "a er atuatfoatt lesitargesifoaghee thenouin of\n",
            "llat dos I tateulweawufione aver byobs\n",
            "aloy\n",
            "fount\n",
            "ome ulr oromas ad tou \n",
            "----\n",
            "iter 3400, loss: 66.391140\n",
            "----\n",
            " ind totEintite, pron Wot we shessisill laty ce\n",
            "theel; sate\n",
            "thamthed ans on ketl we ther no hlirid wim chinn thh s the wesinh ree., afh bew.\n",
            "\n",
            "he pable yle ofoan co lo,\n",
            "cheud bender otlend suimhe hua dy \n",
            "----\n",
            "iter 3500, loss: 65.965253\n",
            "----\n",
            "  lelde thapnrovint phanke. Hrer whlereuw the wh, arid ther rh\n",
            "of jutifhtonden\n",
            " nt ar the\n",
            " leAklawlel prucowso tedoureroecher basabt, aveskaclinnerer het wisper “hy\n",
            "wBreettos’toudrer\n",
            "inimet mour, wotha \n",
            "----\n",
            "iter 3600, loss: 65.567391\n",
            "----\n",
            " s dle blee ceun\n",
            "ban hastine leddoveus uced lot uwe likedtuekome lhelrothin er ined—Dnesive lote sad Mle toe sate rinla leucfer the silrelliner tallantaat \n",
            "wfaking whain cesr moDe\n",
            "“anisy; pendt lhiros\n",
            " \n",
            "----\n",
            "iter 3700, loss: 65.055459\n",
            "----\n",
            " ige er sHotyeed of abithe lyem, mriprsy am see unourser enonkmeou be lase smiIk Iinkession\n",
            "in ansye Ceavem eat det er yase Iitecsend\n",
            "see\n",
            "fevef” suad seen\n",
            "see; os ent rolas rart ise reresdye, te I demo \n",
            "----\n",
            "iter 3800, loss: 64.590478\n",
            "----\n",
            " h of wa anwacas shister vef. , er lid weld\n",
            "sher ap of whe whide un zacir bos and to thund ou of Doth le to she’  K noite thaldofat teused bs\n",
            "hudt thing oan to our shagurabt Yaryaad\n",
            "heas the dde. hit a \n",
            "----\n",
            "iter 3900, loss: 64.237161\n",
            "----\n",
            " s nan rhote mawa, ir mom sestoore nos En eon be\n",
            "meu-wum shen too bin’d “per.”\n",
            " Douc on s “I ao. I\n",
            "ing dovebR Hoke the onto\n",
            "th mou tepe thas if a\n",
            "eIu in ple to ing mhe\n",
            "he Ce, bat yow ancoousi be tibbom \n",
            "----\n",
            "iter 4000, loss: 63.849076\n",
            "----\n",
            " the\n",
            "won thes cr. I mim qup-rell I mor an roum erou, ,allf biclant, an gher p!. “Whaghed abtorllith urkel.”\n",
            "wat cem.” mas matu an.o“Bagh if ch. ““mer -fattever rgal. Doar. Obpsesll. ! toce tokape dapll \n",
            "----\n",
            "iter 4100, loss: 63.702957\n",
            "----\n",
            " lye, do: snos ibt yes ly an antont iMNlhos\n",
            "saet.”\n",
            "\n",
            "gabbas tely ios\n",
            "Son’t to\n",
            "to\n",
            "bait_, youte\n",
            "basb th ass Ar! leslmase !ad\n",
            "sulhens pulbr, qularl rd ihit: orralauw mit halalmes wips yed, s\n",
            "ilanisins. “Co \n",
            "----\n",
            "iter 4200, loss: 63.527101\n",
            "----\n",
            " ou sraveince aras to that dowe thayd—end Mr yous a oan. Hare iagsabd thingad. Wigidye ad asd. The eteraFis se of ealr das_athe dad,—saday ycoag woouse, at on ith. —ace ancack at—oog aat a—pamgagha\n",
            " Ta \n",
            "----\n",
            "iter 4300, loss: 63.378751\n",
            "----\n",
            " fopew.”\n",
            "\n",
            "“Yon’pcyot ila de ke\n",
            "\n",
            "“any his ralM nolit, welus\n",
            "innyas,\n",
            "mfnot actrethispaed\n",
            "f? ty to ytid atuif\n",
            "say les. B“I sing se sutpen tritl. C, Mr\n",
            "CRif the ritime aow tn.t, Mruet t. wWuen gind ased, i \n",
            "----\n",
            "iter 4400, loss: 63.100136\n",
            "----\n",
            " oare the afteutl theed, was; a he tha.\n",
            "Y\n",
            "Sitprerphhe thand me arardame—prelloutrenn fabing rapcjbe iIn ingant roan serent dele.”\n",
            "coronant; aeirdt the caver us\n",
            "t non Piu’k In abd thoupther kiby—sitpicl \n",
            "----\n",
            "iter 4500, loss: 62.835431\n",
            "----\n",
            " lh eld Mpgulg. i“lif ou thtiung wlomighise Jnorily of\n",
            "merlly ur ase. Bince b Cinn awy “Whe syooth yofsings davesteve sot nesikpuve latmattsemre bed my.”\n",
            "ig wfass, an corinp is them sinup fouan.”\n",
            "\n",
            "hers \n",
            "----\n",
            "iter 4600, loss: 62.597665\n",
            "----\n",
            " in nori, oand be netr an sowaau himi the dare fane ine shon Qoveryedspino a hachono bowwa n soremeratuer baosibame bnhrwiot, oanlyele kon? arwond asardeut  siit atlyon wimag thipr—oufsing bame\n",
            "miytowo \n",
            "----\n",
            "iter 4700, loss: 62.481559\n",
            "----\n",
            " lin cous in, anit wand ur, wures, yverave dou thor the ouon nos er mle why rvens to m and. y nom u tive ibuser\n",
            "\n",
            "fore are cereve shimis tous er seva mings un iy waat oo a id vohe tund\n",
            "onidyeuteEis cusk \n",
            "----\n",
            "iter 4800, loss: 62.246181\n",
            "----\n",
            " at—cs ar. Cantig matm th an wher thtithiweed, a bas in boow Mr\n",
            "wond th Jsathinn si the urgored aker yacbees Mbgawthaf ind, marsaco favatir emlar\n",
            "En Sit mand of en\n",
            "Ef ave\n",
            "now.dIwwos\n",
            "ing\n",
            "not abho sistor \n",
            "----\n",
            "iter 4900, loss: 61.841785\n",
            "----\n",
            " thter uathen anand in whcotas on cetitu hanime,, fong fozesta hev foun-Can\n",
            "a be\n",
            "no nourt bend,. che cuss’in waomen\n",
            " ham Mn woprer tf the, betering\n",
            "ever non. Ceremcourding the\n",
            "dere ta\n",
            "bout to no llang\n",
            " \n",
            "----\n",
            "iter 5000, loss: 61.546570\n",
            "----\n",
            " no ginertuun hever by the peut yot b tordus af rad, rous nge\n",
            "sherthey toud it; sue hirg yf mime gury; ti hest\n",
            "tf sued batong. sumted eHting, ming, of,;\n",
            "s; inkof, nald thens raingandestress Gons b, mis \n",
            "----\n",
            "iter 5100, loss: 61.463388\n",
            "----\n",
            " wting ible tonbad cgspat poghd lot asint thes\n",
            "tond Mr irs ke ha Buagt bar-lantillsinrs oM lowhe \n",
            "Tas fasys\n",
            "Bacw paslikf er dogt ghe bive me cespald sitiof at nounlt, sary Mr oceding\n",
            "of reel tindy bimy \n",
            "----\n",
            "iter 5200, loss: 61.193053\n",
            "----\n",
            " nond tont and anr ound ming sid sas. Mnd nee\n",
            "\n",
            "Bined Me, corin shathy, Ancocling chet comt—the, mid re. I dits meaggint\n",
            "\n",
            "third hy as l crala oon suad cingn hof and non than\n",
            "shotuwce tingand.”\n",
            " ous cot  \n",
            "----\n",
            "iter 5300, loss: 61.046206\n",
            "----\n",
            " \n",
            "a nind ticor wa tofeed mand gred be, whp, tus shet darhsshhe to hocing; Tte het hittich, and. Ald at whots dich whe withay sho heNd fount\n",
            "himkcous atco ersy tet com Eowdend ansd de rire of would Fam  \n",
            "----\n",
            "iter 5400, loss: 60.986706\n",
            "----\n",
            " ’ldon tr sut hithen\n",
            "tor, aflaskling wat depcit\n",
            "tom. thaved ant, rom bed mirr tn prabttilan houd pore urd. minonde pune fe soere. Fon mit whit abdran thee unle os dus bat tr mua’s baghens—air iker reut \n",
            "----\n",
            "iter 5500, loss: 60.741630\n",
            "----\n",
            " ry seit of a thichith he whle meicfemte het\n",
            "fofsived—hate wumt handevis hjis\n",
            "dono\n",
            "Cism.”\n",
            "\n",
            "him in if tos, im dawhen and _abuffiaber inon hapt bead sh. W of, hen fensu Cale handiy thta had\n",
            "yerateen sari \n",
            "----\n",
            "iter 5600, loss: 60.467233\n",
            "----\n",
            " ulm kamuol you noutoet aligoaagorofeou woar hion eraed, in lomonou. “Cacapkwoum gar thend savay reroune I mird—ancy hrran arsofous mesacm hro I daaaccan\n",
            "bahis Jas y hot iom. w, Qoumer oo in us Brine t \n",
            "----\n",
            "iter 5700, loss: 60.267215\n",
            "----\n",
            " meder’ut maceravee becustores nsmeret; waus hend the woos asiy onod onkense terin lere or oR ut wust. Canriwsarkengevunt’nge euhwweunt cading sed fowheoks mas the bind\n",
            "ly a beef soer she ese merested  \n",
            "----\n",
            "iter 5800, loss: 60.121162\n",
            "----\n",
            " gaume was rait don sem cat-teld pient nas thed sher dt to’fe tfe he nor he ess, in of arso\n",
            "nt beve, was Vleon\n",
            "Mrrer ropleeshe he Mr mins Fom er wong dinke the ing tit oune\n",
            "cogothe suhheld mloce e herp \n",
            "----\n",
            "iter 5900, loss: 60.090569\n",
            "----\n",
            " Celourouk paplawl siege, Ios hed tone exerioddertever erbiod foullparlocouthe had tirg!\n",
            "houd fuchout fome wanse foilicwoand\n",
            "Res he th dobet wailche thuld hey extonke aghithmaveramla tfabd of anderaubo \n",
            "----\n",
            "iter 6000, loss: 60.021419\n",
            "----\n",
            " heed, “Bma merrederrilos ars suratto kesas-ancofer enpe lekor thes it an anth to the meed wat yadithiot, a has a heary ordenarelo rohh rorith.\n",
            "“Ateas withroneaghy Mlioo cees um she spoot it  rirothe s \n",
            "----\n",
            "iter 6100, loss: 59.760889\n",
            "----\n",
            "  oreand heuca Ils, Do cino gaed a\n",
            "tNonee, her.”\n",
            "no hyoe ans?” biwon antefued Cicon. I s“Ca ag, wheonsles may, vithate siangore,” abe ees.” herinn on,, Mr yoaway Mr a aterene Marmam baad\n",
            "soany, Nowy be \n",
            "----\n",
            "iter 6200, loss: 59.454271\n",
            "----\n",
            " eaklidekormen, mull sive is, prathatingisp and\n",
            "“I sheokising oncwumeanp. Yasking, mard, shes klams buttendivexoubdion pikure hats sart, for. “Cathe\n",
            "thing” Dowhellinnttoo’ dign in thobe eos ul, ach.”\n",
            "t \n",
            "----\n",
            "iter 6300, loss: 59.339901\n",
            "----\n",
            " \n",
            "wass, wowe mer.\n",
            "Sad lerif jfothes,bid jsecly chert—ans ofifuse usdte\n",
            "sld blis yok, foradd hon ,\n",
            "wimy foagthecithotid hididicp s at nts adand. Casseds ca sive wed wenhhont,ent wastessa selr rougurosty \n",
            "----\n",
            "iter 6400, loss: 59.158324\n",
            "----\n",
            " rowh hingattoperthanger.”\n",
            "in’chat wfhatstomyer, wald sall Lede hermtote meiss Mruanghaveitrifband, achedt manot war.\n",
            "Britdy toud stetaon y, outedsithe donne Of ankms—icheng miuthinstroessher.\n",
            "\n",
            "I gidec \n",
            "----\n",
            "iter 6500, loss: 59.061653\n",
            "----\n",
            " m antoox; ar anave aingedesus maln. Btous oumustounda-d thsif levofas am hat nou ar anded\n",
            "En bacl Loub fucle whand ameny “I nce say ha hazeres.”\n",
            "s he I lounmla thasfthe i“\n",
            "\n",
            "hasl funoud phevestustoatis \n",
            "----\n",
            "iter 6600, loss: 59.011585\n",
            "----\n",
            " thade om\n",
            "havl pen heres an, al, co he he custontitemttiof waid yon,f iT y hif h be how abire an be he any ar, “I derr th this a he hee on.\n",
            "Brurgiy ar ins kes dulp. \n",
            "“What on thif ya,ero he. \n",
            "“woah.tig \n",
            "----\n",
            "iter 6700, loss: 58.996990\n",
            "----\n",
            "  s; of mant mife corad derance liwe\n",
            "pohit\n",
            "hell han \n",
            "f the, suly, at asmime ey of lesthingsed in :edeng afema he chad assentrt ac the kiskrva htooydecpit mhew des laver fuch. A of hade dose ths tfe wio \n",
            "----\n",
            "iter 6800, loss: 59.032431\n",
            "----\n",
            " er mirhy rellert heonom cedes om alkas pam ald, no he-pherepdrase wfappi2uvece jaod aid that Chesind heide, hire yano, peridtle, aneroe surfay rumest ounad of of or af jod corinelaventh id wousis tred \n",
            "----\n",
            "iter 6900, loss: 59.090027\n",
            "----\n",
            " inn now hil\n",
            "hurir hhen\n",
            "hicled fn agh Gthe wotselds wousringe hiclxow cowitys of noncu for, the hat exhennarts the ra nib rrunciling thing myons et of hi-spas uhas latard mitirim leslyy whng\n",
            "corpl thab \n",
            "----\n",
            "iter 7000, loss: 58.952888\n",
            "----\n",
            "  houbat in, wated on lito\n",
            "lobt hicedcithellerl heevithals hertinnivelard-hant accend sos inne\n",
            "alle ann Mnas wirtsorlaony ar\n",
            "aecinb the im an don mased\n",
            "atd shans tfion te Ie\n",
            "af erith then thastirt, the \n",
            "----\n",
            "iter 7100, loss: 58.705496\n",
            "----\n",
            " e vare myoD ans see sowraf\n",
            "be fo krinnttot chat haly, thl won las, mat Dead ofious Med Doon. I in ald poutiyepond sadimacila thd naro he lere inot ound of oncus,, thtmofact of wisirevirpage cihcaltoon \n",
            "----\n",
            "iter 7200, loss: 58.451564\n",
            "----\n",
            " or tian wade crenconed\n",
            "sird\n",
            "thotamid thale-tno bithe Mbce of ontteit nous ohimly diacinbed a tht thanditirg. Fon seniteraba dreige melkele wincink dope\n",
            "wora’n bont and feld hat aftineitibe. \n",
            "Direwon y \n",
            "----\n",
            "iter 7300, loss: 58.363933\n",
            "----\n",
            "  the onorouns Castha coonrorpem masely Mnd beryo io\n",
            "nelly bs-ifuide mesld Mr\n",
            "intino oncy row-for. I the\n",
            "thipenousecemtibr ons ofofor; acl\n",
            "noumery on, wleout nallad probe yean tuvling ecmef on onr cast \n",
            "----\n",
            "iter 7400, loss: 58.409960\n",
            "----\n",
            " hkiok thy sate clet inge nt wore the sikk ther.”\n",
            "\n",
            "“Whithat thted, is cout of igr bedarting? Ag? Cadls greeut ensimt, atyis wackcave edseter thut lerfians him matander corik ouved saro whal exrsao hesy \n",
            "----\n",
            "iter 7500, loss: 58.394877\n",
            "----\n",
            "  hiwo\n",
            " He woocmofis’shones somarpancter. H dty rentparpalm, oko\n",
            "hin miginl, ther. He nound\n",
            "theissy on leriky pad’ch hard ly\n",
            "her cratlevermathing thand-ly “Hg and Dominscin shit heing opidasthat ans pa \n",
            "----\n",
            "iter 7600, loss: 58.438716\n",
            "----\n",
            " rowib\n",
            "hate loresesianse to dof. Btautevimicte graoce say yous abyow alf fhpe lew! th thita the thisarter.”\n",
            "\n",
            "an meuf, yot thttinse weas\n",
            "ertorse ape sore hid matt ofey\n",
            "latoror,\n",
            "sowiom, laa! sind anslexo \n",
            "----\n",
            "iter 7700, loss: 58.432364\n",
            "----\n",
            " ut. Actoed pbell wast roltace binby abe hoolfare wame mee wadede—the tur l va seraacomlle thweaqurs nos, fuld wfars an.\n",
            "Bet\n",
            "ras don mitle ard an: soo mauce by nogen\n",
            "war roef antat pra Nonge of wased m \n",
            "----\n",
            "iter 7800, loss: 58.489674\n",
            "----\n",
            " inggomed intssanged,, , ave rrasd fertin yoand comuky alcull—toend besensidegstingeicus bad fusd alwyipcend watned waples ponging, mssary\n",
            "grove nese leind ingist eing on coog afked is the dould fastfa \n",
            "----\n",
            "iter 7900, loss: 58.515916\n",
            "----\n",
            " l\n",
            "palas os my wtoting she sorhen Buoklif anen wowly Rolttoum ca caln deid she menekse narimens woel Mr. Heabco thal. hestishing of houll lithe\n",
            "mowe\n",
            "on eyteoun thate llem seroiwimerOe\n",
            "aflyore\n",
            "in ty abd \n",
            "----\n",
            "iter 8000, loss: 58.541524\n",
            "----\n",
            " Ers, ile absisingur the\n",
            "cuy grare an he to beria han han Gons,onsid-ssacdilibp to wung\n",
            "th. Whal\n",
            "she nomle bri-ner ghe can’s Mr con von whad to sur how thevened of Lencis wower sles in. is fforaal-ged  \n",
            "----\n",
            "iter 8100, loss: 58.390849\n",
            "----\n",
            " my all,” mrisvat fa, imety, yoer meld the himyayey be.””\n",
            "\n",
            "““bum hemion bellinht feit mer, nough; yake maro himrasly yot, alfao hyome dray ore wedpious coobed withe is my ass.\n",
            "\n",
            "“The wome-tistase to sov \n",
            "----\n",
            "iter 8200, loss: 58.317181\n",
            "----\n",
            " .”\n",
            "E Var dees toat asFy Mr pxaved.” gouberithang sicidedessofon woum, bake himerureseok, sadrelestvar’d soet pirgyoutes ey tretaratryoutraid ofeereens.”\n",
            "\n",
            "“,t.”\n",
            "she’d visam\n",
            "opescfourd\n",
            "beno hatlestore y \n",
            "----\n",
            "iter 8300, loss: 58.205667\n",
            "----\n",
            " igow sas fonasprptountarer, frying weat whate whay tocy.”\n",
            "\n",
            "“Indssomes to?\n",
            "“Bpouctabedes ma, whinon ant idimaed, aucese had more tarh” whese.\n",
            "\n",
            "“I sut keedd.”\n",
            "\n",
            "“Hos yo pio“. Rowidis\n",
            "mistabp woot of fouu \n",
            "----\n",
            "iter 8400, loss: 58.023134\n",
            "----\n",
            " linpingood\n",
            "domingiige. T,\n",
            "ainlyof mbeargortte’su icha, wein yol, ngigsidowlas’scotamer ther any toolsidereli’k Fipingliuneconing\n",
            "dotant “I whiy goht, omertingereraed of moll’s-arumesm I teelkind onnom \n",
            "----\n",
            "iter 8500, loss: 57.899230\n",
            "----\n",
            " oly me; hannincelest Caterheed lemtsonged-dad and be uAhel here haabrticl Rupreingus shistlenedtibniwneesthethaklpyeapmute\n",
            "sderifher of womand yous nearis onot vaedochdang gherf apdeendease\n",
            "a thikly t \n",
            "----\n",
            "iter 8600, loss: 57.928454\n",
            "----\n",
            " lytartohe th?s\n",
            "mit one coth lert heit ed he pent. shy-ro theuno tho, cabn\n",
            "thprvoden oce the ming the authe miggs “Wh, an Mr wfalt\n",
            "thenange phost. nideweand yomerimao yaple tr tio\n",
            " “The dall ther hy ka \n",
            "----\n",
            "iter 8700, loss: 57.858225\n",
            "----\n",
            " ilakks wowasfin witivenhs, pabh\n",
            "o hat sorean sof of sirk.”\n",
            "\n",
            "“Whever-thast, why\n",
            " al tin’t call her.\n",
            "\n",
            "anhichta mirter hin the thent out, lemt Mull me ull aT baring a hede co Nousther whhe cto, coacl wan \n",
            "----\n",
            "iter 8800, loss: 57.786380\n",
            "----\n",
            "  to oune thagime un;\n",
            "Suon\n",
            "linde now mion aaddeco (and mass and “I\n",
            "derele th, ard, thenst wt. I be cofang yow\n",
            " Rhly whien wotl ther to doto iber\n",
            "wiousheslowhing dedyoghy; shen.”\n",
            "\n",
            "“Bus mit of ry  bede—g \n",
            "----\n",
            "iter 8900, loss: 57.804140\n",
            "----\n",
            " th! agyemtarly bro hesFaste on the’t fut rid moud arg iWh at then in and what bet in tonlowat an oWh hemes dencit an bes surtering on there yochent andit yullald wheriighonelited it hey to Mrcat wha,  \n",
            "----\n",
            "iter 9000, loss: 57.849252\n",
            "----\n",
            " nconut\n",
            "as hidem coumoriin a “fous Frey.”\n",
            "\n",
            "wast. nothingomyr thetillimut oromer ming memron. “Yt ounlishem iom in tht ound nod wat soesthim tiry brouno sad meneat her simo, toes and. Foedy and, lecd ac \n",
            "----\n",
            "iter 9100, loss: 57.843984\n",
            "----\n",
            " d wiclery. Foo toid mis sontiker to whabe!\n",
            "slowsnlo coull.”\n",
            "\n",
            "stessse ay lesesty at aghpas lI stad.”\n",
            "\n",
            "“lhag proty shiys erstlitn whals Thed lat helil thl—abhith he\n",
            "wome surg yow ak. Bporsikle onter, wh \n",
            "----\n",
            "iter 9200, loss: 57.779467\n",
            "----\n",
            " ithe rad tat woon kithatevorly tlott. Whononnt demtimes\n",
            "bankrom tftofounn Fiphilirf yge thpe ans: sorfy noun ho, musly, Yot midr\n",
            "therees wipar to bens it ht yod ule ered Waninge hokisao ratuot wine an \n",
            "----\n",
            "iter 9300, loss: 57.658403\n",
            "----\n",
            " he highans ghourpher be. But und rom\n",
            "doncless on sud. fud pawens heven\n",
            "ureind, hent hhe ryoet Pees. You at hea the aigh thling, you velf al ans mo he—thar oug me iuver agy he inkerinn austo, hon stong \n",
            "----\n",
            "iter 9400, loss: 57.631452\n",
            "----\n",
            " so natn to sa u lerte, te lyserede o“\n",
            "ine,\n",
            "li wout.”—\n",
            "peser that on ,ut ese staif oo wokile iNhe ry; fo\n",
            "ker soenand, danked, a drey winssame qurime ne, sutiowiot in tene to\n",
            "was re thipealt dostyi have \n",
            "----\n",
            "iter 9500, loss: 57.631701\n",
            "----\n",
            " Ml’t Foaght ined veald at if Ley.”\n",
            "\n",
            "“I mon. Waroudbling, Freyou ong be trapyod ; ““I dy yed,” falf so mol bes tas soingo Murs, issy lantover you\n",
            "porastedR Hat memu, whamr tuad Rola taod sOo ane Mle on \n",
            "----\n",
            "iter 9600, loss: 57.414373\n",
            "----\n",
            " aw\n",
            "blaait at heapdaighy ens hit a?\n",
            "Mr—ter in Ml oune, of sh thaggome rion ire you\n",
            "gnobeve—thime, thesy\n",
            "wons of.,\n",
            "yobl th hen.”\n",
            "\n",
            "“Bhat mast, th andthid coelith tauvtrewe Monsigeetise she why dipas\n",
            "wald \n",
            "----\n",
            "iter 9700, loss: 57.396888\n",
            "----\n",
            " \n",
            "merdive Fo wher parfoatorver of orothat thar hhentoncegith hid, s than suthet ter, ande ine what the sied tt” thith heid ting inebinicted r and, wono bhan,r. andats qus th nolipliok waad heot rowloot \n",
            "----\n",
            "iter 9800, loss: 57.321173\n",
            "----\n",
            " andion nepaoe a dam are of of whis wing nopy, vecnume aidigh.r\n",
            "fo beHe er moid wispare exiw daratreon sfery frowang thh lorsl.\n",
            "soun bewive fond tapastiok ingis\n",
            " I vomimintaodhint\n",
            "iaclphoisining eivens \n",
            "----\n",
            "iter 9900, loss: 57.200994\n",
            "----\n",
            " or cees. Fok. As Mumweitl buy oune Mell “the the, whataders, at de bapedgen” shd saly\n",
            "bar?”\n",
            "\n",
            "“Wus Mestall. Rulouty.’nd, bided at wor, hartar?”\n",
            "\n",
            "““I waco ake. \n",
            "“Asted it propy extaut,” i“\n",
            "“I ibraead a  \n",
            "----\n",
            "iter 10000, loss: 57.085708\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
        "BSD License\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "# data I/O\n",
        "data = open('input.txt', 'r').read() # should be simple plain text file\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
        "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "# hyperparameters\n",
        "hidden_size = 100 # size of hidden layer of neurons\n",
        "seq_length = 25 # number of steps to unroll the RNN for\n",
        "learning_rate = 1e-1\n",
        "\n",
        "# model parameters\n",
        "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
        "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
        "by = np.zeros((vocab_size, 1)) # output bias\n",
        "\n",
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"\n",
        "  inputs,targets are both list of integers.\n",
        "  hprev is Hx1 array of initial hidden state\n",
        "  returns the loss, gradients on model parameters, and last hidden state\n",
        "  \"\"\"\n",
        "  xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  loss = 0\n",
        "  # forward pass\n",
        "  for t in range(len(inputs)):\n",
        "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
        "    xs[t][inputs[t]] = 1\n",
        "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
        "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
        "\n",
        "  # backward pass: compute gradients going backwards\n",
        "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "  for t in reversed(range(len(inputs))):\n",
        "    dy = np.copy(ps[t])\n",
        "    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    dby += dy\n",
        "    dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
        "    dbh += dhraw\n",
        "    dWxh += np.dot(dhraw, xs[t].T)\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T)\n",
        "    dhnext = np.dot(Whh.T, dhraw)\n",
        "\n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
        "\n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
        "\n",
        "\n",
        "def sample(h, seed_ix, n):\n",
        "  \"\"\" \n",
        "  sample a sequence of integers from the model \n",
        "  h is memory state, seed_ix is seed letter for first time step\n",
        "  \"\"\"\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  x[seed_ix] = 1\n",
        "  ixes = []\n",
        "  for t in range(n):\n",
        "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "    y = np.dot(Why, h) + by\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[ix] = 1\n",
        "    ixes.append(ix)\n",
        "  return ixes\n",
        "\n",
        "\n",
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
        "while True:\n",
        "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  if p+seq_length+1 >= len(data) or n == 0: \n",
        "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "    p = 0 # go from start of data\n",
        "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # sample from the model now and then\n",
        "  if n % 100 == 0:\n",
        "    sample_ix = sample(hprev, inputs[0], 200)\n",
        "    txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "    print('----\\n %s \\n----' % (txt, ))\n",
        "\n",
        "  # forward seq_length characters through the net and fetch gradient\n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "  if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
        "  \n",
        "  # perform parameter update with Adagrad\n",
        "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
        "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "  p += seq_length # move data pointer\n",
        "  n += 1 # iteration counter\n",
        "\n",
        "  if( n > 10000 ):\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "# Make it possible to provide input file as a command-line argument; input.txt\n",
        "# is still the default.\n",
        "# if len(sys.argv) > 1:\n",
        "#     filename = sys.argv[1]\n",
        "# else:\n",
        "#     filename = 'input.txt'\n",
        "\n",
        "filename = 'input.txt'\n",
        "\n",
        "with open(filename, 'r') as f:\n",
        "    data = f.read()\n",
        "\n",
        "# All unique characters / entities in the data set.\n",
        "chars = list(set(data))\n",
        "data_size = len(data)\n",
        "V = vocab_size = len(chars)\n",
        "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "\n",
        "# Each character in the vocabulary gets a unique integer index assigned, in the\n",
        "# half-open interval [0:N). These indices are useful to create one-hot encoded\n",
        "# vectors that represent characters in numerical computations.\n",
        "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i:ch for i, ch in enumerate(chars)}\n",
        "print('char_to_ix', char_to_ix)\n",
        "print('ix_to_char', ix_to_char)\n",
        "\n",
        "# Hyperparameters.\n",
        "\n",
        "# Size of hidden state vectors; applies to h and c.\n",
        "H = hidden_size = 100\n",
        "seq_length = 16 # number of steps to unroll the LSTM for\n",
        "learning_rate = 0.1\n",
        "\n",
        "# The input x is concatenated with state h, and the joined vector is used to\n",
        "# feed into most blocks within the LSTM cell. The combined height of the column\n",
        "# vector is HV.\n",
        "HV = H + V\n",
        "\n",
        "# Stop when processed this much data\n",
        "MAX_DATA = 10000000\n",
        "\n",
        "# Model parameters/weights -- these are shared among all steps. Weights\n",
        "# initialized randomly; biases initialized to 0.\n",
        "# Inputs are characters one-hot encoded in a vocab-sized vector.\n",
        "# Dimensions: H = hidden_size, V = vocab_size, HV = hidden_size + vocab_size\n",
        "Wf = np.random.randn(H, HV) * 0.01\n",
        "bf = np.zeros((H, 1))\n",
        "Wi = np.random.randn(H, HV) * 0.01\n",
        "bi = np.zeros((H, 1))\n",
        "Wcc = np.random.randn(H, HV) * 0.01\n",
        "bcc = np.zeros((H, 1))\n",
        "Wo = np.random.randn(H, HV) * 0.01\n",
        "bo = np.zeros((H, 1))\n",
        "Wy = np.random.randn(V, H) * 0.01\n",
        "by = np.zeros((V, 1))\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"Computes sigmoid function.\n",
        "    z: array of input values.\n",
        "    Returns array of outputs, sigmoid(z).\n",
        "    \"\"\"\n",
        "    # Note: this version of sigmoid tries to avoid overflows in the computation\n",
        "    # of e^(-z), by using an alternative formulation when z is negative, to get\n",
        "    # 0. e^z / (1+e^z) is equivalent to the definition of sigmoid, but we won't\n",
        "    # get e^(-z) to overflow when z is very negative.\n",
        "    # Since both the x and y arguments to np.where are evaluated by Python, we\n",
        "    # may still get overflow warnings for large z elements; therefore we ignore\n",
        "    # warnings during this computation.\n",
        "    with np.errstate(over='ignore', invalid='ignore'):\n",
        "        return np.where(z >= 0,\n",
        "                        1 / (1 + np.exp(-z)),\n",
        "                        np.exp(z) / (1 + np.exp(z)))\n",
        "\n",
        "\n",
        "def lossFun(inputs, targets, hprev, cprev):\n",
        "    \"\"\"Runs forward and backward passes through the RNN.\n",
        "      TODO: keep me updated!\n",
        "      inputs, targets: Lists of integers. For some i, inputs[i] is the input\n",
        "                       character (encoded as an index into the ix_to_char map)\n",
        "                       and targets[i] is the corresponding next character in the\n",
        "                       training data (similarly encoded).\n",
        "      hprev: Hx1 array of initial hidden state\n",
        "      cprev: Hx1 array of initial hidden state\n",
        "      returns: loss, gradients on model parameters, and last hidden states\n",
        "    \"\"\"\n",
        "    # Caches that keep values computed in the forward pass at each time step, to\n",
        "    # be reused in the backward pass.\n",
        "    xs, xhs, ys, ps, hs, cs, fgs, igs, ccs, ogs = (\n",
        "            {}, {}, {}, {}, {}, {}, {}, {}, {}, {})\n",
        "\n",
        "    # Initial incoming states.\n",
        "    hs[-1] = np.copy(hprev)\n",
        "    cs[-1] = np.copy(cprev)\n",
        "\n",
        "    loss = 0\n",
        "    # Forward pass\n",
        "    for t in range(len(inputs)):\n",
        "        # Input at time step t is xs[t]. Prepare a one-hot encoded vector of\n",
        "        # shape (V, 1). inputs[t] is the index where the 1 goes.\n",
        "        xs[t] = np.zeros((V, 1))\n",
        "        xs[t][inputs[t]] = 1\n",
        "\n",
        "        # hprev and xs[t] are column vector; stack them together into a \"taller\"\n",
        "        # column vector - first the elements of x, then h.\n",
        "        xhs[t] = np.vstack((xs[t], hs[t-1]))\n",
        "\n",
        "        # Gates f, i and o.\n",
        "        fgs[t] = sigmoid(np.dot(Wf, xhs[t]) + bf)\n",
        "        igs[t] = sigmoid(np.dot(Wi, xhs[t]) + bi)\n",
        "        ogs[t] = sigmoid(np.dot(Wo, xhs[t]) + bo)\n",
        "\n",
        "        # Candidate cc.\n",
        "        ccs[t] = np.tanh(np.dot(Wcc, xhs[t]) + bcc)\n",
        "\n",
        "        # This step's h and c.\n",
        "        cs[t] = fgs[t] * cs[t-1] + igs[t] * ccs[t]\n",
        "        hs[t] = np.tanh(cs[t]) * ogs[t]\n",
        "\n",
        "        # Softmax for output.\n",
        "        ys[t] = np.dot(Wy, hs[t]) + by\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
        "\n",
        "        # Cross-entropy loss.\n",
        "        loss += -np.log(ps[t][targets[t], 0])\n",
        "\n",
        "    # Initialize gradients of all weights/biases to 0.\n",
        "    dWf = np.zeros_like(Wf)\n",
        "    dbf = np.zeros_like(bf)\n",
        "    dWi = np.zeros_like(Wi)\n",
        "    dbi = np.zeros_like(bi)\n",
        "    dWcc = np.zeros_like(Wcc)\n",
        "    dbcc = np.zeros_like(bcc)\n",
        "    dWo = np.zeros_like(Wo)\n",
        "    dbo = np.zeros_like(bo)\n",
        "    dWy = np.zeros_like(Wy)\n",
        "    dby = np.zeros_like(by)\n",
        "\n",
        "    # Incoming gradients for h and c; for backwards loop step these represent\n",
        "    # dh[t] and dc[t]; we do truncated BPTT, so assume they are 0 initially.\n",
        "    dhnext = np.zeros_like(hs[0])\n",
        "    dcnext = np.zeros_like(cs[0])\n",
        "\n",
        "    # The backwards pass iterates over the input sequence backwards.\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backprop through the gradients of loss and softmax.\n",
        "        dy = np.copy(ps[t])\n",
        "        dy[targets[t]] -= 1\n",
        "\n",
        "        # Compute gradients for the Wy and by parameters.\n",
        "        dWy += np.dot(dy, hs[t].T)\n",
        "        dby += dy\n",
        "\n",
        "        # Backprop through the fully-connected layer (Wy, by) to h. Also add up\n",
        "        # the incoming gradient for h from the next cell.\n",
        "        dh = np.dot(Wy.T, dy) + dhnext\n",
        "\n",
        "        # Backprop through multiplication with output gate; here \"dtanh\" means\n",
        "        # the gradient at the output of tanh.\n",
        "        dctanh = ogs[t] * dh\n",
        "        # Backprop through the tanh function; since cs[t] branches in two\n",
        "        # directions we add dcnext too.\n",
        "        dc = dctanh * (1 - np.tanh(cs[t]) ** 2) + dcnext\n",
        "\n",
        "        # Backprop through multiplication with the tanh; here \"dhogs\" means\n",
        "        # the gradient at the output of the sigmoid of the output gate. Then\n",
        "        # backprop through the sigmoid itself (ogs[t] is the sigmoid output).\n",
        "        dhogs = dh * np.tanh(cs[t])\n",
        "        dho = dhogs * ogs[t] * (1 - ogs[t])\n",
        "\n",
        "        # Compute gradients for the output gate parameters.\n",
        "        dWo += np.dot(dho, xhs[t].T)\n",
        "        dbo += dho\n",
        "\n",
        "        # Backprop dho to the xh input.\n",
        "        dxh_from_o = np.dot(Wo.T, dho)\n",
        "\n",
        "        # Backprop through the forget gate: sigmoid and elementwise mul.\n",
        "        dhf = cs[t-1] * dc * fgs[t] * (1 - fgs[t])\n",
        "        dWf += np.dot(dhf, xhs[t].T)\n",
        "        dbf += dhf\n",
        "        dxh_from_f = np.dot(Wf.T, dhf)\n",
        "\n",
        "        # Backprop through the input gate: sigmoid and elementwise mul.\n",
        "        dhi = ccs[t] * dc * igs[t] * (1 - igs[t])\n",
        "        dWi += np.dot(dhi, xhs[t].T)\n",
        "        dbi += dhi\n",
        "        dxh_from_i = np.dot(Wi.T, dhi)\n",
        "\n",
        "        dhcc = igs[t] * dc * (1 - ccs[t] ** 2)\n",
        "        dWcc += np.dot(dhcc, xhs[t].T)\n",
        "        dbcc += dhcc\n",
        "        dxh_from_cc = np.dot(Wcc.T, dhcc)\n",
        "\n",
        "        # Combine all contributions to dxh, and extract the gradient for the\n",
        "        # h part to propagate backwards as dhnext.\n",
        "        dxh = dxh_from_o + dxh_from_f + dxh_from_i + dxh_from_cc\n",
        "        dhnext = dxh[V:, :]\n",
        "\n",
        "        # dcnext from dc and the forget gate.\n",
        "        dcnext = fgs[t] * dc\n",
        "\n",
        "    # Gradient clipping to the range [-5, 5].\n",
        "    for dparam in [dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby]:\n",
        "        np.clip(dparam, -5, 5, out=dparam)\n",
        "\n",
        "    return (loss, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n",
        "            hs[len(inputs)-1], cs[len(inputs)-1])\n",
        "\n",
        "\n",
        "def sample(h, c, seed_ix, n):\n",
        "    \"\"\"Sample a sequence of integers from the model.\n",
        "    Runs the LSTM in forward mode for n steps; seed_ix is the seed letter for\n",
        "    the first time step, h and c are the memory state. Returns a sequence of\n",
        "    letters produced by the model (indices).\n",
        "    \"\"\"\n",
        "    x = np.zeros((V, 1))\n",
        "    x[seed_ix] = 1\n",
        "    ixes = []\n",
        "\n",
        "    for t in range(n):\n",
        "        # Run the forward pass only.\n",
        "        xh = np.vstack((x, h))\n",
        "        fg = sigmoid(np.dot(Wf, xh) + bf)\n",
        "        ig = sigmoid(np.dot(Wi, xh) + bi)\n",
        "        og = sigmoid(np.dot(Wo, xh) + bo)\n",
        "        cc = np.tanh(np.dot(Wcc, xh) + bcc)\n",
        "        c = fg * c + ig * cc\n",
        "        h = np.tanh(c) * og\n",
        "        y = np.dot(Wy, h) + by\n",
        "        p = np.exp(y) / np.sum(np.exp(y))\n",
        "\n",
        "        # Sample from the distribution produced by softmax.\n",
        "        ix = np.random.choice(range(V), p=p.ravel())\n",
        "        x = np.zeros((V, 1))\n",
        "        x[ix] = 1\n",
        "        ixes.append(ix)\n",
        "    return ixes\n",
        "\n",
        "\n",
        "def gradCheck(inputs, targets, hprev, cprev):\n",
        "    global Wf, Wi, bf, bi, Wcc, bcc, Wo, bo, Wy, by\n",
        "    num_checks, delta = 10, 1e-5\n",
        "    (_, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n",
        "     _, _) = lossFun(inputs, targets, hprev, cprev)\n",
        "    for param, dparam, name in zip(\n",
        "            [Wf, bf, Wi, bi, Wcc, bcc, Wo, bo, Wy, by],\n",
        "            [dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby],\n",
        "            ['Wf', 'bf', 'Wi', 'bi', 'Wcc', 'bcc', 'Wo', 'bo', 'Wy', 'by']):\n",
        "        assert dparam.shape == param.shape\n",
        "        print(name)\n",
        "        for i in range(num_checks):\n",
        "            ri = np.random.randint(0, param.size)\n",
        "            old_val = param.flat[ri]\n",
        "            param.flat[ri] = old_val + delta\n",
        "            numloss0 = lossFun(inputs, targets, hprev, cprev)[0]\n",
        "            param.flat[ri] = old_val - delta\n",
        "            numloss1 = lossFun(inputs, targets, hprev, cprev)[0]\n",
        "            param.flat[ri] = old_val # reset\n",
        "            grad_analytic = dparam.flat[ri]\n",
        "            grad_numerical = (numloss0 - numloss1) / (2 * delta)\n",
        "            if grad_numerical + grad_analytic == 0:\n",
        "                rel_error = 0\n",
        "            else:\n",
        "                rel_error = (abs(grad_analytic - grad_numerical) /\n",
        "                             abs(grad_numerical + grad_analytic))\n",
        "            print('%s, %s => %e' % (grad_numerical, grad_analytic, rel_error))\n",
        "\n",
        "\n",
        "def basicGradCheck():\n",
        "    inputs = [char_to_ix[ch] for ch in data[:seq_length]]\n",
        "    targets = [char_to_ix[ch] for ch in data[1:seq_length+1]]\n",
        "    hprev = np.random.randn(H, 1)\n",
        "    cprev = np.random.randn(H, 1)\n",
        "    gradCheck(inputs, targets, hprev, cprev)\n",
        "\n",
        "# Uncomment this to run gradient checking instead of training\n",
        "#basicGradCheck()\n",
        "#sys.exit()\n",
        "\n",
        "# n is the iteration counter; p is the input sequence pointer, at the beginning\n",
        "# of each step it points at the sequence in the input that will be used for\n",
        "# training this iteration.\n",
        "n, p = 0, 0\n",
        "\n",
        "# Memory variables for Adagrad.\n",
        "mWf = np.zeros_like(Wf)\n",
        "mbf = np.zeros_like(bf)\n",
        "mWi = np.zeros_like(Wi)\n",
        "mbi = np.zeros_like(bi)\n",
        "mWcc = np.zeros_like(Wcc)\n",
        "mbcc = np.zeros_like(bcc)\n",
        "mWo = np.zeros_like(Wo)\n",
        "mbo = np.zeros_like(bo)\n",
        "mWy = np.zeros_like(Wy)\n",
        "mby = np.zeros_like(by)\n",
        "smooth_loss = -np.log(1.0/V) * seq_length\n",
        "\n",
        "while p < MAX_DATA:\n",
        "    # Prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "    if p+seq_length+1 >= len(data) or n == 0:\n",
        "        # Reset RNN memory\n",
        "        hprev = np.zeros((H, 1))\n",
        "        cprev = np.zeros((H, 1))\n",
        "        p = 0 # go from start of data\n",
        "\n",
        "    # In each step we unroll the RNN for seq_length cells, and present it with\n",
        "    # seq_length inputs and seq_length target outputs to learn.\n",
        "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "    # Sample from the model now and then.\n",
        "    if n % 1000 == 0:\n",
        "        sample_ix = sample(hprev, cprev, inputs[0], 200)\n",
        "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "        print('----\\n %s \\n----' % (txt,))\n",
        "\n",
        "    # Forward seq_length characters through the RNN and fetch gradient.\n",
        "    (loss, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n",
        "     hprev, cprev) = lossFun(inputs, targets, hprev, cprev)\n",
        "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "    if n % 200 == 0:\n",
        "        print('iter %d (p=%d), loss %f' % (n, p, smooth_loss))\n",
        "\n",
        "    # Perform parameter update with Adagrad.\n",
        "    for param, dparam, mem in zip(\n",
        "            [Wf, bf, Wi, bi, Wcc, bcc, Wo, bo, Wy, by],\n",
        "            [dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby],\n",
        "            [mWf, mbf, mWi, mbi, mWcc, mbcc, mWo, mbo, mWy, mby]):\n",
        "        mem += dparam * dparam\n",
        "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
        "\n",
        "    p += seq_length\n",
        "    n += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 24227
        },
        "id": "-yymnf9AHwZR",
        "outputId": "ff046c59-2466-4bee-ba4a-6171c86ac013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1799298 characters, 105 unique.\n",
            "char_to_ix {'%': 0, 'g': 1, 'F': 2, 'j': 3, 'P': 4, '?': 5, '“': 6, 'Q': 7, '…': 8, 'ä': 9, 'L': 10, ':': 11, ';': 12, '(': 13, '1': 14, '\\n': 15, '—': 16, 'é': 17, '_': 18, 'J': 19, 'ö': 20, 'O': 21, 'p': 22, 'A': 23, 'u': 24, 'œ': 25, 't': 26, 'ô': 27, ']': 28, 'q': 29, '2': 30, 'H': 31, '\"': 32, '$': 33, 'i': 34, '8': 35, 'x': 36, 'I': 37, 's': 38, 'è': 39, 'ù': 40, 'b': 41, 'R': 42, 'X': 43, 'T': 44, '#': 45, '’': 46, '*': 47, 'h': 48, '4': 49, 'E': 50, 'ü': 51, 'm': 52, 'v': 53, 'N': 54, '.': 55, \"'\": 56, 'z': 57, 'f': 58, 'S': 59, '-': 60, 'û': 61, 'o': 62, 'ò': 63, 'Y': 64, 'k': 65, '5': 66, 'y': 67, '”': 68, 'î': 69, ')': 70, ' ': 71, 'B': 72, '‘': 73, 'w': 74, 'a': 75, ',': 76, 'e': 77, '!': 78, '[': 79, 'n': 80, '/': 81, '9': 82, '7': 83, 'K': 84, 'l': 85, 'W': 86, 'D': 87, '6': 88, 'U': 89, 'c': 90, '3': 91, 'ê': 92, 'r': 93, 'â': 94, '&': 95, 'æ': 96, 'M': 97, 'd': 98, 'G': 99, 'C': 100, 'V': 101, 'Z': 102, 'à': 103, '0': 104}\n",
            "ix_to_char {0: '%', 1: 'g', 2: 'F', 3: 'j', 4: 'P', 5: '?', 6: '“', 7: 'Q', 8: '…', 9: 'ä', 10: 'L', 11: ':', 12: ';', 13: '(', 14: '1', 15: '\\n', 16: '—', 17: 'é', 18: '_', 19: 'J', 20: 'ö', 21: 'O', 22: 'p', 23: 'A', 24: 'u', 25: 'œ', 26: 't', 27: 'ô', 28: ']', 29: 'q', 30: '2', 31: 'H', 32: '\"', 33: '$', 34: 'i', 35: '8', 36: 'x', 37: 'I', 38: 's', 39: 'è', 40: 'ù', 41: 'b', 42: 'R', 43: 'X', 44: 'T', 45: '#', 46: '’', 47: '*', 48: 'h', 49: '4', 50: 'E', 51: 'ü', 52: 'm', 53: 'v', 54: 'N', 55: '.', 56: \"'\", 57: 'z', 58: 'f', 59: 'S', 60: '-', 61: 'û', 62: 'o', 63: 'ò', 64: 'Y', 65: 'k', 66: '5', 67: 'y', 68: '”', 69: 'î', 70: ')', 71: ' ', 72: 'B', 73: '‘', 74: 'w', 75: 'a', 76: ',', 77: 'e', 78: '!', 79: '[', 80: 'n', 81: '/', 82: '9', 83: '7', 84: 'K', 85: 'l', 86: 'W', 87: 'D', 88: '6', 89: 'U', 90: 'c', 91: '3', 92: 'ê', 93: 'r', 94: 'â', 95: '&', 96: 'æ', 97: 'M', 98: 'd', 99: 'G', 100: 'C', 101: 'V', 102: 'Z', 103: 'à', 104: '0'}\n",
            "----\n",
            "  üin1Rq…8uT5…ô[sö'lh‘Y]KgMym!61v/’ô3àpæ)kp9\"nî”n*\n",
            "æK?ZèY$VEI‘1‘O!\"/)û“-JPkòoslô:gmEbY;# ôlZ%2*)üäùcJSNT‘VzœfDwp nRi?nuh :”_œüôù,û GWr“\n",
            "b%hàmè’oQGWzNu2\"ù]!ézKä3A!_lB’ca0LkòK6lè,];F4è-z$--)% üNhBO—àBW0ô \n",
            "----\n",
            "iter 0 (p=0), loss 74.463368\n",
            "iter 200 (p=3200), loss 67.538025\n",
            "iter 400 (p=6400), loss 62.889180\n",
            "iter 600 (p=9600), loss 58.559700\n",
            "iter 800 (p=12800), loss 54.779447\n",
            "----\n",
            " whast of could muld acion wave\n",
            "am bed beimalbid-my omust ow.\n",
            "\n",
            "\n",
            "Do meacssnideband lod yomar toWineousand .\n",
            ";oom sadeict mived blave lom hen lt.t\n",
            "iodX, beren of mereyss—d Cood yof,\n",
            "of of that for sedey  \n",
            "----\n",
            "iter 1000 (p=16000), loss 51.472862\n",
            "iter 1200 (p=19200), loss 48.578481\n",
            "iter 1400 (p=22400), loss 45.874793\n",
            "iter 1600 (p=25600), loss 44.101693\n",
            "iter 1800 (p=28800), loss 42.409957\n",
            "----\n",
            " hon lovecalyer ttementy tean ale Dorocharudice pliting of to grootly.\n",
            "\n",
            "“You in.t inding cad soh saa lacinot\n",
            "lidcrey to herres, you of sele arlling a\n",
            "Wess be epeed was and onst is Dorother that surt-li \n",
            "----\n",
            "iter 2000 (p=32000), loss 40.623527\n",
            "iter 2200 (p=35200), loss 39.095026\n",
            "iter 2400 (p=38400), loss 37.795396\n",
            "iter 2600 (p=41600), loss 36.705202\n",
            "iter 2800 (p=44800), loss 35.967574\n",
            "----\n",
            " s, he the or wa, wore _he goont of thon is a sulntorned not a the Duratted of a mit the his hit the maf whe thac to her, it. I wam-ory of owach the forlolnde, budgunred for it\n",
            "he ma,s Miss and jul thi \n",
            "----\n",
            "iter 3000 (p=48000), loss 35.253196\n",
            "iter 3200 (p=51200), loss 34.815944\n",
            "iter 3400 (p=54400), loss 34.223459\n",
            "iter 3600 (p=57600), loss 33.543858\n",
            "iter 3800 (p=60800), loss 32.939034\n",
            "----\n",
            " hmuntextooo’s at withingy was and been his if a dinding vablent, mfforke, thererisened an verention, in conestonkouth and Lonked that wold gaaked tifirgess be readcention crlaotur, her of —icher unche \n",
            "----\n",
            "iter 4000 (p=64000), loss 32.352285\n",
            "iter 4200 (p=67200), loss 31.960427\n",
            "iter 4400 (p=70400), loss 31.515880\n",
            "iter 4600 (p=73600), loss 31.313462\n",
            "iter 4800 (p=76800), loss 30.878191\n",
            "----\n",
            " s 2o herieged sow uptore let\n",
            "vero was neer anm is but was I care that sort emerseling how lige and it vish ghem, then, for teln ow\n",
            "muse to she\n",
            "leotions. Dohitk\n",
            "of his had yom.”\n",
            "\n",
            "“I chat now nishr ett  \n",
            "----\n",
            "iter 5000 (p=80000), loss 30.289346\n",
            "iter 5200 (p=83200), loss 30.471804\n",
            "iter 5400 (p=86400), loss 30.633493\n",
            "iter 5600 (p=89600), loss 30.306697\n",
            "iter 5800 (p=92800), loss 30.004531\n",
            "----\n",
            " itwut staket kre of shisther surm hordain’s sime: I sinen conse sire line.”\n",
            "\n",
            "THelk said seer Mr. Casaubon’s svertainth-indon’s light had stought.\n",
            "\n",
            "Dorothea’s was now ort denstage in\n",
            "bidargitters shous \n",
            "----\n",
            "iter 6000 (p=96000), loss 29.687400\n",
            "iter 6200 (p=99200), loss 29.514785\n",
            "iter 6400 (p=102400), loss 29.725262\n",
            "iter 6600 (p=105600), loss 30.046677\n",
            "iter 6800 (p=108800), loss 29.949601\n",
            "----\n",
            " ut Dolut man a mards M\n",
            "spall. Pinder, aid snely spable tayon’s\n",
            "all the edwuration, she blweited\n",
            "spoole sought Mr. Casaubon is simpled. That your some to she appentios\n",
            "ond see Oat a tait. I have mar de \n",
            "----\n",
            "iter 7000 (p=112000), loss 29.476597\n",
            "iter 7200 (p=115200), loss 29.276300\n",
            "iter 7400 (p=118400), loss 29.320691\n",
            "iter 7600 (p=121600), loss 29.286223\n",
            "iter 7800 (p=124800), loss 29.041613\n",
            "----\n",
            " e the of usbor\n",
            "whon he not it\n",
            "he hapuring bitplace but thim I fers what very sort surt clanks rlC-id! Teing chaty as, it fas shaint his\n",
            "min! that and did errainds to commons to\n",
            "than leev’s of\n",
            "Tenns; h \n",
            "----\n",
            "iter 8000 (p=128000), loss 29.089752\n",
            "iter 8200 (p=131200), loss 28.943178\n",
            "iter 8400 (p=134400), loss 28.878840\n",
            "iter 8600 (p=137600), loss 28.678930\n",
            "iter 8800 (p=140800), loss 28.343609\n",
            "----\n",
            " p bo\n",
            "hap a senituseal a wech is prespuseg but quield to my him Grlace Sir James.\n",
            "\n",
            "“He and Dogod”\n",
            "was those to not much not with mike; you marralusion foo news—the\n",
            "lrot—etialy. And\n",
            "as dindrace.”\n",
            "\n",
            "“Phan \n",
            "----\n",
            "iter 9000 (p=144000), loss 28.050378\n",
            "iter 9200 (p=147200), loss 28.147940\n",
            "iter 9400 (p=150400), loss 28.357804\n",
            "iter 9600 (p=153600), loss 28.128593\n",
            "iter 9800 (p=156800), loss 27.962259\n",
            "----\n",
            " . Nosed, so thouse comthhing to the groon of muncecely murter looness on had not hee, as meread, whisere beAt wishopt sovires, the marious picleshoones fow they, with had\n",
            "asped thom the d-figrame comp \n",
            "----\n",
            "iter 10000 (p=160000), loss 27.854655\n",
            "iter 10200 (p=163200), loss 27.824516\n",
            "iter 10400 (p=166400), loss 27.778707\n",
            "iter 10600 (p=169600), loss 27.897094\n",
            "iter 10800 (p=172800), loss 28.301443\n",
            "----\n",
            " om of the gaothess to. That as not stirmiaver intion, you law, pain which heredverorne empone\n",
            "still lood, as dis ver. The trothere: to whinisg grand\n",
            "ymas on the nottineas of of he id\n",
            "myEletmes in homi \n",
            "----\n",
            "iter 11000 (p=176000), loss 28.206325\n",
            "iter 11200 (p=179200), loss 27.996160\n",
            "iter 11400 (p=182400), loss 27.852283\n",
            "iter 11600 (p=185600), loss 27.983052\n",
            "iter 11800 (p=188800), loss 28.036137\n",
            "----\n",
            " l my am like cunverse of she mysed well but ser soveling and omen a relested the petting ton what the firm stays some that shoul  in the rather, he will convrpingrange.”\n",
            "\n",
            "““zand of\n",
            "Mr.\n",
            "Bnowa of\n",
            "am\n",
            "sor \n",
            "----\n",
            "iter 12000 (p=192000), loss 28.002306\n",
            "iter 12200 (p=195200), loss 27.970926\n",
            "iter 12400 (p=198400), loss 28.331145\n",
            "iter 12600 (p=201600), loss 28.239190\n",
            "iter 12800 (p=204800), loss 28.274293\n",
            "----\n",
            " . Itconvess the colling to fitad the read or him, who heane is is you\n",
            "for gind Mry. “Ald own.”\n",
            "\n",
            "“It replecs.”\n",
            "\n",
            "“I she leagt,” said Chetty; got upsent?’\n",
            "\n",
            "“He all yot un anotely for is the the partt yoo \n",
            "----\n",
            "iter 13000 (p=208000), loss 28.326895\n",
            "iter 13200 (p=211200), loss 28.134208\n",
            "iter 13400 (p=214400), loss 28.231764\n",
            "iter 13600 (p=217600), loss 28.379157\n",
            "iter 13800 (p=220800), loss 28.374403\n",
            "----\n",
            " he betterses, and used anger bien for have stenise of ynu tem,” said\n",
            "Mr. Faydoly feemys; make someare, gaally buse musiendy, I alled beel, I hould, Mrs. Heam and\n",
            "say adout of say Casaubon; inceresause \n",
            "----\n",
            "iter 14000 (p=224000), loss 28.490133\n",
            "iter 14200 (p=227200), loss 28.535523\n",
            "iter 14400 (p=230400), loss 28.484709\n",
            "iter 14600 (p=233600), loss 28.524818\n",
            "iter 14800 (p=236800), loss 28.314528\n",
            "----\n",
            " and mado your roth to abyout that,”’sald peyplance.”\n",
            "\n",
            "“Ros it me noth am a to wife whe not\n",
            "a\n",
            "gond pash\n",
            "cricks’s ofk it acher she geghed homem’s were! She wal?” said Rosabol.”\n",
            "\n",
            "“I’ is a too?’’s whoth f \n",
            "----\n",
            "iter 15000 (p=240000), loss 27.892892\n",
            "iter 15200 (p=243200), loss 27.760493\n",
            "iter 15400 (p=246400), loss 27.661761\n",
            "iter 15600 (p=249600), loss 27.444229\n",
            "iter 15800 (p=252800), loss 27.531759\n",
            "----\n",
            " bature tingugh an ol ward. Lyd and there a gight so elforyto say had excerties so fiel owht onfansed froaked clossurest-with his opproyor Courte, we\n",
            "hadde in littlousing should Reshare ashe\n",
            "with, if f \n",
            "----\n",
            "iter 16000 (p=256000), loss 27.569291\n",
            "iter 16200 (p=259200), loss 27.511752\n",
            "iter 16400 (p=262400), loss 27.356768\n",
            "iter 16600 (p=265600), loss 27.271835\n",
            "iter 16800 (p=268800), loss 27.062048\n",
            "----\n",
            " ,” he than the neveld as by is a cleas.\n",
            "\n",
            "Dory.”\n",
            "\n",
            "“There you to by doem not fely in murtively\n",
            "moal Rossmeat’s should have samenance, thes If immaseed;. Any core to careded sort, on succh a new, which o \n",
            "----\n",
            "iter 17000 (p=272000), loss 27.301564\n",
            "iter 17200 (p=275200), loss 27.224443\n",
            "iter 17400 (p=278400), loss 27.004187\n",
            "iter 17600 (p=281600), loss 26.879499\n",
            "iter 17800 (p=284800), loss 26.640786\n",
            "----\n",
            " ”\n",
            "\n",
            "“Wo pan to kpen’t his accouther this bsicause man.\n",
            "I then she want him she my dody gect home do bred any say with octered to lais.\n",
            "\n",
            "He touck we the profe,” said love is dod that is of Middyeminend  \n",
            "----\n",
            "iter 18000 (p=288000), loss 26.495674\n",
            "iter 18200 (p=291200), loss 26.777071\n",
            "iter 18400 (p=294400), loss 27.062527\n",
            "iter 18600 (p=297600), loss 27.299424\n",
            "iter 18800 (p=300800), loss 27.439163\n",
            "----\n",
            " mperestion to the sorom more as of mayes tell as stunt very carry Lydgate, a proved to distadiiness—witsom greature lhan the\n",
            "wraded\n",
            "mpallens brut\n",
            "should he call tire. He leve Ladged the mostad of you  \n",
            "----\n",
            "iter 19000 (p=304000), loss 27.670649\n",
            "iter 19200 (p=307200), loss 27.708666\n",
            "iter 19400 (p=310400), loss 27.732226\n",
            "iter 19600 (p=313600), loss 27.718217\n",
            "iter 19800 (p=316800), loss 27.636889\n",
            "----\n",
            " usheinine.”\n",
            "\n",
            "“Whritts hopuwnels and possion marrating\n",
            "for was never forst gone! You, and “She looded yeh hen to bride wilt care at looker, wh\n",
            "incescal\n",
            "made by lust in the moneasy wads in her ackuninat \n",
            "----\n",
            "iter 20000 (p=320000), loss 27.596112\n",
            "iter 20200 (p=323200), loss 27.476796\n",
            "iter 20400 (p=326400), loss 27.383554\n",
            "iter 20600 (p=329600), loss 27.335035\n",
            "iter 20800 (p=332800), loss 27.261737\n",
            "----\n",
            " nus sewe.”\n",
            "\n",
            "Dortvo sturn\n",
            "in\n",
            "dysher word plossicy way peasing filly less impaeiturable resaying of miall on of “Py_whise mus oclies were\n",
            "ourd heround flien of about that him. There prefes of the sould  \n",
            "----\n",
            "iter 21000 (p=336000), loss 27.194458\n",
            "iter 21200 (p=339200), loss 27.212181\n",
            "iter 21400 (p=342400), loss 26.725932\n",
            "iter 21600 (p=345600), loss 26.915779\n",
            "iter 21800 (p=348800), loss 26.715215\n",
            "----\n",
            " ersmend, and loous feilmest, have all the spaction of never subjedting the whom that felhor at her avele and ever plory are who heom with some. \n",
            " ‘on\n",
            "Hegred actions\n",
            "ald\n",
            "should happly: been lais, her o \n",
            "----\n",
            "iter 22000 (p=352000), loss 26.621052\n",
            "iter 22200 (p=355200), loss 26.792638\n",
            "iter 22400 (p=358400), loss 26.674842\n",
            "iter 22600 (p=361600), loss 26.726703\n",
            "iter 22800 (p=364800), loss 26.819291\n",
            "----\n",
            "  in this paintione,” sage it’s mairime. I did moy hand, he proleest. OD  MJ. Lydgate, in the bass my to po. them?”\n",
            "\n",
            "“Why alwers?” too you cill shayingse, and. I dickine rather had insintrniciled to qu \n",
            "----\n",
            "iter 23000 (p=368000), loss 26.803648\n",
            "iter 23200 (p=371200), loss 26.760384\n",
            "iter 23400 (p=374400), loss 26.675405\n",
            "iter 23600 (p=377600), loss 26.302674\n",
            "iter 23800 (p=380800), loss 26.180425\n",
            "----\n",
            "  those obversing of his greet him brttry be\n",
            "samet is vationude of the sturmen fron to the\n",
            "\n",
            "oen with the plost eabout eathop to herself, droper\n",
            "to bifte, if which arrate for semal drlandal of sare dear \n",
            "----\n",
            "iter 24000 (p=384000), loss 26.365506\n",
            "iter 24200 (p=387200), loss 26.574288\n",
            "iter 24400 (p=390400), loss 26.605924\n",
            "iter 24600 (p=393600), loss 26.343182\n",
            "iter 24800 (p=396800), loss 26.193979\n",
            "----\n",
            " s juctions marry are a lary of the been strugpy going,\n",
            "the clergaressiinscina-uctationous evine\n",
            "moratiel beponace was a guct the\n",
            "dite harty were stomes agains. Mr. Hadwaining with a poor just of well  \n",
            "----\n",
            "iter 25000 (p=400000), loss 26.895011\n",
            "iter 25200 (p=403200), loss 27.241848\n",
            "iter 25400 (p=406400), loss 27.261827\n",
            "iter 25600 (p=409600), loss 27.514942\n",
            "iter 25800 (p=412800), loss 27.437397\n",
            "----\n",
            " the\n",
            "beer, sullily a lotion taoure is squenion\n",
            "or wratient of the sepemed\n",
            "enow the speriations foed even courses from the\n",
            "sorementsser people in\n",
            "the prive to dr! Hecthay bare explay\n",
            "othoriod retien sof \n",
            "----\n",
            "iter 26000 (p=416000), loss 27.245972\n",
            "iter 26200 (p=419200), loss 27.050732\n",
            "iter 26400 (p=422400), loss 26.905840\n",
            "iter 26600 (p=425600), loss 26.602120\n",
            "iter 26800 (p=428800), loss 26.622663\n",
            "----\n",
            " ll this mray, to whel seem no not a more lettles; mutheating the riMble nopor belired partive to his shoclas than lead could he the\n",
            "was areminity to the forder juce. But from-proprusts had been explet \n",
            "----\n",
            "iter 27000 (p=432000), loss 26.509226\n",
            "iter 27200 (p=435200), loss 26.191104\n",
            "iter 27400 (p=438400), loss 26.243211\n",
            "iter 27600 (p=441600), loss 25.859117\n",
            "iter 27800 (p=444800), loss 25.743285\n",
            "----\n",
            " t, wer sepersed him befores pable one combuogate.\n",
            "\n",
            "Mr. Casaubonge, Me.” Bulst own that she said not his epplees\n",
            "of nears in to highs mure forsed out beenn to him, or some regming. In hearding the\n",
            "word \n",
            "----\n",
            "iter 28000 (p=448000), loss 25.634378\n",
            "iter 28200 (p=451200), loss 25.959471\n",
            "iter 28400 (p=454400), loss 26.046001\n",
            "iter 28600 (p=457600), loss 25.974680\n",
            "iter 28800 (p=460800), loss 25.906149\n",
            "----\n",
            " o that at compured it will jose “brough judinesced to make\n",
            "the\n",
            "own with her own\n",
            "save my  hark not as acchust nothes, anclethant’s here the flete. Nouman of tahas tallicaging the foace thock than “them \n",
            "----\n",
            "iter 29000 (p=464000), loss 25.860049\n",
            "iter 29200 (p=467200), loss 25.647361\n",
            "iter 29400 (p=470400), loss 25.521906\n",
            "iter 29600 (p=473600), loss 25.409501\n",
            "iter 29800 (p=476800), loss 25.060917\n",
            "----\n",
            " t his stust on you know who had a\n",
            "morre excopted at as Mrs. Casaubon, and even bennge of find a vigrention only.\n",
            "With morning with the simite enferetures oppuspects. “Wh chill you hading the consectat \n",
            "----\n",
            "iter 30000 (p=480000), loss 24.725577\n",
            "iter 30200 (p=483200), loss 25.297295\n",
            "iter 30400 (p=486400), loss 25.515459\n",
            "iter 30600 (p=489600), loss 25.642294\n",
            "iter 30800 (p=492800), loss 25.724988\n",
            "----\n",
            " th popies of more of his hife farst struld I am his beride: bettance him\n",
            "not an the cleadlity he\n",
            "coall lifenthally a little’s busiody; have a conversacoly had glet\n",
            "for the swaking!” said Notmand is mi \n",
            "----\n",
            "iter 31000 (p=496000), loss 25.871464\n",
            "iter 31200 (p=499200), loss 26.058973\n",
            "iter 31400 (p=502400), loss 26.210913\n",
            "iter 31600 (p=505600), loss 26.139808\n",
            "iter 31800 (p=508800), loss 26.028884\n",
            "----\n",
            "  but the mosmint to dising and\n",
            "\n",
            "uscired that I was an\n",
            "would calling mystacking denestiling thapeard\n",
            "considatiols inthibled has to precede, and an ifution,\n",
            "deppocies incempacted and sentpit.\n",
            "\n",
            "“If a ray \n",
            "----\n",
            "iter 32000 (p=512000), loss 25.837490\n",
            "iter 32200 (p=515200), loss 25.830920\n",
            "iter 32400 (p=518400), loss 26.297086\n",
            "iter 32600 (p=521600), loss 26.168718\n",
            "iter 32800 (p=524800), loss 26.055153\n",
            "----\n",
            " his eally life,” said a dere of her think they was all see at his time, dinching lest quest out of the praceinys “focty’s, that there’s howes,” said Lasam. But have made inded\n",
            "actadone. But he will th \n",
            "----\n",
            "iter 33000 (p=528000), loss 25.918082\n",
            "iter 33200 (p=531200), loss 26.076542\n",
            "iter 33400 (p=534400), loss 26.251660\n",
            "iter 33600 (p=537600), loss 25.862146\n",
            "iter 33800 (p=540800), loss 25.535032\n",
            "----\n",
            " u govers anten. Wourd up spett illided himself’h Gathers she was unon, in itme to be a words and are stay worth Paying, there fay alothant’s comberity. “Hu fendardy I wank were had nothoul amose thong \n",
            "----\n",
            "iter 34000 (p=544000), loss 25.442392\n",
            "iter 34200 (p=547200), loss 25.288731\n",
            "iter 34400 (p=550400), loss 25.665979\n",
            "iter 34600 (p=553600), loss 25.669010\n",
            "iter 34800 (p=556800), loss 25.617775\n",
            "----\n",
            " al work Mrs. Will noce and a cerratmed of ponion, she not\n",
            "beary of refew of know partaghts which meant was not could beem, very for\n",
            "side very own or unonst becters other smeat’s drind might anwired ea \n",
            "----\n",
            "iter 35000 (p=560000), loss 25.652708\n",
            "iter 35200 (p=563200), loss 25.465964\n",
            "iter 35400 (p=566400), loss 25.397808\n",
            "iter 35600 (p=569600), loss 25.451403\n",
            "iter 35800 (p=572800), loss 25.635349\n",
            "----\n",
            " led.\n",
            "Fred?” Mary find-coust sheplet when.\n",
            "\n",
            "“You oulders” Rosamod’s dreagy be surmence, than’s nequed, if the short Mindledsered the ever noversmything wond obdershit,\n",
            "their was stunk pony fettmen.”\n",
            "\n",
            "\n",
            " \n",
            "----\n",
            "iter 36000 (p=576000), loss 25.489019\n",
            "iter 36200 (p=579200), loss 25.882402\n",
            "iter 36400 (p=582400), loss 26.040757\n",
            "iter 36600 (p=585600), loss 25.969319\n",
            "iter 36800 (p=588800), loss 25.692804\n",
            "----\n",
            "  shrone, usectanced with a more betoing aguid-could talks even to him elted even partimingivasts.\n",
            "\n",
            "She was\n",
            "shill he was not\n",
            "now in musive that which with all each) which Mr. Vincy. —o should-be of som \n",
            "----\n",
            "iter 37000 (p=592000), loss 25.600166\n",
            "iter 37200 (p=595200), loss 25.910401\n",
            "iter 37400 (p=598400), loss 25.883775\n",
            "iter 37600 (p=601600), loss 25.584963\n",
            "iter 37800 (p=604800), loss 25.361026\n",
            "----\n",
            " ng question. Ither. Mr.\n",
            "Casaubon may which\n",
            "her about him. Perhand have had abon itsols.”\n",
            "\n",
            "“Oh, indersaticale. “love of enivires to bear the\n",
            "rost other, say. “He pouse, on a\n",
            "quastering whencher in a si \n",
            "----\n",
            "iter 38000 (p=608000), loss 25.493838\n",
            "iter 38200 (p=611200), loss 25.235104\n",
            "iter 38400 (p=614400), loss 24.950206\n",
            "iter 38600 (p=617600), loss 24.790707\n",
            "iter 38800 (p=620800), loss 24.789691\n",
            "----\n",
            " n with make before begeftering\n",
            "excelsed are said, a to well by come tomencimom\n",
            "erections\n",
            "incentrence airtos and he like into tended a who hand,\n",
            "ittlieblly’s atclemaraed mennigative twory was a sewrets \n",
            "----\n",
            "iter 39000 (p=624000), loss 24.888011\n",
            "iter 39200 (p=627200), loss 24.567107\n",
            "iter 39400 (p=630400), loss 24.424202\n",
            "iter 39600 (p=633600), loss 24.253331\n",
            "iter 39800 (p=636800), loss 24.512525\n",
            "----\n",
            " ooks to meen her pantad of Mrs. Fded, !ust theidelve bother up a Loog; to be a lemirisent on deinly capppy, the asceations were so find laby, a not have a monion.”\n",
            "\n",
            "Mart Mr. Brooke?”\n",
            "\n",
            "“Write them sitt \n",
            "----\n",
            "iter 40000 (p=640000), loss 24.714255\n",
            "iter 40200 (p=643200), loss 24.895342\n",
            "iter 40400 (p=646400), loss 25.082406\n",
            "iter 40600 (p=649600), loss 25.277007\n",
            "iter 40800 (p=652800), loss 25.599154\n",
            "----\n",
            " \n",
            "away of Glardiry.\n",
            "“Dorothea: Soway, comper day must in the resent haven beet so\n",
            "dreveriged of thought\n",
            "be have herew each on came would be more, oud remard an was could, why be nored of oclitallent kn \n",
            "----\n",
            "iter 41000 (p=656000), loss 25.531368\n",
            "iter 41200 (p=659200), loss 25.487969\n",
            "iter 41400 (p=662400), loss 25.492909\n",
            "iter 41600 (p=665600), loss 25.920463\n",
            "iter 41800 (p=668800), loss 25.845079\n",
            "----\n",
            " n) he oppose and had always had\n",
            "cainty\n",
            "on the\n",
            "saccessous on?”\n",
            "\n",
            "“On were to be\n",
            "plead to the sense\n",
            "was to\n",
            "the right the untortent.\n",
            "\n",
            "“Chat not and\n",
            "plays you know, a deep othis you, and was lang hand, and \n",
            "----\n",
            "iter 42000 (p=672000), loss 25.689150\n",
            "iter 42200 (p=675200), loss 25.802488\n",
            "iter 42400 (p=678400), loss 25.557845\n",
            "iter 42600 (p=681600), loss 25.835725\n",
            "iter 42800 (p=684800), loss 25.713418\n",
            "----\n",
            " hings his lady\n",
            "pyorceef my prectation of else spene handracs have an sur-ictation toush he down anxaw this tenming\n",
            "that you, not on her in the san. Not by granf-en, Mrs. Lay Vincymoned without her uno \n",
            "----\n",
            "iter 43000 (p=688000), loss 25.608283\n",
            "iter 43200 (p=691200), loss 25.463974\n",
            "iter 43400 (p=694400), loss 25.066741\n",
            "iter 43600 (p=697600), loss 25.795812\n",
            "iter 43800 (p=700800), loss 25.852150\n",
            "----\n",
            "  a prosencious outing which wert roblijy goveriment. “Vere by, hed dnowedsel, what\n",
            "everything in imagerable a\n",
            "duckous time authed said with the sare the dwere—siding when scrantate\n",
            "and persond will re \n",
            "----\n",
            "iter 44000 (p=704000), loss 25.845498\n",
            "iter 44200 (p=707200), loss 25.700852\n",
            "iter 44400 (p=710400), loss 25.612758\n",
            "iter 44600 (p=713600), loss 25.579730\n",
            "iter 44800 (p=716800), loss 25.551815\n",
            "----\n",
            " ther was litter and\n",
            "dally up sidity by the poberning faving able met it well—hound to though\n",
            "serposst\n",
            "was to\n",
            "your would be alived a your driedd, Igard thems remonst as other\n",
            "has arrrast-cousin\n",
            "with th \n",
            "----\n",
            "iter 45000 (p=720000), loss 25.425073\n",
            "iter 45200 (p=723200), loss 25.597749\n",
            "iter 45400 (p=726400), loss 25.418014\n",
            "iter 45600 (p=729600), loss 25.329315\n",
            "iter 45800 (p=732800), loss 25.540848\n",
            "----\n",
            " ay ke; ersely me.\n",
            "A îadazing the up in the collitieting\n",
            "had treet above had . spent enter ade in all epoousan was relaged’nts, I shasan sho man gies. It we must to say yet himself a stretty. Yed; and  \n",
            "----\n",
            "iter 46000 (p=736000), loss 25.224358\n",
            "iter 46200 (p=739200), loss 25.250193\n",
            "iter 46400 (p=742400), loss 25.259245\n",
            "iter 46600 (p=745600), loss 25.366753\n",
            "iter 46800 (p=748800), loss 25.219276\n",
            "----\n",
            " s gredst, bured to reary me tout his perpartion bether acclicanting undlitay and bettely think. On a saves were a sease. of for his\n",
            "wimes. To\n",
            "yep, inLused as a\n",
            "lot\n",
            "han haw shill, and rate—a\n",
            "deen Gette \n",
            "----\n",
            "iter 47000 (p=752000), loss 25.262561\n",
            "iter 47200 (p=755200), loss 25.455188\n",
            "iter 47400 (p=758400), loss 25.965361\n",
            "iter 47600 (p=761600), loss 25.940710\n",
            "iter 47800 (p=764800), loss 25.599772\n",
            "----\n",
            " ted room an tamered her up first be agrooking, but his possing atsorhand Tono you away came ipquife” he is\n",
            "to common and inthate, and there _lox had been that the consait; the hand thing\n",
            "agreeates for \n",
            "----\n",
            "iter 48000 (p=768000), loss 25.500944\n",
            "iter 48200 (p=771200), loss 25.230826\n",
            "iter 48400 (p=774400), loss 24.818519\n",
            "iter 48600 (p=777600), loss 24.581910\n",
            "iter 48800 (p=780800), loss 24.250886\n",
            "----\n",
            " el, deal, it, yot some\n",
            "on so\n",
            "cance. Howeveing the intamined\n",
            "stains old come\n",
            "that horevide some for a quisnes forth\n",
            "not live himself when Mr. Casaubon, and perle shas\n",
            "resseveneter,” said Dorothea.\n",
            "\n",
            "Mr. \n",
            "----\n",
            "iter 49000 (p=784000), loss 24.167510\n",
            "iter 49200 (p=787200), loss 24.498079\n",
            "iter 49400 (p=790400), loss 24.433040\n",
            "iter 49600 (p=793600), loss 24.240471\n",
            "iter 49800 (p=796800), loss 24.195381\n",
            "----\n",
            " nd might for the coud mostlabory. In a pospirity\n",
            "willady painty should grosn on to come mann which to hives in shall impunving\n",
            "sono let nomered sortationp-face and expection, and her\n",
            "ismestative, turn \n",
            "----\n",
            "iter 50000 (p=800000), loss 24.243444\n",
            "iter 50200 (p=803200), loss 24.670149\n",
            "iter 50400 (p=806400), loss 24.600743\n",
            "iter 50600 (p=809600), loss 24.594563\n",
            "iter 50800 (p=812800), loss 24.894269\n",
            "----\n",
            " ous?”\n",
            "\n",
            "“You have been a subject faul the gainhing as that to accups. He would not will-hatves. “\n",
            "“IAghore severoum on asseable-as; he was pelincidel must feel must Leshind; the ear his\n",
            "and,\n",
            "what the p \n",
            "----\n",
            "iter 51000 (p=816000), loss 24.942992\n",
            "iter 51200 (p=819200), loss 24.845945\n",
            "iter 51400 (p=822400), loss 25.000615\n",
            "iter 51600 (p=825600), loss 24.914116\n",
            "iter 51800 (p=828800), loss 24.660814\n",
            "----\n",
            " t\n",
            "not to dusing bull was moder. He wishow.”\n",
            "\n",
            "“I do,”’s deel ad—radule, usans. I think rope. I love Will obserfullage considulibyts well, of lover other, troublish and Loded’t is thinger\n",
            "it,\n",
            "I did been \n",
            "----\n",
            "iter 52000 (p=832000), loss 24.205551\n",
            "iter 52200 (p=835200), loss 24.730332\n",
            "iter 52400 (p=838400), loss 25.208189\n",
            "iter 52600 (p=841600), loss 25.601830\n",
            "iter 52800 (p=844800), loss 26.107672\n",
            "----\n",
            " dopenish-because for rost thus,” said Mr. Brooke, and Hlaw could instive part-rays letterly, assief al imigatical naGring woman cape.\n",
            "\n",
            "A“low as sens it and made improds stort cheitians well by, he she \n",
            "----\n",
            "iter 53000 (p=848000), loss 26.033307\n",
            "iter 53200 (p=851200), loss 25.753993\n",
            "iter 53400 (p=854400), loss 25.637363\n",
            "iter 53600 (p=857600), loss 25.340388\n",
            "iter 53800 (p=860800), loss 25.261756\n",
            "----\n",
            " arugate fut when the\n",
            "Welling plesser,” shise\n",
            "was intofer.\n",
            "\n",
            "“Poblems, the tains, so my doque pause cirsin. Her :othe— ‘wod, indered my granfarthull, and you to find’s acconded bow,” said I’re own lures \n",
            "----\n",
            "iter 54000 (p=864000), loss 24.887630\n",
            "iter 54200 (p=867200), loss 24.833187\n",
            "iter 54400 (p=870400), loss 24.682678\n",
            "iter 54600 (p=873600), loss 24.568301\n",
            "iter 54800 (p=876800), loss 25.042434\n",
            "----\n",
            " Not: That nother. “Thims be to to the money,” said Doryy treativesem,” said Good Jotefuct withies as thon. I should\n",
            "favinut—of there way on the greaulay betrouctioned which the \n",
            "land,” said Fratmess.  \n",
            "----\n",
            "iter 55000 (p=880000), loss 25.290562\n",
            "iter 55200 (p=883200), loss 25.552569\n",
            "iter 55400 (p=886400), loss 25.678479\n",
            "iter 55600 (p=889600), loss 25.387692\n",
            "iter 55800 (p=892800), loss 25.122507\n",
            "----\n",
            " e him. He pellems lands her conkence me. Glithted to the simphing\n",
            "it the scoris\n",
            "wourd open sont of wastrancing visetyly, could nat him within which his visitsing him or whisseing elstritt, and visits  \n",
            "----\n",
            "iter 56000 (p=896000), loss 25.116987\n",
            "iter 56200 (p=899200), loss 25.206614\n",
            "iter 56400 (p=902400), loss 25.082312\n",
            "iter 56600 (p=905600), loss 25.203717\n",
            "iter 56800 (p=908800), loss 25.029940\n",
            "----\n",
            "  pailions, if us and growwerens, eyes him thought being, Fred\n",
            "lay\n",
            "distraituble of liven and solf would plara? Lowest her uneven, into sultly. But had long allow, and trat the coctituline of a clea in  \n",
            "----\n",
            "iter 57000 (p=912000), loss 25.015983\n",
            "iter 57200 (p=915200), loss 24.920281\n",
            "iter 57400 (p=918400), loss 24.398482\n",
            "iter 57600 (p=921600), loss 24.362484\n",
            "iter 57800 (p=924800), loss 24.501222\n",
            "----\n",
            " h a manxand of things be a for a nave to ser marriage my passisal of weilhably from Dorothea, bear house but aguing vit to do the feor have nead. On the own his from the work,” That she was a crite\n",
            "th \n",
            "----\n",
            "iter 58000 (p=928000), loss 24.361881\n",
            "iter 58200 (p=931200), loss 24.050121\n",
            "iter 58400 (p=934400), loss 24.207482\n",
            "iter 58600 (p=937600), loss 24.472933\n",
            "iter 58800 (p=940800), loss 24.821384\n",
            "----\n",
            " it could not askant of I shach the pipted as imparessions acfor everybods, a Whopt he fampabience. Should redipness becilline with his submit the stocking awate the Mistorcention? But with Mr. Haif to \n",
            "----\n",
            "iter 59000 (p=944000), loss 25.085464\n",
            "iter 59200 (p=947200), loss 25.164861\n",
            "iter 59400 (p=950400), loss 25.297031\n",
            "iter 59600 (p=953600), loss 25.525137\n",
            "iter 59800 (p=956800), loss 25.513021\n",
            "----\n",
            " to been\n",
            "will bering, and by a pairse of lover in the parting umodench, pockeving carnering trone atsons, and the other ‘èit, but he drawry that he is neich sit young to fother he had man he ince” the\n",
            " \n",
            "----\n",
            "iter 60000 (p=960000), loss 25.292994\n",
            "iter 60200 (p=963200), loss 25.372987\n",
            "iter 60400 (p=966400), loss 25.260699\n",
            "iter 60600 (p=969600), loss 25.179988\n",
            "iter 60800 (p=972800), loss 25.152595\n",
            "----\n",
            "  briaking\n",
            "unor, and decked mearocely things\n",
            "lengFy.”\n",
            "\n",
            "The wrile of highor.\n",
            "\n",
            "Mr. Troun meginatuners, and eruscuse other to way with the\n",
            "Wime; he said, had blouginefy way on high handsary; “Ifwied, and  \n",
            "----\n",
            "iter 61000 (p=976000), loss 24.964888\n",
            "iter 61200 (p=979200), loss 24.875826\n",
            "iter 61400 (p=982400), loss 25.125950\n",
            "iter 61600 (p=985600), loss 25.166858\n",
            "iter 61800 (p=988800), loss 25.151890\n",
            "----\n",
            " . For mising as for the bully, and small looking upon Roggt to he dawly: I speak which from conquites in her must hand-counted to bore my dimwers-compo. Gor kneppering infeture to tark, and bright, bl \n",
            "----\n",
            "iter 62000 (p=992000), loss 24.781803\n",
            "iter 62200 (p=995200), loss 24.562666\n",
            "iter 62400 (p=998400), loss 24.502691\n",
            "iter 62600 (p=1001600), loss 24.833193\n",
            "iter 62800 (p=1004800), loss 24.686985\n",
            "----\n",
            " fulable fellown goings ef doecas hands the meems in pospituous as he charbers of encered hels to be hundsing had along and not in Romgh-Clain had bood was not a monn, frame. It was blunding when Mr. C \n",
            "----\n",
            "iter 63000 (p=1008000), loss 24.509146\n",
            "iter 63200 (p=1011200), loss 24.396058\n",
            "iter 63400 (p=1014400), loss 24.297545\n",
            "iter 63600 (p=1017600), loss 24.418864\n",
            "iter 63800 (p=1020800), loss 24.372258\n",
            "----\n",
            " unsiret on the real in\n",
            "the chnow looked bad in RamNt said that she was got that he will be tempter, in will be she would be given fown he come at yeeng skection at a\n",
            "ham take: Dight Brooke on retunage \n",
            "----\n",
            "iter 64000 (p=1024000), loss 24.440289\n",
            "iter 64200 (p=1027200), loss 24.180075\n",
            "iter 64400 (p=1030400), loss 24.122590\n",
            "iter 64600 (p=1033600), loss 24.278942\n",
            "iter 64800 (p=1036800), loss 23.903754\n",
            "----\n",
            " yence withous.\n",
            "\n",
            "“To lot a little. Whell hay’d\n",
            "money in may mading wise reneival was one to come fortage\n",
            "he did do me about the coused pause, awned to a way to him. He had at busext fasilistions: at Mr \n",
            "----\n",
            "iter 65000 (p=1040000), loss 23.790005\n",
            "iter 65200 (p=1043200), loss 23.686778\n",
            "iter 65400 (p=1046400), loss 23.423568\n",
            "iter 65600 (p=1049600), loss 23.579420\n",
            "iter 65800 (p=1052800), loss 23.642446\n",
            "----\n",
            " to love borfitumen what Gearsusing passing with mims,” Casaubon’s which Ladislaw—I can\n",
            "formis them hovered Lrydare, in the spictic gring of you sand fferred\n",
            "eary where he was correct of\n",
            "ditter done, h \n",
            "----\n",
            "iter 66000 (p=1056000), loss 23.836904\n",
            "iter 66200 (p=1059200), loss 23.882498\n",
            "iter 66400 (p=1062400), loss 24.103661\n",
            "iter 66600 (p=1065600), loss 24.239119\n",
            "iter 66800 (p=1068800), loss 24.211216\n",
            "----\n",
            " ter, velled observiness from it was a viryandary at persond of lively of the firmor; himminance, and Migdne belief have given inshilland, silting forewhose, “what which simply\n",
            "elsedvenally and lode ne \n",
            "----\n",
            "iter 67000 (p=1072000), loss 24.480121\n",
            "iter 67200 (p=1075200), loss 24.731872\n",
            "iter 67400 (p=1078400), loss 24.880574\n",
            "iter 67600 (p=1081600), loss 24.677832\n",
            "iter 67800 (p=1084800), loss 24.657157\n",
            "----\n",
            " th her, and even\n",
            "on—what was turn with ryhand parievely was rugdeancy. It, as it as an ashing dion—its; “Yes in his sawgen: of Lidglate whine of discupt with Pary us arwad resome to expressity: from d \n",
            "----\n",
            "iter 68000 (p=1088000), loss 24.642533\n",
            "iter 68200 (p=1091200), loss 24.294037\n",
            "iter 68400 (p=1094400), loss 24.172913\n",
            "iter 68600 (p=1097600), loss 23.804524\n",
            "iter 68800 (p=1100800), loss 23.629946\n",
            "----\n",
            " upht a freadiwant\n",
            "with freems,\n",
            "there day, the Unit made any\n",
            "istinate pass, me that will to tarded tear to made diepon, as quite victic\n",
            "or with kinding propessionate cantions, marth at somethan\n",
            "the goo \n",
            "----\n",
            "iter 69000 (p=1104000), loss 23.749987\n",
            "iter 69200 (p=1107200), loss 24.018444\n",
            "iter 69400 (p=1110400), loss 24.273955\n",
            "iter 69600 (p=1113600), loss 24.512621\n",
            "iter 69800 (p=1116800), loss 24.546930\n",
            "----\n",
            "  him for\n",
            "some fertomen, which Dorothea! But the bistlough it the mistens dexestenss before that no propise what  tram—these and had agnadence was\n",
            "no med throw that I can kind had not to be obler sone  \n",
            "----\n",
            "iter 70000 (p=1120000), loss 24.639577\n",
            "iter 70200 (p=1123200), loss 24.559735\n",
            "iter 70400 (p=1126400), loss 24.660659\n",
            "iter 70600 (p=1129600), loss 24.735138\n",
            "iter 70800 (p=1132800), loss 25.880506\n",
            "----\n",
            " her think of sead to dosarded pausentary. Ie ever glass and right, he incenter, and allost’s diding as ullepenting\n",
            "who. No! when his woighted,\n",
            "which added\n",
            "nothing you own Chertage\n",
            "rical seciint is a h \n",
            "----\n",
            "iter 71000 (p=1136000), loss 25.410301\n",
            "iter 71200 (p=1139200), loss 25.118125\n",
            "iter 71400 (p=1142400), loss 24.822672\n",
            "iter 71600 (p=1145600), loss 24.702616\n",
            "iter 71800 (p=1148800), loss 24.338598\n",
            "----\n",
            " rly. I am sotements with left that at suhstiliedily large pride that the confince on the\n",
            "husing neighbors it is its pualing croshed of the ferval soul\n",
            "at you are had childing you lettle” Pettly pubme. \n",
            "----\n",
            "iter 72000 (p=1152000), loss 23.905300\n",
            "iter 72200 (p=1155200), loss 23.764158\n",
            "iter 72400 (p=1158400), loss 23.866009\n",
            "iter 72600 (p=1161600), loss 24.064837\n",
            "iter 72800 (p=1164800), loss 23.807685\n",
            "----\n",
            " he not any poid, and cupting the heiranable, whom everybook side\n",
            "of commanning you hope to? Dougnellyed muck other at those from the pretery mar. When, you had genty, when she well to say that me.”\n",
            "\n",
            "“ \n",
            "----\n",
            "iter 73000 (p=1168000), loss 23.913824\n",
            "iter 73200 (p=1171200), loss 24.365630\n",
            "iter 73400 (p=1174400), loss 24.659284\n",
            "iter 73600 (p=1177600), loss 24.898048\n",
            "iter 73800 (p=1180800), loss 25.127057\n",
            "----\n",
            " tinsal of remaking I’ve one\n",
            "quesgion for all, you know, “Thy man my gevener.\n",
            "I think inward his a minces on she drean and most the just oddered to one longer?”\n",
            "\n",
            "“Not: Gave\n",
            "a’d it.\n",
            "\n",
            "Mr. Suloure; and fr \n",
            "----\n",
            "iter 74000 (p=1184000), loss 24.824670\n",
            "iter 74200 (p=1187200), loss 25.318336\n",
            "iter 74400 (p=1190400), loss 25.002116\n",
            "iter 74600 (p=1193600), loss 24.591727\n",
            "iter 74800 (p=1196800), loss 24.178810\n",
            "----\n",
            "  pointly carend-poor knowlied a looms; and through judicule to “preputate both his hapial alworty said that it book mearion.\n",
            "For words away. It a comfistently considism a touther ov ochin.\n",
            "\n",
            "“I’ll ares \n",
            "----\n",
            "iter 75000 (p=1200000), loss 24.226697\n",
            "iter 75200 (p=1203200), loss 24.011833\n",
            "iter 75400 (p=1206400), loss 23.910475\n",
            "iter 75600 (p=1209600), loss 24.450554\n",
            "iter 75800 (p=1212800), loss 24.583780\n",
            "----\n",
            " y ghthis his living\n",
            "this was seed, Lydgate, but in there assual timal. If I widls, and Perormed-chood was any. “When Mrs. Casaubon his money with to be of” at upying very\n",
            "supporsable getting a sermon! \n",
            "----\n",
            "iter 76000 (p=1216000), loss 24.274438\n",
            "iter 76200 (p=1219200), loss 24.146696\n",
            "iter 76400 (p=1222400), loss 23.730924\n",
            "iter 76600 (p=1225600), loss 23.548435\n",
            "iter 76800 (p=1228800), loss 23.157528\n",
            "----\n",
            " on.\n",
            "\n",
            "“I laggen—looking for moter having empace-onear reformination with the passed which seemed to exclusen ablen.”\n",
            "But deend to Holicine tame remaod at spoke, troub! and maaret had belie awient of he \n",
            "----\n",
            "iter 77000 (p=1232000), loss 23.545778\n",
            "iter 77200 (p=1235200), loss 23.624549\n",
            "iter 77400 (p=1238400), loss 23.677306\n",
            "iter 77600 (p=1241600), loss 23.466697\n",
            "iter 77800 (p=1244800), loss 23.512025\n",
            "----\n",
            " detts and soti-ffere ownedsning. We are husband of himseffle considing find seque had. I mont of anoub, I shall marth this, with his one and foun-notic inmoned betternill Stom Mr. Bulstrode, and dense \n",
            "----\n",
            "iter 78000 (p=1248000), loss 23.713955\n",
            "iter 78200 (p=1251200), loss 23.681876\n",
            "iter 78400 (p=1254400), loss 23.737065\n",
            "iter 78600 (p=1257600), loss 23.669009\n",
            "iter 78800 (p=1260800), loss 23.722820\n",
            "----\n",
            "  must not regarding wattence\n",
            "as he had son away\n",
            "would have as the weak to called not to\n",
            "having to him lay. I did not because—meed which as Toman way traing as take “ Pony uping to file, will deding at \n",
            "----\n",
            "iter 79000 (p=1264000), loss 23.786065\n",
            "iter 79200 (p=1267200), loss 23.531320\n",
            "iter 79400 (p=1270400), loss 23.374269\n",
            "iter 79600 (p=1273600), loss 23.310132\n",
            "iter 79800 (p=1276800), loss 23.246591\n",
            "----\n",
            " he feeling woth it was\n",
            "other pondisting rears to fiver\n",
            "of their\n",
            "by strudge and recass, it with\n",
            "acponswically as if I was remalk give, looking as it side\n",
            "to all clemmene think light of feeled no might  \n",
            "----\n",
            "iter 80000 (p=1280000), loss 23.746202\n",
            "iter 80200 (p=1283200), loss 23.729473\n",
            "iter 80400 (p=1286400), loss 24.407594\n",
            "iter 80600 (p=1289600), loss 25.158377\n",
            "iter 80800 (p=1292800), loss 25.110893\n",
            "----\n",
            " ape Parity to this pays, as a granve by reasorted with his histording; the trie—a came into jud-cat hore from the talced by looks and washee. If the came\n",
            "Fred was things Was a cover.”\n",
            "\n",
            "Where, who woul \n",
            "----\n",
            "iter 81000 (p=1296000), loss 25.236879\n",
            "iter 81200 (p=1299200), loss 25.237177\n",
            "iter 81400 (p=1302400), loss 25.177876\n",
            "iter 81600 (p=1305600), loss 24.939825\n",
            "iter 81800 (p=1308800), loss 24.915718\n",
            "----\n",
            " erton-yes bacived without think the loodfue of huvely the puncten without the …inancer. “You to\n",
            "be sub3uted much the room of\n",
            "consord two\n",
            "year what with pincted of his better: it, “Presentes on the new \n",
            "----\n",
            "iter 82000 (p=1312000), loss 25.075120\n",
            "iter 82200 (p=1315200), loss 24.884790\n",
            "iter 82400 (p=1318400), loss 24.891794\n",
            "iter 82600 (p=1321600), loss 24.788249\n",
            "iter 82800 (p=1324800), loss 24.465912\n",
            "----\n",
            "  an.\n",
            "\n",
            "The acture cross plain\n",
            "had mean\n",
            "simple truscent to\n",
            "the which marry\n",
            "loos of his meedingly exacter which ofdee as he said as been one but\n",
            "pare,, wife: there\n",
            "intine’s\n",
            "dow sulpory\n",
            "of distonss. He\n",
            "sa \n",
            "----\n",
            "iter 83000 (p=1328000), loss 24.145670\n",
            "iter 83200 (p=1331200), loss 24.017696\n",
            "iter 83400 (p=1334400), loss 23.771447\n",
            "iter 83600 (p=1337600), loss 23.700278\n",
            "iter 83800 (p=1340800), loss 23.674013\n",
            "----\n",
            "  he\n",
            "well down an\n",
            "engen to wat’t mothem’s touse in its that said you are motait, as he accendized she mers herk and ons, it\n",
            "as was desirate abie my glaspermory empoannded. Whephing seemed too repeptyon \n",
            "----\n",
            "iter 84000 (p=1344000), loss 23.578867\n",
            "iter 84200 (p=1347200), loss 23.358605\n",
            "iter 84400 (p=1350400), loss 23.192461\n",
            "iter 84600 (p=1353600), loss 23.234178\n",
            "iter 84800 (p=1356800), loss 23.301000\n",
            "----\n",
            " he mesoriage should surjett him again. The own wanted the Garm stear look and his own husband to his frome and lioking\n",
            "him I shall a habinantion of young govet on be charse Many would need by the dest \n",
            "----\n",
            "iter 85000 (p=1360000), loss 23.341994\n",
            "iter 85200 (p=1363200), loss 23.281047\n",
            "iter 85400 (p=1366400), loss 23.386679\n",
            "iter 85600 (p=1369600), loss 23.358367\n",
            "iter 85800 (p=1372800), loss 23.274822\n",
            "----\n",
            " earrection his young stran tone a letter of them. In a wince-forture eainy for diffubat!”\n",
            " They sure drywald its now going that\n",
            "flowent, such\n",
            "him that Mon intent of almobents hign silfy trutther pind  \n",
            "----\n",
            "iter 86000 (p=1376000), loss 23.767115\n",
            "iter 86200 (p=1379200), loss 23.608265\n",
            "iter 86400 (p=1382400), loss 23.608092\n",
            "iter 86600 (p=1385600), loss 23.467403\n",
            "iter 86800 (p=1388800), loss 23.405377\n",
            "----\n",
            " house. He was call think not condected out as\n",
            "Mr. There his back six that the outher. But Mr. Lydithel or shoush to show as\n",
            "Rosamond. Mr. Lydgate, I think it above.\n",
            "\n",
            "“Do helh put you, fieness, absaid  \n",
            "----\n",
            "iter 87000 (p=1392000), loss 23.234222\n",
            "iter 87200 (p=1395200), loss 23.150518\n",
            "iter 87400 (p=1398400), loss 23.075785\n",
            "iter 87600 (p=1401600), loss 23.008060\n",
            "iter 87800 (p=1404800), loss 23.029860\n",
            "----\n",
            " it was prud-convidoncesing you would souch speese inturced that I knows on lowly. He contruminess sticked. She had to\n",
            "tenter.\n",
            "\n",
            "“I\n",
            "can had\n",
            "not fultice poscubral it othersing the fatuened the less Banth \n",
            "----\n",
            "iter 88000 (p=1408000), loss 23.078102\n",
            "iter 88200 (p=1411200), loss 23.218358\n",
            "iter 88400 (p=1414400), loss 23.342811\n",
            "iter 88600 (p=1417600), loss 23.251840\n",
            "iter 88800 (p=1420800), loss 23.334686\n",
            "----\n",
            " hat sumprise. “I havily in his sain by a\n",
            "disconderence, but that that Will him not on the thin looked. He was that you.\n",
            "\n",
            "My\n",
            "day heark’s wise of hiseling he fellowing rosp fiencwions, with hard he, and \n",
            "----\n",
            "iter 89000 (p=1424000), loss 23.542285\n",
            "iter 89200 (p=1427200), loss 23.652142\n",
            "iter 89400 (p=1430400), loss 23.686390\n",
            "iter 89600 (p=1433600), loss 23.444732\n",
            "iter 89800 (p=1436800), loss 23.209970\n",
            "----\n",
            " a mean af ople, one—that he can selvencely, he was in couss, but that mogite on to the\n",
            "canter at not gentratt’r money to have been with this just to go not.” Crebaterage when ergom his letter might be \n",
            "----\n",
            "iter 90000 (p=1440000), loss 23.226665\n",
            "iter 90200 (p=1443200), loss 23.437212\n",
            "iter 90400 (p=1446400), loss 23.335872\n",
            "iter 90600 (p=1449600), loss 23.460990\n",
            "iter 90800 (p=1452800), loss 23.474227\n",
            "----\n",
            " ng the oppropar had Rimbul, meneral his mother in my until\n",
            "place, with by disceffed\n",
            "with so profusing efgen rather farlt, mostance should an asperiation reasfulasing, but ever monsteand askite?”\n",
            "\n",
            "“Eld \n",
            "----\n",
            "iter 91000 (p=1456000), loss 23.306027\n",
            "iter 91200 (p=1459200), loss 23.487471\n",
            "iter 91400 (p=1462400), loss 23.586609\n",
            "iter 91600 (p=1465600), loss 23.546172\n",
            "iter 91800 (p=1468800), loss 23.528254\n",
            "----\n",
            " ver by his ouch ho revible at if an better to? I should\n",
            "not lure immelining sugght, though a\n",
            "bine on Mr.\n",
            "Hays he sense lust fous with his\n",
            "deepent in him trum, and he had confulnous—I say relieed he ha \n",
            "----\n",
            "iter 92000 (p=1472000), loss 23.346143\n",
            "iter 92200 (p=1475200), loss 23.151760\n",
            "iter 92400 (p=1478400), loss 23.026056\n",
            "iter 92600 (p=1481600), loss 22.853447\n",
            "iter 92800 (p=1484800), loss 22.913636\n",
            "----\n",
            " rt\n",
            "before\n",
            "his hank, but he would be within, thrien cortir insteal about the nueble where. I will because\n",
            "resubrets anvile that\n",
            "ance her confectafict of that an her, and that by where in a little to\n",
            "re \n",
            "----\n",
            "iter 93000 (p=1488000), loss 22.983147\n",
            "iter 93200 (p=1491200), loss 22.913856\n",
            "iter 93400 (p=1494400), loss 22.909289\n",
            "iter 93600 (p=1497600), loss 23.064654\n",
            "iter 93800 (p=1500800), loss 23.226781\n",
            "----\n",
            " lase for\n",
            "liking to Mary, while could not reflecten of\n",
            "condirial into concertage. It is been hiad now\n",
            "rewill a solends of ankive benings unertain for sation of looking to be.\n",
            "Fred, though of discomemen \n",
            "----\n",
            "iter 94000 (p=1504000), loss 23.137810\n",
            "iter 94200 (p=1507200), loss 23.134585\n",
            "iter 94400 (p=1510400), loss 23.143674\n",
            "iter 94600 (p=1513600), loss 23.230768\n",
            "iter 94800 (p=1516800), loss 23.425741\n",
            "----\n",
            " \n",
            "sort beting the good. Bulstrode were in retaid. I could be a good at his give before him in sadishood, nothing her to gave traccued him and near in the distand. “I limver much on the con, ross\n",
            "from h \n",
            "----\n",
            "iter 95000 (p=1520000), loss 23.162198\n",
            "iter 95200 (p=1523200), loss 23.049800\n",
            "iter 95400 (p=1526400), loss 23.410546\n",
            "iter 95600 (p=1529600), loss 23.815740\n",
            "iter 95800 (p=1532800), loss 23.480138\n",
            "----\n",
            " anlight to\n",
            "be possed to convinding intabout benegeagieal\n",
            "straving anything. He was no doeshing angter inquained upeas of than inconcelrenaty. She right was one who was drawing prome oldentick the lows \n",
            "----\n",
            "iter 96000 (p=1536000), loss 23.550820\n",
            "iter 96200 (p=1539200), loss 23.751349\n",
            "iter 96400 (p=1542400), loss 24.055632\n",
            "iter 96600 (p=1545600), loss 24.387064\n",
            "iter 96800 (p=1548800), loss 24.251375\n",
            "----\n",
            " ning to his remons obligation and beforemen as that by one moreom affect to acred Mr. Awance well agays on young for the strongednected\n",
            "him that he\n",
            "then’s senses,\n",
            "because\n",
            "by any sorned on\n",
            "the sabet to \n",
            "----\n",
            "iter 97000 (p=1552000), loss 24.143337\n",
            "iter 97200 (p=1555200), loss 24.298750\n",
            "iter 97400 (p=1558400), loss 24.208879\n",
            "iter 97600 (p=1561600), loss 23.901997\n",
            "iter 97800 (p=1564800), loss 23.662023\n",
            "----\n",
            " th his, wejling badned towarms\n",
            "the uneopary. A’lmed out there must for inquilion after some him to Calebingly, whelling veaking tell for up to cas a delicants; and\n",
            "sender. After? I fulling all which I \n",
            "----\n",
            "iter 98000 (p=1568000), loss 23.427799\n",
            "iter 98200 (p=1571200), loss 23.430798\n",
            "iter 98400 (p=1574400), loss 23.141455\n",
            "iter 98600 (p=1577600), loss 23.205075\n",
            "iter 98800 (p=1580800), loss 23.374802\n",
            "----\n",
            "  knowing that calling his\n",
            "glascome, as her contumbuition with her talkingly\n",
            "just the lightly the beirn weirt brot who had like light is by on\n",
            "its\n",
            "happined with (Mr. Lydgate, what would be taken of man \n",
            "----\n",
            "iter 99000 (p=1584000), loss 23.284649\n",
            "iter 99200 (p=1587200), loss 23.060254\n",
            "iter 99400 (p=1590400), loss 22.869641\n",
            "iter 99600 (p=1593600), loss 22.593555\n",
            "iter 99800 (p=1596800), loss 22.686014\n",
            "----\n",
            " y ob\n",
            "a companyth traised of Lydgate, she had deeplacist your enceres\n",
            "poor hermany towarrouslt\n",
            "concertained many bain, but she would make am on her mother had began against to she heart nourinane if he \n",
            "----\n",
            "iter 100000 (p=1600000), loss 22.901035\n",
            "iter 100200 (p=1603200), loss 23.027961\n",
            "iter 100400 (p=1606400), loss 22.992491\n",
            "iter 100600 (p=1609600), loss 22.843925\n",
            "iter 100800 (p=1612800), loss 22.538216\n",
            "----\n",
            "  then mind—bad was already warsaus of the letth a homany of his pather he must perpose\n",
            "the knowing he allowed on\n",
            "this Reables that Rosamond wintied to acquieted,—but the dry prijtles and thrhan the pr \n",
            "----\n",
            "iter 101000 (p=1616000), loss 22.329398\n",
            "iter 101200 (p=1619200), loss 22.549558\n",
            "iter 101400 (p=1622400), loss 22.393476\n",
            "iter 101600 (p=1625600), loss 22.190569\n",
            "iter 101800 (p=1628800), loss 22.246809\n",
            "----\n",
            " ing. And diptites of\n",
            "the\n",
            "be trust\n",
            "being enough it uncast him,” neither could not it?”\n",
            "\n",
            "“Lydgate! Dorothea. If that he had to thing they’nidg anxied objegied. And nisive his words, lonked a breat they  \n",
            "----\n",
            "iter 102000 (p=1632000), loss 22.107727\n",
            "iter 102200 (p=1635200), loss 21.944024\n",
            "iter 102400 (p=1638400), loss 22.024199\n",
            "iter 102600 (p=1641600), loss 22.281934\n",
            "iter 102800 (p=1644800), loss 22.386599\n",
            "----\n",
            " inst Bulstrode,” relic, being till—that our endory at the converdations from her handmed his\n",
            "positule had a first for her desard of\n",
            "his pootherance. I had gonewed\n",
            "that his house evening was that way i \n",
            "----\n",
            "iter 103000 (p=1648000), loss 22.531666\n",
            "iter 103200 (p=1651200), loss 22.550051\n",
            "iter 103400 (p=1654400), loss 22.455875\n",
            "iter 103600 (p=1657600), loss 22.580233\n",
            "iter 103800 (p=1660800), loss 22.822212\n",
            "----\n",
            " lfules—Pet noise used for Will. Agenayly ove accomtous persence loods in\n",
            "how Rosamond naght me called with a somethelips from hel that were at expectle. He had cemple to inceriate\n",
            "wit\n",
            "belom indard com \n",
            "----\n",
            "iter 104000 (p=1664000), loss 22.900439\n",
            "iter 104200 (p=1667200), loss 22.736506\n",
            "iter 104400 (p=1670400), loss 22.970099\n",
            "iter 104600 (p=1673600), loss 23.379168\n",
            "iter 104800 (p=1676800), loss 23.522357\n",
            "----\n",
            " ed to still beenient of taver,\n",
            "furm nothing is was going herself-blanted werrays in Bulstrode, by hurd for with the compence laided expression to have become\n",
            "Middling-indly as greet preferved in the\n",
            "n \n",
            "----\n",
            "iter 105000 (p=1680000), loss 23.614201\n",
            "iter 105200 (p=1683200), loss 24.393043\n",
            "iter 105400 (p=1686400), loss 23.902292\n",
            "iter 105600 (p=1689600), loss 23.566907\n",
            "iter 105800 (p=1692800), loss 23.095551\n",
            "----\n",
            "  sat her part. The has never you ad1ad and graymbustly used fateer have much her a\n",
            "straw, and in sharl worans seenething the lays,” she mover, and ‘Will buch might yeend everything to mershme to troug \n",
            "----\n",
            "iter 106000 (p=1696000), loss 22.820536\n",
            "iter 106200 (p=1699200), loss 22.784761\n",
            "iter 106400 (p=1702400), loss 22.457276\n",
            "iter 106600 (p=1705600), loss 22.542952\n",
            "iter 106800 (p=1708800), loss 22.624948\n",
            "----\n",
            " genes which have taking with the nefore, sustion visiccauge the grange prosoce texed a confulty buck\n",
            "in;\n",
            "Thrse becerness of Momarriake his wanted to her by,”\n",
            "said make she was at a pinent out her let  \n",
            "----\n",
            "iter 107000 (p=1712000), loss 22.675503\n",
            "iter 107200 (p=1715200), loss 22.883992\n",
            "iter 107400 (p=1718400), loss 23.028173\n",
            "iter 107600 (p=1721600), loss 22.736046\n",
            "iter 107800 (p=1724800), loss 22.476506\n",
            "----\n",
            " the mild in the wordd\n",
            "had\n",
            "achomed\n",
            "to be\n",
            "dould out out\n",
            "door each efftion of if it mean; but unhind he was therrise showed ence and short on the fereing the rram—the out, to posore the thrivante own in  \n",
            "----\n",
            "iter 108000 (p=1728000), loss 22.527352\n",
            "iter 108200 (p=1731200), loss 22.885037\n",
            "iter 108400 (p=1734400), loss 22.991002\n",
            "iter 108600 (p=1737600), loss 22.831766\n",
            "iter 108800 (p=1740800), loss 22.707706\n",
            "----\n",
            " In thinging more at her otnearly start persfeted him un\n",
            "infermiving the Bother But Lydgate vely, becare say fellow the seluradly\n",
            "all beal would to say to\n",
            "be all her\n",
            "uss, and accertable mater joom lean \n",
            "----\n",
            "iter 109000 (p=1744000), loss 22.954657\n",
            "iter 109200 (p=1747200), loss 22.694654\n",
            "iter 109400 (p=1750400), loss 22.729638\n",
            "iter 109600 (p=1753600), loss 22.943369\n",
            "iter 109800 (p=1756800), loss 23.207798\n",
            "----\n",
            " nds with her foldred as. He sopen——utther, “Oh,\n",
            "just each, wimance—not making coust on the brotheen\n",
            "tinatye what cauinged it. I have not pare that,\n",
            "I said, him. \n",
            "“Oh I well have a pollimy mine to hear \n",
            "----\n",
            "iter 110000 (p=1760000), loss 23.734321\n",
            "iter 110200 (p=1763200), loss 23.722971\n",
            "iter 110400 (p=1766400), loss 24.054627\n",
            "iter 110600 (p=1769600), loss 24.340816\n",
            "iter 110800 (p=1772800), loss 24.448879\n",
            "----\n",
            " good\n",
            "me, Signistibled and choderness, and to make the\n",
            "wordely made about that\n",
            "maval, and Lydgate, easy, but mer confisents, and Mrs. Lydgate; yes,” said Rosamond, she was before, that you dind, nith t \n",
            "----\n",
            "iter 111000 (p=1776000), loss 23.990837\n",
            "iter 111200 (p=1779200), loss 23.633165\n",
            "iter 111400 (p=1782400), loss 24.989017\n",
            "iter 111600 (p=1785600), loss 25.796980\n",
            "iter 111800 (p=1788800), loss 26.362736\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b66f60177c51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;31m# Forward seq_length characters through the RNN and fetch gradient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     (loss, dWf, dbf, dWi, dbi, dWcc, dbcc, dWo, dbo, dWy, dby,\n\u001b[0;32m--> 325\u001b[0;31m      hprev, cprev) = lossFun(inputs, targets, hprev, cprev)\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-b66f60177c51>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev, cprev)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Candidate cc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mccs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbcc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This step's h and c.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = open('input2.txt', 'r', encoding='UTF-8').read()\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMMUqN0_ZJFq",
        "outputId": "83a7eabb-cc1d-4866-dd00-c2b260a25d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many Theresas have been born who found for themselves no epic life wherein there was a constant unfolding of far-resonant action;\n",
            "perhaps only a life of mistakes, the offspring of a certain spiritual grandeur ill-matched with the meanness of opportunity;\n",
            "perhaps a tragic failure which found no sacred poet and sank unwept into oblivion.\n",
            "With dim lights and tangled circumstance they tried to shape their thought and deed in noble agreement;\n",
            "but after all, to common eyes their struggles seemed mere inconsistency and formlessness;\n",
            "for these later-born Theresas were helped by no coherent social faith and order which could perform the function of knowledge for the ardently willing soul.\n",
            "Their ardor alternated between a vague ideal and the common yearning of womanhood; so that the one was disapproved as extravagance, and the other condemned as a lapse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(\"data has %d characters, %d unique\" % (data_size, vocab_size))\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "print('char_to_idx', char_to_idx)\n",
        "print('idx_to_char', idx_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgZB7DJPZT3a",
        "outputId": "bbb268ec-19f4-404a-a2fb-792175d10023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 950 characters, 33 unique\n",
            "char_to_idx {'T': 0, 'l': 1, 'y': 2, 'k': 3, ' ': 4, ';': 5, 's': 6, 't': 7, 'S': 8, 'W': 9, 'm': 10, ',': 11, 'v': 12, '\\n': 13, 'x': 14, 'u': 15, 'i': 16, '.': 17, 'a': 18, 'g': 19, 'c': 20, 'b': 21, 'w': 22, 'f': 23, 'M': 24, 'd': 25, '-': 26, 'o': 27, 'h': 28, 'e': 29, 'r': 30, 'p': 31, 'n': 32}\n",
            "idx_to_char {0: 'T', 1: 'l', 2: 'y', 3: 'k', 4: ' ', 5: ';', 6: 's', 7: 't', 8: 'S', 9: 'W', 10: 'm', 11: ',', 12: 'v', 13: '\\n', 14: 'x', 15: 'u', 16: 'i', 17: '.', 18: 'a', 19: 'g', 20: 'c', 21: 'b', 22: 'w', 23: 'f', 24: 'M', 25: 'd', 26: '-', 27: 'o', 28: 'h', 29: 'e', 30: 'r', 31: 'p', 32: 'n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_LSTM_parameters (hidden_size, vocab_size):\n",
        "\n",
        "    Wf = np.random.randn(hidden_size, vocab_size+hidden_size) * 0.1\n",
        "    Wi = np.random.randn(hidden_size, vocab_size+hidden_size) * 0.1\n",
        "    Wg = np.random.randn(hidden_size, vocab_size+hidden_size) * 0.1\n",
        "    Wo = np.random.randn(hidden_size, vocab_size+hidden_size) * 0.1\n",
        "\n",
        "    bf = np.random.randn(hidden_size, 1) * 0.1 + 0.5\n",
        "    bi = np.random.randn(hidden_size, 1) * 0.1 + 0.5\n",
        "    bg = np.random.randn(hidden_size, 1) * 0.1\n",
        "    bo = np.random.randn(hidden_size, 1) * 0.1 + 0.5\n",
        "\n",
        "    # hidden -> output을 위한 Parameter\n",
        "\n",
        "    Wy = np.random.randn(vocab_size, hidden_size)  * 0.1\n",
        "    by = np.random.randn(vocab_size, 1)  * 0.1\n",
        "\n",
        "    return Wf, Wi, Wg, Wo, bf, bi, bg, bo, Wy, by\n",
        "\n",
        "\n",
        "def sigmoid (v):\n",
        "    return 1./(1. + np.exp(-v))"
      ],
      "metadata": {
        "id": "z5tQAb7mbwgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init Parameters\n",
        "hidden_size = 6\n",
        "Wf, Wi, Wg, Wo, bf, bi, bg, bo, Wy, by = make_LSTM_parameters(hidden_size, vocab_size)\n",
        "\n",
        "# forward pass\n",
        "# input_text = 'In mathematics and physics, a vector is an element of a vector space.'\n",
        "input_text = 'That Spanish woman who lived three hundred years ago,'\n",
        "xs, hs, zs, cs, ys, ps = {}, {}, {}, {}, {}, {}\n",
        "hs[-1] = hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "cs[-1] = cprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "\n",
        "print('-----------------------------')\n",
        "for t in range(len(input_text)):\n",
        "    ch = input_text[t]\n",
        "    print(str(t) +'th character is ' + ch + '\\n-----------------------------')\n",
        "\n",
        "    xs[t] = np.zeros((vocab_size, 1))\n",
        "    xs[t][char_to_idx[ch]] = 1\n",
        "    print('\\t' + ch +'\\'s one-hot encoding is')\n",
        "    print('\\t'+str(xs[t].T))      \n",
        "    print('\\t Let us denote this vector by xs['+ str(t) + ']\\n' )\n",
        "\n",
        "    print('\\t zs['+ str(t) + '] < - Concatinate (xs['+ str(t) + '], h['+ str(t-1) + '] ):' )\n",
        "    zs[t] = np.concatenate((xs[t], hs[t-1]), axis=0)\n",
        "    print('\\t'+str(zs[t].T))       \n",
        "\n",
        "    f_raw = np.dot(Wf, zs[t]) + bf\n",
        "    i_raw = np.dot(Wi, zs[t]) + bi\n",
        "    g_raw = np.dot(Wg, zs[t]) + bg\n",
        "    o_raw = np.dot(Wo, zs[t]) + bo\n",
        "\n",
        "    f = sigmoid(f_raw)\n",
        "    i = sigmoid(i_raw)\n",
        "    g = np.tanh(g_raw)\n",
        "    o = sigmoid(i_raw)\n",
        "\n",
        "\n",
        "    cs[t] = f*cs[t-1] + i*g\n",
        "    hs[t] = o*np.tanh(cs[t])\n",
        "\n",
        "    ys[t] = np.dot(Wy, hs[t]) + by\n",
        "    print('\\t softmax of (ys['+ str(t) + ']) =')\n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "    print('\\t\\t'+str(ps[t].T) + '\\n')    \n",
        "\n",
        "    print('-----------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDBeaPXxcJzC",
        "outputId": "a304cbe2-fd95-44c6-b9b6-cd2a7c3c3113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "0th character is T\n",
            "-----------------------------\n",
            "\tT's one-hot encoding is\n",
            "\t[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[0]\n",
            "\n",
            "\t zs[0] < - Concatinate (xs[0], h[-1] ):\n",
            "\t[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t softmax of (ys[0]) =\n",
            "\t\t[[0.027843   0.03387965 0.03230799 0.02600186 0.02970198 0.03080868\n",
            "  0.03238236 0.03176233 0.02766051 0.03024863 0.03006616 0.02959527\n",
            "  0.03437871 0.03499112 0.03581522 0.02985335 0.02601842 0.02955141\n",
            "  0.03165055 0.02550027 0.03255083 0.03006258 0.03269984 0.02891663\n",
            "  0.03311597 0.02576293 0.02602184 0.02796628 0.03032848 0.02960942\n",
            "  0.0320077  0.03298615 0.02795389]]\n",
            "\n",
            "-----------------------------\n",
            "1th character is h\n",
            "-----------------------------\n",
            "\th's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[1]\n",
            "\n",
            "\t zs[1] < - Concatinate (xs[1], h[0] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.07629211  0.0426529   0.08339242\n",
            "   0.08179285 -0.0167482   0.00342301]]\n",
            "\t softmax of (ys[1]) =\n",
            "\t\t[[0.02792678 0.03386057 0.03226638 0.02601127 0.02955429 0.03058724\n",
            "  0.03233243 0.03190283 0.02771926 0.0303651  0.0302454  0.02979094\n",
            "  0.0346473  0.0352559  0.03582158 0.02964687 0.0261164  0.02963936\n",
            "  0.03169821 0.02548632 0.03236664 0.03000918 0.03280716 0.02906492\n",
            "  0.0330106  0.02560436 0.02576239 0.02808042 0.03005655 0.02959937\n",
            "  0.03173606 0.03304917 0.02797876]]\n",
            "\n",
            "-----------------------------\n",
            "2th character is a\n",
            "-----------------------------\n",
            "\ta's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[2]\n",
            "\n",
            "\t zs[2] < - Concatinate (xs[2], h[1] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   1.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.07171446  0.03185055  0.07174784\n",
            "   0.07279602  0.0227237  -0.0194629 ]]\n",
            "\t softmax of (ys[2]) =\n",
            "\t\t[[0.02764462 0.03410539 0.03284936 0.02607857 0.02960323 0.03113335\n",
            "  0.03263759 0.03168462 0.02768808 0.03055884 0.03009711 0.02968653\n",
            "  0.03414896 0.03484348 0.03599545 0.03013845 0.02601139 0.02955249\n",
            "  0.03157222 0.025619   0.03278264 0.02990386 0.03245748 0.02843177\n",
            "  0.03303415 0.02546946 0.02601373 0.02751986 0.03002137 0.02964587\n",
            "  0.03222384 0.03291371 0.02793353]]\n",
            "\n",
            "-----------------------------\n",
            "3th character is t\n",
            "-----------------------------\n",
            "\tt's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[3]\n",
            "\n",
            "\t zs[3] < - Concatinate (xs[3], h[2] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          1.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.0816711  -0.0042859   0.11939964\n",
            "   0.08189059  0.01301851  0.03794348]]\n",
            "\t softmax of (ys[3]) =\n",
            "\t\t[[0.02735312 0.03407779 0.03259499 0.02596035 0.02918644 0.03056425\n",
            "  0.03240739 0.03223702 0.02769445 0.03012384 0.03019901 0.03028019\n",
            "  0.03385044 0.03488231 0.0360636  0.02992893 0.02620888 0.03013258\n",
            "  0.03144239 0.02573438 0.03241138 0.02977126 0.03300008 0.02863373\n",
            "  0.03332097 0.02541991 0.02571025 0.02760502 0.03009432 0.02971422\n",
            "  0.0322781  0.03275991 0.02835848]]\n",
            "\n",
            "-----------------------------\n",
            "4th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[4]\n",
            "\n",
            "\t zs[4] < - Concatinate (xs[4], h[3] ):\n",
            "\t[[0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.11408934 0.03624149 0.05505723\n",
            "  0.13349313 0.04394053 0.03821756]]\n",
            "\t softmax of (ys[4]) =\n",
            "\t\t[[0.02742343 0.03391836 0.03226025 0.0258282  0.02939994 0.03038873\n",
            "  0.03217328 0.03256619 0.02748156 0.02986705 0.02988788 0.03020172\n",
            "  0.0340949  0.03483774 0.03590332 0.02959177 0.02624226 0.03010537\n",
            "  0.03167486 0.02578702 0.03218042 0.02980012 0.03346544 0.02911835\n",
            "  0.03315466 0.02552554 0.02562058 0.02797333 0.03030765 0.02976386\n",
            "  0.03221112 0.03290243 0.02834265]]\n",
            "\n",
            "-----------------------------\n",
            "5th character is S\n",
            "-----------------------------\n",
            "\tS's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[5]\n",
            "\n",
            "\t zs[5] < - Concatinate (xs[5], h[4] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.12192105 0.09939048 0.05887901\n",
            "  0.14522655 0.03052819 0.00171748]]\n",
            "\t softmax of (ys[5]) =\n",
            "\t\t[[0.02721815 0.03428689 0.03258949 0.02607602 0.02924643 0.03094894\n",
            "  0.03249758 0.0319278  0.02779742 0.03031984 0.03002529 0.03031349\n",
            "  0.03368408 0.03441146 0.03600585 0.03000329 0.02614492 0.0301094\n",
            "  0.03139876 0.02603165 0.03259435 0.02992581 0.03297492 0.02846902\n",
            "  0.03326174 0.02530356 0.02603976 0.0273424  0.02999687 0.02966301\n",
            "  0.03236697 0.03271342 0.02831141]]\n",
            "\n",
            "-----------------------------\n",
            "6th character is p\n",
            "-----------------------------\n",
            "\tp's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "\t Let us denote this vector by xs[6]\n",
            "\n",
            "\t zs[6] < - Concatinate (xs[6], h[5] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.08237807 0.04387913 0.08735232\n",
            "  0.15023512 0.04387272 0.07147859]]\n",
            "\t softmax of (ys[6]) =\n",
            "\t\t[[0.027268   0.03401277 0.03239457 0.02587816 0.02921092 0.03038491\n",
            "  0.03243697 0.03237844 0.02776738 0.030137   0.0300653  0.0308067\n",
            "  0.0339988  0.0347561  0.03589073 0.02941151 0.02622569 0.0304024\n",
            "  0.03163913 0.0257625  0.03233279 0.02996227 0.03310612 0.02889897\n",
            "  0.03312407 0.02531642 0.02546166 0.02778217 0.02992793 0.02980681\n",
            "  0.03207128 0.03321773 0.02816381]]\n",
            "\n",
            "-----------------------------\n",
            "7th character is a\n",
            "-----------------------------\n",
            "\ta's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[7]\n",
            "\n",
            "\t zs[7] < - Concatinate (xs[7], h[6] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   1.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.0906107   0.04709443  0.05715573\n",
            "   0.18421615  0.06518378 -0.00620194]]\n",
            "\t softmax of (ys[7]) =\n",
            "\t\t[[0.02722585 0.03418973 0.03294445 0.02597923 0.02943621 0.03102906\n",
            "  0.03273423 0.03197768 0.02770951 0.03042921 0.02993311 0.0303444\n",
            "  0.03376308 0.03449468 0.03601383 0.02993939 0.02606167 0.03002442\n",
            "  0.0315793  0.02578165 0.03279159 0.02990353 0.03261283 0.02833928\n",
            "  0.03305741 0.02528696 0.02580613 0.0273479  0.02993571 0.02979744\n",
            "  0.03243981 0.03310609 0.02798462]]\n",
            "\n",
            "-----------------------------\n",
            "8th character is n\n",
            "-----------------------------\n",
            "\tn's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "\t Let us denote this vector by xs[8]\n",
            "\n",
            "\t zs[8] < - Concatinate (xs[8], h[7] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.09070339 0.0044591  0.11693894\n",
            "  0.15775222 0.03769085 0.04011742]]\n",
            "\t softmax of (ys[8]) =\n",
            "\t\t[[0.02740738 0.03413986 0.03272503 0.02597867 0.02906946 0.03045772\n",
            "  0.03235271 0.03251424 0.02764687 0.03036028 0.03027907 0.03041598\n",
            "  0.03419919 0.03510696 0.03612139 0.0297477  0.02635143 0.03014604\n",
            "  0.03156034 0.02586071 0.03219606 0.02958217 0.0332626  0.02875208\n",
            "  0.03303633 0.02508578 0.02539717 0.0276353  0.0296735  0.02972062\n",
            "  0.03204247 0.03276829 0.0284066 ]]\n",
            "\n",
            "-----------------------------\n",
            "9th character is i\n",
            "-----------------------------\n",
            "\ti's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[9]\n",
            "\n",
            "\t zs[9] < - Concatinate (xs[9], h[8] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.11724724 0.02934987 0.07053948\n",
            "  0.11447974 0.1052761  0.02007549]]\n",
            "\t softmax of (ys[9]) =\n",
            "\t\t[[0.02766431 0.03393072 0.0330073  0.02598776 0.02923145 0.03058059\n",
            "  0.03265499 0.03195226 0.02775588 0.03034486 0.03060684 0.02992982\n",
            "  0.03404173 0.03548685 0.03619378 0.03022178 0.02610617 0.02981101\n",
            "  0.03139619 0.0251965  0.03264177 0.02969168 0.03223556 0.02832643\n",
            "  0.03336521 0.02556617 0.02557809 0.02761113 0.0300397  0.02969862\n",
            "  0.03217786 0.03286309 0.0281039 ]]\n",
            "\n",
            "-----------------------------\n",
            "10th character is s\n",
            "-----------------------------\n",
            "\ts's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[10]\n",
            "\n",
            "\t zs[10] < - Concatinate (xs[10], h[9] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   1.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.1230002  -0.05351915  0.04989546\n",
            "   0.07414996  0.0264788   0.02129967]]\n",
            "\t softmax of (ys[10]) =\n",
            "\t\t[[0.02761713 0.03399384 0.03266232 0.02611192 0.02899812 0.03052921\n",
            "  0.03262661 0.03154163 0.02807357 0.03024846 0.03087546 0.03006792\n",
            "  0.03372004 0.03536119 0.03615869 0.03036142 0.026069   0.02999584\n",
            "  0.03108394 0.02521949 0.03269677 0.02992966 0.03207815 0.02822221\n",
            "  0.03379912 0.02575195 0.02591632 0.02750852 0.03018178 0.02956255\n",
            "  0.03213045 0.03263376 0.028273  ]]\n",
            "\n",
            "-----------------------------\n",
            "11th character is h\n",
            "-----------------------------\n",
            "\th's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[11]\n",
            "\n",
            "\t zs[11] < - Concatinate (xs[11], h[10] ):\n",
            "\t[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  9.54999599e-02 -5.54445655e-02 -4.46799043e-04\n",
            "   8.69803038e-02  5.75042877e-03  5.69406204e-02]]\n",
            "\t softmax of (ys[11]) =\n",
            "\t\t[[0.02783351 0.03390687 0.03248274 0.02607637 0.02912157 0.03038348\n",
            "  0.03246161 0.03178363 0.02795785 0.03035452 0.03077984 0.03002431\n",
            "  0.03428373 0.03556834 0.03604399 0.02997275 0.02615513 0.02987872\n",
            "  0.03134818 0.02527608 0.03243105 0.02990174 0.03242266 0.02865538\n",
            "  0.0334351  0.02561943 0.02567894 0.02783382 0.0299814  0.0295632\n",
            "  0.03178886 0.032808   0.02818717]]\n",
            "\n",
            "-----------------------------\n",
            "12th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[12]\n",
            "\n",
            "\t zs[12] < - Concatinate (xs[12], h[11] ):\n",
            "\t[[ 0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.08714179 -0.02973918  0.01676046\n",
            "   0.06739416  0.03562024  0.01084649]]\n",
            "\t softmax of (ys[12]) =\n",
            "\t\t[[0.02777573 0.03380697 0.03213099 0.02591907 0.0293699  0.03027804\n",
            "  0.03217328 0.03223815 0.02765378 0.03001612 0.0302651  0.02997407\n",
            "  0.03441889 0.03529108 0.0358767  0.02962481 0.02620899 0.02990278\n",
            "  0.03160894 0.02550449 0.03217166 0.02989822 0.03311846 0.02917377\n",
            "  0.03323225 0.02567546 0.02564455 0.02815178 0.03025094 0.02964088\n",
            "  0.03186145 0.03289439 0.0282483 ]]\n",
            "\n",
            "-----------------------------\n",
            "13th character is w\n",
            "-----------------------------\n",
            "\tw's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[13]\n",
            "\n",
            "\t zs[13] < - Concatinate (xs[13], h[12] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.10149335  0.06196938  0.03289367\n",
            "   0.09586613  0.02289561 -0.01484646]]\n",
            "\t softmax of (ys[13]) =\n",
            "\t\t[[0.02761339 0.03384273 0.03291338 0.02584452 0.02952464 0.03062834\n",
            "  0.03248792 0.03228765 0.02742476 0.03004437 0.03014267 0.0296747\n",
            "  0.03403973 0.03525567 0.0361221  0.03015037 0.0260967  0.02970021\n",
            "  0.03158907 0.02530287 0.03255755 0.02961655 0.03264502 0.0285776\n",
            "  0.03323406 0.02568542 0.02561952 0.02778087 0.03038574 0.0297839\n",
            "  0.03242582 0.03287935 0.02812282]]\n",
            "\n",
            "-----------------------------\n",
            "14th character is o\n",
            "-----------------------------\n",
            "\to's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[14]\n",
            "\n",
            "\t zs[14] < - Concatinate (xs[14], h[13] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          1.          0.          0.\n",
            "   0.          0.          0.          0.14968646  0.01054353  0.07443755\n",
            "   0.08649989 -0.00388043  0.01181134]]\n",
            "\t softmax of (ys[14]) =\n",
            "\t\t[[0.02775918 0.03400825 0.03242099 0.0260438  0.02928005 0.03053875\n",
            "  0.03220671 0.03215158 0.02763616 0.03025119 0.0303454  0.02975107\n",
            "  0.03436832 0.03527461 0.03603688 0.02996387 0.02624803 0.0297451\n",
            "  0.03149325 0.02568967 0.03222435 0.02970627 0.0331293  0.02889385\n",
            "  0.03319965 0.0254731  0.02577686 0.02785294 0.03005314 0.0295693\n",
            "  0.03195766 0.03255499 0.02839572]]\n",
            "\n",
            "-----------------------------\n",
            "15th character is m\n",
            "-----------------------------\n",
            "\tm's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[15]\n",
            "\n",
            "\t zs[15] < - Concatinate (xs[15], h[14] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.10769134 0.04544368 0.05597954\n",
            "  0.06070838 0.04607097 0.02629236]]\n",
            "\t softmax of (ys[15]) =\n",
            "\t\t[[0.02778734 0.03425592 0.03215023 0.02632146 0.02886854 0.03056382\n",
            "  0.03214413 0.03167982 0.02805459 0.03051599 0.03080293 0.02991443\n",
            "  0.03429101 0.03525783 0.03606739 0.03011374 0.02631851 0.02987634\n",
            "  0.03114014 0.02594464 0.03215846 0.02984472 0.0331121  0.02875612\n",
            "  0.03346841 0.02533211 0.02608961 0.0276171  0.02974739 0.02933024\n",
            "  0.03166171 0.03213368 0.02867956]]\n",
            "\n",
            "-----------------------------\n",
            "16th character is a\n",
            "-----------------------------\n",
            "\ta's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[16]\n",
            "\n",
            "\t zs[16] < - Concatinate (xs[16], h[15] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.06241642 0.02924927 0.01884345\n",
            "  0.04102614 0.08124237 0.07596322]]\n",
            "\t softmax of (ys[16]) =\n",
            "\t\t[[0.0276143  0.03431839 0.03273957 0.02625751 0.02922907 0.03110689\n",
            "  0.03247401 0.03158281 0.02784405 0.03062522 0.03040561 0.02966336\n",
            "  0.0340086  0.03488541 0.03613318 0.03041741 0.02613817 0.02963405\n",
            "  0.03126345 0.0258917  0.03261521 0.02978632 0.03270143 0.02831729\n",
            "  0.03328457 0.02533269 0.02621824 0.02730565 0.0298731  0.02947404\n",
            "  0.03215916 0.03233707 0.02836247]]\n",
            "\n",
            "-----------------------------\n",
            "17th character is n\n",
            "-----------------------------\n",
            "\tn's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "\t Let us denote this vector by xs[17]\n",
            "\n",
            "\t zs[17] < - Concatinate (xs[17], h[16] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.07889413 0.00236865 0.08878727\n",
            "  0.05413595 0.04485658 0.09184565]]\n",
            "\t softmax of (ys[17]) =\n",
            "\t\t[[0.02772791 0.03420444 0.03255558 0.02616495 0.02893523 0.03047317\n",
            "  0.03211725 0.03228477 0.02769716 0.03046092 0.03061867 0.02984695\n",
            "  0.03442989 0.03545608 0.03621262 0.03009187 0.02642236 0.02982555\n",
            "  0.03134677 0.02592725 0.03202039 0.02945917 0.03339562 0.02879251\n",
            "  0.0331931  0.02513918 0.02566933 0.02765819 0.02966352 0.02948331\n",
            "  0.03183272 0.03218029 0.02871327]]\n",
            "\n",
            "-----------------------------\n",
            "18th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[18]\n",
            "\n",
            "\t zs[18] < - Concatinate (xs[18], h[17] ):\n",
            "\t[[0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.11486325 0.0344816  0.04819886\n",
            "  0.0319804  0.10909119 0.05440889]]\n",
            "\t softmax of (ys[18]) =\n",
            "\t\t[[0.02770596 0.03400297 0.03221314 0.02597378 0.02925601 0.03035609\n",
            "  0.03194428 0.032584   0.02745498 0.03006851 0.03015146 0.02979716\n",
            "  0.03449224 0.03522356 0.03600734 0.02975615 0.02638254 0.02983788\n",
            "  0.03159824 0.02593034 0.03191092 0.02957399 0.03377009 0.02923891\n",
            "  0.0330855  0.02536983 0.02565348 0.02801829 0.03007061 0.0295884\n",
            "  0.03193278 0.03243814 0.02861242]]\n",
            "\n",
            "-----------------------------\n",
            "19th character is w\n",
            "-----------------------------\n",
            "\tw's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[19]\n",
            "\n",
            "\t zs[19] < - Concatinate (xs[19], h[18] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.12490255 0.10474769 0.05502629\n",
            "  0.06845157 0.06799715 0.01769421]]\n",
            "\t softmax of (ys[19]) =\n",
            "\t\t[[0.02758824 0.03396222 0.03297605 0.02587634 0.02945477 0.03066839\n",
            "  0.03231883 0.03254251 0.02727291 0.03007126 0.03007229 0.02951929\n",
            "  0.03411435 0.03524885 0.03621769 0.03024928 0.0262178  0.02963438\n",
            "  0.03158852 0.02557155 0.03236479 0.02937953 0.03308799 0.0286347\n",
            "  0.03313052 0.02548744 0.02561275 0.02771188 0.03027402 0.02974774\n",
            "  0.03247091 0.03255895 0.02837325]]\n",
            "\n",
            "-----------------------------\n",
            "20th character is h\n",
            "-----------------------------\n",
            "\th's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[20]\n",
            "\n",
            "\t zs[20] < - Concatinate (xs[20], h[19] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.16839782 0.03989371 0.08895983\n",
            "  0.06350135 0.02599666 0.03222344]]\n",
            "\t softmax of (ys[20]) =\n",
            "\t\t[[0.02779763 0.03392422 0.03270533 0.02594359 0.0294375  0.03054636\n",
            "  0.03229477 0.03238378 0.02745793 0.03031286 0.03022427 0.02969586\n",
            "  0.03455458 0.03542001 0.03606631 0.02988134 0.02624175 0.02963614\n",
            "  0.03169482 0.02555446 0.03225123 0.0295723  0.03306408 0.02890173\n",
            "  0.03295048 0.02539754 0.02550902 0.02792338 0.02997637 0.0296807\n",
            "  0.0319953  0.03279231 0.02821204]]\n",
            "\n",
            "-----------------------------\n",
            "21th character is o\n",
            "-----------------------------\n",
            "\to's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[21]\n",
            "\n",
            "\t zs[21] < - Concatinate (xs[21], h[20] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          1.          0.          0.\n",
            "   0.          0.          0.          0.12668764  0.03001421  0.08456049\n",
            "   0.05511398  0.05329414 -0.00348718]]\n",
            "\t softmax of (ys[21]) =\n",
            "\t\t[[0.02791089 0.03404475 0.03227488 0.02610832 0.02923405 0.03048055\n",
            "  0.03205349 0.03220964 0.02763437 0.03039006 0.03040967 0.02967141\n",
            "  0.03470499 0.03541859 0.03601085 0.02984207 0.02633982 0.02965818\n",
            "  0.03154348 0.02583358 0.03201633 0.02965585 0.0334119  0.02911166\n",
            "  0.03304722 0.02533073 0.02573512 0.02796323 0.02984653 0.02948837\n",
            "  0.03169709 0.03243994 0.02848239]]\n",
            "\n",
            "-----------------------------\n",
            "22th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[22]\n",
            "\n",
            "\t zs[22] < - Concatinate (xs[22], h[21] ):\n",
            "\t[[0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.0970636  0.0609072  0.05945958\n",
            "  0.03237943 0.0768179  0.01913969]]\n",
            "\t softmax of (ys[22]) =\n",
            "\t\t[[0.02779115 0.03390479 0.03207674 0.02592893 0.02943814 0.03036137\n",
            "  0.03193289 0.03254664 0.02741889 0.03002635 0.03001855 0.02973189\n",
            "  0.03463137 0.03518696 0.03588875 0.02959823 0.02632625 0.02975923\n",
            "  0.0317237  0.02585865 0.03192959 0.02969523 0.03374613 0.0294085\n",
            "  0.03299388 0.02547978 0.02566987 0.02818904 0.03017612 0.02960807\n",
            "  0.03186997 0.03263309 0.02845122]]\n",
            "\n",
            "-----------------------------\n",
            "23th character is l\n",
            "-----------------------------\n",
            "\tl's one-hot encoding is\n",
            "\t[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[23]\n",
            "\n",
            "\t zs[23] < - Concatinate (xs[23], h[22] ):\n",
            "\t[[ 0.          1.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.11495038  0.11704005  0.06294177\n",
            "   0.07371997  0.04865252 -0.00502048]]\n",
            "\t softmax of (ys[23]) =\n",
            "\t\t[[0.02732934 0.03410183 0.03245176 0.02604487 0.02967752 0.03124729\n",
            "  0.03272491 0.03132147 0.02788627 0.0301995  0.02984759 0.03000496\n",
            "  0.03349684 0.03415855 0.0357985  0.03009976 0.02583164 0.02986528\n",
            "  0.03141937 0.02566503 0.03306557 0.03036108 0.0322683  0.02834902\n",
            "  0.03342009 0.02582551 0.02646568 0.02742242 0.03051548 0.02967156\n",
            "  0.03252679 0.03307276 0.02786348]]\n",
            "\n",
            "-----------------------------\n",
            "24th character is i\n",
            "-----------------------------\n",
            "\ti's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[24]\n",
            "\n",
            "\t zs[24] < - Concatinate (xs[24], h[23] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.05278002  0.02750151  0.09477388\n",
            "   0.16674305 -0.0561518   0.06248929]]\n",
            "\t softmax of (ys[24]) =\n",
            "\t\t[[0.02759462 0.0339361  0.03279238 0.02605643 0.02952947 0.03103866\n",
            "  0.03289399 0.03117384 0.02798258 0.03028388 0.03040402 0.02979047\n",
            "  0.03359063 0.0348809  0.03598694 0.03040844 0.02579903 0.02970898\n",
            "  0.03126924 0.0251054  0.03317105 0.03020939 0.0316235  0.0280676\n",
            "  0.03363361 0.02599045 0.02623746 0.02745077 0.03048934 0.02964857\n",
            "  0.03241655 0.03304593 0.02778978]]\n",
            "\n",
            "-----------------------------\n",
            "25th character is v\n",
            "-----------------------------\n",
            "\tv's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[25]\n",
            "\n",
            "\t zs[25] < - Concatinate (xs[25], h[24] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   1.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.07487379 -0.05932001  0.057862\n",
            "   0.11137121 -0.06452148  0.05005575]]\n",
            "\t softmax of (ys[25]) =\n",
            "\t\t[[0.02774875 0.03377761 0.03268306 0.02588114 0.02981596 0.03087364\n",
            "  0.03252259 0.03185367 0.02747559 0.03001093 0.02999662 0.02935727\n",
            "  0.03402685 0.0350401  0.03595661 0.03020863 0.02592466 0.02946571\n",
            "  0.03160024 0.02522136 0.0327724  0.02990072 0.03238644 0.02860866\n",
            "  0.03329184 0.02598794 0.02600862 0.0278581  0.03069235 0.02972047\n",
            "  0.03244678 0.0329689  0.0279158 ]]\n",
            "\n",
            "-----------------------------\n",
            "26th character is e\n",
            "-----------------------------\n",
            "\te's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[26]\n",
            "\n",
            "\t zs[26] < - Concatinate (xs[26], h[25] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          1.\n",
            "   0.          0.          0.          0.12146427  0.02119191  0.08425269\n",
            "   0.08267149 -0.06239393  0.01454828]]\n",
            "\t softmax of (ys[26]) =\n",
            "\t\t[[0.02721175 0.03430051 0.03276458 0.02605338 0.02974264 0.03150254\n",
            "  0.03267035 0.03166598 0.0276292  0.03040346 0.02955484 0.02990786\n",
            "  0.03362832 0.03397211 0.03589354 0.03012451 0.02594082 0.02975137\n",
            "  0.03158062 0.02606013 0.03295984 0.03006482 0.03273154 0.02833622\n",
            "  0.03303352 0.02542237 0.02636775 0.02724481 0.03021862 0.02971555\n",
            "  0.03265598 0.03292828 0.02796219]]\n",
            "\n",
            "-----------------------------\n",
            "27th character is d\n",
            "-----------------------------\n",
            "\td's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[27]\n",
            "\n",
            "\t zs[27] < - Concatinate (xs[27], h[26] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          1.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.0707421   0.05283028  0.15470954\n",
            "   0.15516758 -0.00847794  0.07667281]]\n",
            "\t softmax of (ys[27]) =\n",
            "\t\t[[0.02694178 0.03429116 0.03285074 0.02602242 0.02934543 0.03125501\n",
            "  0.03269728 0.031655   0.0277418  0.02994647 0.02988331 0.03004309\n",
            "  0.03284451 0.03402832 0.03612492 0.03063641 0.02593803 0.03009195\n",
            "  0.03112138 0.02585272 0.03305077 0.02995114 0.03249166 0.0279189\n",
            "  0.03376195 0.02571212 0.02648548 0.02699401 0.0306478  0.02971737\n",
            "  0.03309706 0.03251061 0.0283494 ]]\n",
            "\n",
            "-----------------------------\n",
            "28th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[28]\n",
            "\n",
            "\t zs[28] < - Concatinate (xs[28], h[27] ):\n",
            "\t[[ 0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.11391966  0.03293447  0.07804076\n",
            "   0.17769485 -0.04466494  0.13129526]]\n",
            "\t softmax of (ys[28]) =\n",
            "\t\t[[0.02713014 0.03407043 0.03240093 0.02587432 0.02945288 0.03079313\n",
            "  0.03236161 0.03219721 0.02755056 0.02976021 0.02971679 0.03015007\n",
            "  0.03342242 0.03427919 0.03594304 0.03000392 0.0260839  0.03014924\n",
            "  0.0314528  0.02588457 0.03257115 0.02993228 0.03315125 0.02865381\n",
            "  0.03345466 0.02568733 0.02609174 0.02756479 0.03062609 0.02976837\n",
            "  0.03271077 0.03275329 0.0283571 ]]\n",
            "\n",
            "-----------------------------\n",
            "29th character is t\n",
            "-----------------------------\n",
            "\tt's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[29]\n",
            "\n",
            "\t zs[29] < - Concatinate (xs[29], h[28] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          1.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.11899304  0.09631281  0.06870066\n",
            "   0.18022299 -0.01995793  0.06152395]]\n",
            "\t softmax of (ys[29]) =\n",
            "\t\t[[0.02701334 0.03407944 0.03228892 0.0258522  0.02908201 0.03038144\n",
            "  0.03227005 0.03248668 0.0276689  0.02968338 0.02996356 0.03066738\n",
            "  0.03339247 0.034466   0.03600068 0.0297942  0.02624191 0.03054953\n",
            "  0.03136622 0.02592571 0.03231    0.02985284 0.03339213 0.0287593\n",
            "  0.03356899 0.02552554 0.02577595 0.02760631 0.03041111 0.02978471\n",
            "  0.03253974 0.03271601 0.02858336]]\n",
            "\n",
            "-----------------------------\n",
            "30th character is h\n",
            "-----------------------------\n",
            "\th's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[30]\n",
            "\n",
            "\t zs[30] < - Concatinate (xs[30], h[29] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.12634552 0.09495341 0.02521369\n",
            "  0.20276159 0.02706841 0.05243554]]\n",
            "\t softmax of (ys[30]) =\n",
            "\t\t[[0.02736187 0.03399817 0.03220519 0.02591944 0.02917155 0.030334\n",
            "  0.03229194 0.03231342 0.02777401 0.03002611 0.03014762 0.03058598\n",
            "  0.03399718 0.03482911 0.03589488 0.0295297  0.02624284 0.0303283\n",
            "  0.03153494 0.02578087 0.03224382 0.02995713 0.03322225 0.02897933\n",
            "  0.03327948 0.0254434  0.02562155 0.02784674 0.03008249 0.02971933\n",
            "  0.03204496 0.03296746 0.02832494]]\n",
            "\n",
            "-----------------------------\n",
            "31th character is r\n",
            "-----------------------------\n",
            "\tr's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\t Let us denote this vector by xs[31]\n",
            "\n",
            "\t zs[31] < - Concatinate (xs[31], h[30] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.09297455 0.06567163 0.03666912\n",
            "  0.16307398 0.05104776 0.00825886]]\n",
            "\t softmax of (ys[31]) =\n",
            "\t\t[[0.02751587 0.03406737 0.03222483 0.02600456 0.02938392 0.03067682\n",
            "  0.0321806  0.03213767 0.02761288 0.03005372 0.03000122 0.02990099\n",
            "  0.03404733 0.03473707 0.03592436 0.02988839 0.02619401 0.02990067\n",
            "  0.03150193 0.02591573 0.03229953 0.02987147 0.03332704 0.02893637\n",
            "  0.03325868 0.02554481 0.02601056 0.02778291 0.03028454 0.02960987\n",
            "  0.03219153 0.03259966 0.02841308]]\n",
            "\n",
            "-----------------------------\n",
            "32th character is e\n",
            "-----------------------------\n",
            "\te's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[32]\n",
            "\n",
            "\t zs[32] < - Concatinate (xs[32], h[31] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.09879414 0.09125845 0.06520987\n",
            "  0.1120664  0.02082958 0.04272919]]\n",
            "\t softmax of (ys[32]) =\n",
            "\t\t[[0.02710971 0.03447987 0.03237874 0.02615481 0.02949904 0.03139885\n",
            "  0.03244621 0.0317402  0.02776239 0.03045769 0.02955974 0.03023847\n",
            "  0.03368594 0.03375898 0.03582287 0.0298824  0.02608867 0.03000035\n",
            "  0.03152088 0.02651382 0.03267097 0.03012688 0.03329677 0.0285827\n",
            "  0.03300901 0.02517921 0.02643593 0.02723026 0.02996874 0.02961729\n",
            "  0.03242558 0.03272169 0.02823533]]\n",
            "\n",
            "-----------------------------\n",
            "33th character is e\n",
            "-----------------------------\n",
            "\te's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[33]\n",
            "\n",
            "\t zs[33] < - Concatinate (xs[33], h[32] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.04451525 0.0991343  0.14217137\n",
            "  0.17273729 0.03915058 0.09238382]]\n",
            "\t softmax of (ys[33]) =\n",
            "\t\t[[0.02691774 0.03471144 0.03242051 0.02625535 0.02956093 0.03181107\n",
            "  0.03256942 0.03148141 0.02785674 0.03069011 0.02933546 0.03037707\n",
            "  0.03351079 0.0332395  0.03575723 0.0298916  0.02602895 0.03002459\n",
            "  0.03151517 0.02685908 0.03286453 0.0302797  0.03328823 0.02840515\n",
            "  0.03288178 0.02499224 0.0267146  0.02693928 0.02980049 0.02959548\n",
            "  0.03252472 0.03274876 0.02815087]]\n",
            "\n",
            "-----------------------------\n",
            "34th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[34]\n",
            "\n",
            "\t zs[34] < - Concatinate (xs[34], h[33] ):\n",
            "\t[[0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.01146154 0.10566716 0.18294032\n",
            "  0.20046549 0.04781049 0.12204547]]\n",
            "\t softmax of (ys[34]) =\n",
            "\t\t[[0.02721842 0.03430051 0.03201119 0.02604462 0.02959326 0.03109781\n",
            "  0.03220373 0.03202674 0.02763268 0.03019197 0.02943271 0.03021503\n",
            "  0.0339269  0.03386544 0.03569501 0.02957174 0.02614439 0.03002929\n",
            "  0.03167229 0.02648861 0.03240131 0.03015482 0.03369366 0.02904796\n",
            "  0.032954   0.02531059 0.02630593 0.02760996 0.03015593 0.02963727\n",
            "  0.03227878 0.03280986 0.02827757]]\n",
            "\n",
            "-----------------------------\n",
            "35th character is h\n",
            "-----------------------------\n",
            "\th's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[35]\n",
            "\n",
            "\t zs[35] < - Concatinate (xs[35], h[34] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.0520352  0.14921146 0.12451138\n",
            "  0.1777691  0.03040424 0.05648173]]\n",
            "\t softmax of (ys[35]) =\n",
            "\t\t[[0.02753444 0.0341369  0.03198166 0.02606005 0.02948859 0.03078089\n",
            "  0.03221112 0.03199275 0.02775528 0.0303426  0.02983718 0.03022411\n",
            "  0.03436556 0.03447444 0.0357017  0.02942231 0.02618586 0.02996215\n",
            "  0.03170501 0.02614083 0.03227662 0.03014659 0.03343844 0.02918689\n",
            "  0.03291952 0.02533328 0.02600023 0.02786764 0.02994644 0.0295962\n",
            "  0.03185769 0.0329575  0.02816952]]\n",
            "\n",
            "-----------------------------\n",
            "36th character is u\n",
            "-----------------------------\n",
            "\tu's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[36]\n",
            "\n",
            "\t zs[36] < - Concatinate (xs[36], h[35] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         1.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.04515454 0.1038101  0.09556902\n",
            "  0.13847282 0.05067381 0.01438177]]\n",
            "\t softmax of (ys[36]) =\n",
            "\t\t[[0.02752103 0.03418797 0.03169333 0.0261865  0.02951776 0.03099616\n",
            "  0.03233847 0.03134343 0.02808333 0.03038653 0.02990307 0.03028642\n",
            "  0.0341009  0.03414417 0.03555158 0.02949053 0.02601868 0.02999531\n",
            "  0.03152129 0.02612271 0.03255743 0.03057551 0.03301044 0.02903497\n",
            "  0.03317933 0.02556487 0.02647111 0.02775209 0.03010431 0.02948954\n",
            "  0.03182733 0.0330113  0.0280326 ]]\n",
            "\n",
            "-----------------------------\n",
            "37th character is n\n",
            "-----------------------------\n",
            "\tn's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "\t Let us denote this vector by xs[37]\n",
            "\n",
            "\t zs[37] < - Concatinate (xs[37], h[36] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          1.         -0.00581535  0.09227037  0.08001775\n",
            "   0.16090122  0.0073999   0.03834335]]\n",
            "\t softmax of (ys[37]) =\n",
            "\t\t[[0.02760935 0.0341414  0.03178404 0.02614234 0.02907963 0.0304126\n",
            "  0.03206567 0.03199138 0.0279605  0.03028815 0.03031468 0.03038417\n",
            "  0.03436396 0.03484835 0.03579581 0.02947686 0.02631399 0.03015914\n",
            "  0.0314517  0.0260899  0.03204181 0.03008358 0.03351521 0.02922401\n",
            "  0.03322394 0.02533313 0.02592061 0.02790823 0.02984432 0.02947985\n",
            "  0.03162174 0.03264404 0.02848591]]\n",
            "\n",
            "-----------------------------\n",
            "38th character is d\n",
            "-----------------------------\n",
            "\td's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[38]\n",
            "\n",
            "\t zs[38] < - Concatinate (xs[38], h[37] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.04696456 0.09117143 0.03259261\n",
            "  0.11810629 0.07756076 0.02620699]]\n",
            "\t softmax of (ys[38]) =\n",
            "\t\t[[0.0271864  0.03421278 0.03222464 0.02609428 0.0290053  0.03068262\n",
            "  0.032337   0.03177254 0.02794717 0.02989217 0.03029264 0.03026914\n",
            "  0.0332649  0.03447277 0.03604002 0.03026112 0.02613599 0.03029355\n",
            "  0.0310486  0.02590445 0.03253656 0.03001038 0.03294549 0.02843956\n",
            "  0.03387447 0.02568597 0.0263116  0.02737479 0.03045794 0.02956175\n",
            "  0.03248251 0.0323371  0.02864379]]\n",
            "\n",
            "-----------------------------\n",
            "39th character is r\n",
            "-----------------------------\n",
            "\tr's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\t Let us denote this vector by xs[39]\n",
            "\n",
            "\t zs[39] < - Concatinate (xs[39], h[38] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   1.          0.          0.          0.09342758  0.0609417   0.01077864\n",
            "   0.15519736 -0.00100899  0.10547438]]\n",
            "\t softmax of (ys[39]) =\n",
            "\t\t[[0.02732728 0.03421528 0.03225217 0.02609387 0.02923247 0.03086238\n",
            "  0.03220317 0.03187441 0.0277086  0.0299121  0.03007202 0.02979155\n",
            "  0.03349197 0.03446729 0.03603576 0.03035202 0.02614415 0.02996216\n",
            "  0.03117608 0.02602361 0.03246735 0.02987693 0.03320897 0.02857606\n",
            "  0.03366737 0.02567922 0.02641853 0.02745312 0.0305378  0.02953313\n",
            "  0.03253005 0.03218829 0.02866482]]\n",
            "\n",
            "-----------------------------\n",
            "40th character is e\n",
            "-----------------------------\n",
            "\te's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[40]\n",
            "\n",
            "\t zs[40] < - Concatinate (xs[40], h[39] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          1.\n",
            "   0.          0.          0.          0.10545316  0.09263214  0.04492264\n",
            "   0.11806788 -0.0090502   0.10772701]]\n",
            "\t softmax of (ys[40]) =\n",
            "\t\t[[0.0270105  0.03456263 0.03236117 0.02621094 0.02938943 0.0314791\n",
            "  0.03244172 0.03158869 0.02783149 0.03036519 0.02962366 0.03018502\n",
            "  0.03337473 0.0336253  0.03588412 0.03013947 0.02606781 0.03004909\n",
            "  0.03131992 0.02657789 0.03274677 0.03013468 0.0332452  0.02839534\n",
            "  0.03326028 0.02526064 0.02667367 0.0270569  0.03011219 0.02956341\n",
            "  0.03259799 0.03246639 0.02839871]]\n",
            "\n",
            "-----------------------------\n",
            "41th character is d\n",
            "-----------------------------\n",
            "\td's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[41]\n",
            "\n",
            "\t zs[41] < - Concatinate (xs[41], h[40] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.04750549 0.10118031 0.12601419\n",
            "  0.17544651 0.02301512 0.12979855]]\n",
            "\t softmax of (ys[41]) =\n",
            "\t\t[[0.02684523 0.0344529  0.03253485 0.02613263 0.02913826 0.03124298\n",
            "  0.03252221 0.03157995 0.02787412 0.02991918 0.02992385 0.03017673\n",
            "  0.03271971 0.03380637 0.03609948 0.03063689 0.02601736 0.03025559\n",
            "  0.03095875 0.02619139 0.03289687 0.03001507 0.03284316 0.0279958\n",
            "  0.03390895 0.02562737 0.02671786 0.02690323 0.03059631 0.02960162\n",
            "  0.03302898 0.03219988 0.02863646]]\n",
            "\n",
            "-----------------------------\n",
            "42th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[42]\n",
            "\n",
            "\t zs[42] < - Concatinate (xs[42], h[41] ):\n",
            "\t[[ 0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.09589562  0.06894629  0.05893941\n",
            "   0.18679303 -0.0270786   0.16502169]]\n",
            "\t softmax of (ys[42]) =\n",
            "\t\t[[0.02707636 0.03418016 0.03216394 0.02595793 0.0293141  0.03079229\n",
            "  0.03223925 0.03211262 0.02765333 0.02974427 0.02975226 0.03022564\n",
            "  0.03333909 0.03412314 0.03591766 0.03000936 0.02613272 0.03025078\n",
            "  0.03133215 0.02611519 0.03247194 0.02999368 0.03338511 0.02871205\n",
            "  0.03356556 0.02564425 0.02627465 0.02750577 0.03060069 0.02967843\n",
            "  0.03265306 0.03253092 0.02855167]]\n",
            "\n",
            "-----------------------------\n",
            "43th character is y\n",
            "-----------------------------\n",
            "\ty's one-hot encoding is\n",
            "\t[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[43]\n",
            "\n",
            "\t zs[43] < - Concatinate (xs[43], h[42] ):\n",
            "\t[[ 0.          0.          1.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.10419378  0.12132818  0.05451721\n",
            "   0.18505819 -0.01009685  0.08552847]]\n",
            "\t softmax of (ys[43]) =\n",
            "\t\t[[0.02725551 0.03419026 0.03189323 0.02603744 0.02915553 0.03051785\n",
            "  0.03225069 0.03209869 0.02795317 0.03022546 0.02998403 0.03088121\n",
            "  0.03403625 0.03436588 0.03572447 0.02927063 0.02625464 0.03045929\n",
            "  0.03156084 0.0261969  0.03221768 0.03021233 0.03347938 0.02911716\n",
            "  0.03313503 0.02524291 0.02584413 0.02775889 0.02984553 0.02964385\n",
            "  0.03186617 0.03301446 0.02831052]]\n",
            "\n",
            "-----------------------------\n",
            "44th character is e\n",
            "-----------------------------\n",
            "\te's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[44]\n",
            "\n",
            "\t zs[44] < - Concatinate (xs[44], h[43] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.04206071 0.0974082  0.05615314\n",
            "  0.19371028 0.07583225 0.01999068]]\n",
            "\t softmax of (ys[44]) =\n",
            "\t\t[[0.02699435 0.03454207 0.03215231 0.02618355 0.02939221 0.03132071\n",
            "  0.03248739 0.03166612 0.02797458 0.03057575 0.02955214 0.03077499\n",
            "  0.03371481 0.03355269 0.03568539 0.02951194 0.02610953 0.03029076\n",
            "  0.03156211 0.02666644 0.03263403 0.03035918 0.03335499 0.02870435\n",
            "  0.0329295  0.02502767 0.02636711 0.02723792 0.02972246 0.02962405\n",
            "  0.03220279 0.03297896 0.02814713]]\n",
            "\n",
            "-----------------------------\n",
            "45th character is a\n",
            "-----------------------------\n",
            "\ta's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[45]\n",
            "\n",
            "\t zs[45] < - Concatinate (xs[45], h[44] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.00671729 0.10228982 0.13773596\n",
            "  0.21706731 0.06748679 0.07721403]]\n",
            "\t softmax of (ys[45]) =\n",
            "\t\t[[0.02713328 0.03448344 0.03262691 0.02618399 0.02951792 0.03152117\n",
            "  0.03267356 0.03152163 0.02786515 0.03065105 0.02968172 0.03027499\n",
            "  0.03366114 0.03383106 0.03585795 0.0299706  0.02600973 0.02994671\n",
            "  0.03150418 0.02632825 0.03289126 0.03017657 0.03284125 0.0283346\n",
            "  0.03299304 0.02516803 0.02640271 0.02711057 0.02985253 0.02963837\n",
            "  0.03242248 0.03288953 0.02803461]]\n",
            "\n",
            "-----------------------------\n",
            "46th character is r\n",
            "-----------------------------\n",
            "\tr's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\t Let us denote this vector by xs[46]\n",
            "\n",
            "\t zs[46] < - Concatinate (xs[46], h[45] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.03516548 0.04877137 0.15175837\n",
            "  0.16911138 0.036475   0.08925332]]\n",
            "\t softmax of (ys[46]) =\n",
            "\t\t[[0.0273396  0.03436756 0.03248961 0.02615309 0.02956899 0.03137658\n",
            "  0.03239311 0.03171378 0.02764673 0.03038692 0.02970854 0.02973367\n",
            "  0.03379232 0.03411662 0.03592121 0.03017723 0.02606701 0.02970502\n",
            "  0.03146801 0.02626773 0.0326711  0.02997109 0.03314695 0.02853706\n",
            "  0.03311089 0.02537226 0.026477   0.02732034 0.03016705 0.02956982\n",
            "  0.03246508 0.032515   0.028283  ]]\n",
            "\n",
            "-----------------------------\n",
            "47th character is s\n",
            "-----------------------------\n",
            "\ts's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[47]\n",
            "\n",
            "\t zs[47] < - Concatinate (xs[47], h[46] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.06973924 0.08539002 0.13225598\n",
            "  0.11895398 0.01290147 0.09539855]]\n",
            "\t softmax of (ys[47]) =\n",
            "\t\t[[0.02742354 0.03426061 0.03226256 0.02622986 0.02916761 0.03097528\n",
            "  0.03246398 0.0313421  0.02807522 0.03028326 0.03036221 0.03003044\n",
            "  0.03357536 0.03451273 0.03596149 0.03027634 0.02604513 0.02998273\n",
            "  0.03110939 0.02587182 0.03270493 0.03016798 0.03261363 0.0283812\n",
            "  0.03367157 0.02564153 0.026483   0.02735114 0.03023846 0.02946833\n",
            "  0.03223883 0.03244995 0.02837781]]\n",
            "\n",
            "-----------------------------\n",
            "48th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[48]\n",
            "\n",
            "\t zs[48] < - Concatinate (xs[48], h[47] ):\n",
            "\t[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  5.36676313e-02  2.88300274e-02  4.21795254e-02\n",
            "   1.19731394e-01 -9.14975922e-04  1.00118394e-01]]\n",
            "\t softmax of (ys[48]) =\n",
            "\t\t[[0.02749743 0.0340411  0.03197446 0.02602259 0.02936176 0.03063166\n",
            "  0.0321658  0.03195131 0.027747   0.02995407 0.03002911 0.03000739\n",
            "  0.03392799 0.03461744 0.03583345 0.02982876 0.02614758 0.0300031\n",
            "  0.03142589 0.02589713 0.032334   0.03006787 0.03325616 0.02898653\n",
            "  0.0334222  0.02569187 0.02616272 0.02782642 0.03042133 0.02957391\n",
            "  0.03215301 0.03263301 0.02840596]]\n",
            "\n",
            "-----------------------------\n",
            "49th character is a\n",
            "-----------------------------\n",
            "\ta's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[49]\n",
            "\n",
            "\t zs[49] < - Concatinate (xs[49], h[48] ):\n",
            "\t[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 8.05202113e-02 9.99745018e-02 4.36951624e-02\n",
            "  1.31094234e-01 6.63478308e-04 4.56556101e-02]]\n",
            "\t softmax of (ys[49]) =\n",
            "\t\t[[0.0273651  0.03422096 0.03261772 0.02608476 0.0294737  0.03114236\n",
            "  0.0325259  0.03171737 0.02772501 0.03030015 0.02995211 0.02988412\n",
            "  0.03371226 0.03441933 0.03598028 0.0301965  0.02603544 0.02981621\n",
            "  0.03141235 0.02589802 0.0327466  0.02997029 0.03276179 0.02841706\n",
            "  0.03328074 0.02551104 0.02625665 0.02737831 0.03022925 0.02963136\n",
            "  0.0324586  0.03268566 0.02819302]]\n",
            "\n",
            "-----------------------------\n",
            "50th character is g\n",
            "-----------------------------\n",
            "\tg's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[50]\n",
            "\n",
            "\t zs[50] < - Concatinate (xs[50], h[49] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.08289101 0.04148056 0.10068755\n",
            "  0.12438512 0.00204472 0.07588602]]\n",
            "\t softmax of (ys[50]) =\n",
            "\t\t[[0.02711795 0.03419371 0.03306481 0.02595384 0.02947797 0.03114218\n",
            "  0.0328172  0.03190329 0.0276803  0.03030821 0.02986869 0.03026789\n",
            "  0.03347023 0.03435405 0.03605861 0.03014724 0.02599087 0.03002373\n",
            "  0.03149748 0.02572197 0.03295677 0.02990382 0.03244527 0.02814109\n",
            "  0.03320765 0.02541087 0.02593682 0.02721968 0.03014434 0.02982871\n",
            "  0.03270213 0.03305796 0.02798468]]\n",
            "\n",
            "-----------------------------\n",
            "51th character is o\n",
            "-----------------------------\n",
            "\to's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[51]\n",
            "\n",
            "\t zs[51] < - Concatinate (xs[51], h[50] ):\n",
            "\t[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  1.02671098e-01 -5.38407815e-04  1.16737817e-01\n",
            "   1.69198738e-01  8.85838710e-03  5.93929495e-02]]\n",
            "\t softmax of (ys[51]) =\n",
            "\t\t[[0.02747958 0.0341869  0.03247596 0.02609712 0.02923459 0.03077686\n",
            "  0.03238257 0.03194786 0.02779398 0.03036841 0.03021612 0.03011625\n",
            "  0.03403555 0.03479272 0.03599871 0.02994464 0.02619205 0.02995819\n",
            "  0.03143294 0.02590085 0.03243189 0.02987794 0.03301106 0.02866302\n",
            "  0.03321676 0.02534343 0.02593699 0.02756056 0.02993888 0.02959736\n",
            "  0.03209902 0.03266479 0.02832647]]\n",
            "\n",
            "-----------------------------\n",
            "52th character is ,\n",
            "-----------------------------\n",
            "\t,'s one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[52]\n",
            "\n",
            "\t zs[52] < - Concatinate (xs[52], h[51] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.08206162 0.03907386 0.07302251\n",
            "  0.10961134 0.05197957 0.05053571]]\n",
            "\t softmax of (ys[52]) =\n",
            "\t\t[[0.02760467 0.03419951 0.0324228  0.02615103 0.02903646 0.03059983\n",
            "  0.03218797 0.032069   0.02776501 0.03031834 0.03046452 0.02990549\n",
            "  0.03413331 0.03511641 0.03612038 0.03011954 0.02630921 0.02989828\n",
            "  0.03129686 0.02592622 0.03220416 0.0296721  0.03324109 0.02872089\n",
            "  0.03334733 0.02532643 0.02591472 0.02760174 0.02991894 0.0294997\n",
            "  0.03200679 0.03227109 0.0286302 ]]\n",
            "\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_derivative_LSTM (params, inputs, targets, cprev, hprev):\n",
        "\n",
        "    Wf, Wi, Wg, Wo, bf, bi, bg, bo, Wy, by = params\n",
        "\n",
        "    xs, hs, zs, cs, ys, ps = {}, {}, {}, {}, {}, {}\n",
        "    fs, i_s, gs, os, tanhcs = {}, {}, {}, {}, {}\n",
        "    cs[-1] = np.copy(cprev) # reset RNN memory\n",
        "    hs[-1] = np.copy(hprev) # reset RNN memory\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # forward pass\n",
        "    for t in range(len(inputs)):\n",
        "\n",
        "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
        "        xs[t][inputs[t]] = 1\n",
        "        zs[t] = np.concatenate((xs[t], hs[t-1]), axis=0)\n",
        "\n",
        "        f_raw = np.dot(Wf, zs[t]) + bf\n",
        "        i_raw = np.dot(Wi, zs[t]) + bi\n",
        "        g_raw = np.dot(Wg, zs[t]) + bg\n",
        "        o_raw = np.dot(Wo, zs[t]) + bo\n",
        "\n",
        "        fs[t] = sigmoid(f_raw)\n",
        "        i_s[t] = sigmoid(i_raw)\n",
        "        gs[t] = np.tanh(g_raw)\n",
        "        os[t] = sigmoid(i_raw)\n",
        "\n",
        "        cs[t] = fs[t]*cs[t-1] + i_s[t]*gs[t]\n",
        "        tanhcs[t] = np.tanh(cs[t])\n",
        "        hs[t] = os[t]*tanhcs[t]\n",
        "        ys[t] = np.dot(Wy, hs[t]) + by\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "\n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
        "\n",
        "    # backward pass: compute gradients going backwards\n",
        "    dWf, dWi, dWg, dWo = np.zeros_like(Wf), np.zeros_like(Wi), np.zeros_like(Wg), np.zeros_like(Wo)\n",
        "    dbf,dbi, dbg, dbo = np.zeros_like(bf), np.zeros_like(bi), np.zeros_like(bg), np.zeros_like(bo)\n",
        "    dWy = np.zeros_like(Wy)\n",
        "    dby = np.zeros_like(by)\n",
        "\n",
        "    dcnext = np.zeros((hidden_size, 1))\n",
        "    dhnext = np.zeros((hidden_size, 1))\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t])\n",
        "        dy[targets[t]] -= 1\n",
        "\n",
        "        # Phase 1\n",
        "        dWy += np.dot(dy, hs[t].T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Wy.T, dy) + dhnext\n",
        "        dc = dh * os[t] * (1-tanhcs[t]*tanhcs[t]) + dcnext\n",
        "\n",
        "        ## Phase 2\n",
        "        df = cs[t-1] * dc\n",
        "        di = gs[t] * dc\n",
        "        dg = i_s[t] * dc\n",
        "        do = tanhcs[t] * dh\n",
        "\n",
        "        ## Phase 3\n",
        "        df_raw = fs[t]*(1-fs[t])*df\n",
        "        di_raw = i_s[t]*(1-i_s[t])*di\n",
        "        dg_raw = (1-gs[t]*gs[t])*dg\n",
        "        do_raw = os[t]*(1-os[t])*do\n",
        "\n",
        "        ## Phase4\n",
        "        dWf += np.dot(df_raw, zs[t].T)\n",
        "        dWi += np.dot(di_raw, zs[t].T)\n",
        "        dWg += np.dot(dg_raw, zs[t].T)\n",
        "        dWo += np.dot(do_raw, zs[t].T)\n",
        "\n",
        "        dbf += df_raw\n",
        "        dbi += di_raw\n",
        "        dbg += dg_raw\n",
        "        dbo += do_raw\n",
        "\n",
        "        ## Phase 5\n",
        "        dcnext = fs[t] * dc\n",
        "        dz = np.dot(Wf.T, df_raw) + np.dot(Wi.T, di_raw) + np.dot(Wg.T, dg_raw) + np.dot(Wo.T, do_raw)\n",
        "        dhnext = dz[vocab_size:]\n",
        "\n",
        "        for dparam in [dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby]:\n",
        "            np.clip(dparam, -5, 5, out=dparam)\n",
        "\n",
        "    return loss, dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby, cs[len(inputs)-1], hs[len(inputs)-1]\n",
        "\n",
        "def guess_sentence_LSTM (params, ch, max_seq = 250):\n",
        "\n",
        "    Wf, Wi, Wg, Wo, bf, bi, bg, bo, Wy, by  = params\n",
        "    initial_char = char_to_idx[ch]\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[initial_char] = 1\n",
        "\n",
        "    h = np.zeros((hidden_size, 1))\n",
        "    c = np.zeros((hidden_size, 1))\n",
        "\n",
        "    ixes = [initial_char]\n",
        "\n",
        "    n=0\n",
        "    while True:\n",
        "\n",
        "        z = np.concatenate((x, h), axis=0)\n",
        "\n",
        "        f_raw = np.dot(Wf, z) + bf\n",
        "        i_raw = np.dot(Wi, z) + bi\n",
        "        g_raw = np.dot(Wg, z) + bg\n",
        "        o_raw = np.dot(Wo, z) + bo\n",
        "\n",
        "        f = sigmoid(f_raw)\n",
        "        i = sigmoid(i_raw)\n",
        "        g = np.tanh(g_raw)\n",
        "        o = sigmoid(i_raw)\n",
        "\n",
        "        c = f*c + i*g\n",
        "        h = o*np.tanh(c)\n",
        "        y = np.dot(Wy, h) + by\n",
        "        p = np.exp(y) / np.sum(np.exp(y)) # probabilities for next chars\n",
        "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "        x = np.zeros((vocab_size, 1))\n",
        "        x[ix] = 1\n",
        "\n",
        "        n+=1\n",
        "        if ( n > max_seq):\n",
        "            break\n",
        "\n",
        "        ixes.append(ix)\n",
        "\n",
        "    return ''.join([ idx_to_char[x] for x in ixes ])"
      ],
      "metadata": {
        "id": "ZYWwuEiicTM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import signal\n",
        "learning_rate = 1e-1\n",
        "\n",
        "def optimize(iteration = 10000, hidden_size = 8) :\n",
        "\n",
        "    n  =  0\n",
        "    loss_trace = []\n",
        "\n",
        "    params = make_LSTM_parameters(hidden_size, vocab_size)\n",
        "    mems = []\n",
        "    for param in params:\n",
        "        mems.append(np.zeros_like(param))\n",
        "\n",
        "    for n in range(iteration):\n",
        "        try:\n",
        "\n",
        "            loss_total = 0\n",
        "\n",
        "            sentence = data # Whole BackPropagation Through Time (Not Truncated Version)\n",
        "\n",
        "            loss_sentence = 0\n",
        "            hprev, cprev = np.zeros((hidden_size,1)), np.zeros((hidden_size,1))\n",
        "\n",
        "            inputs = [char_to_idx[ch] for ch in sentence[:-1]]\n",
        "            targets = [char_to_idx[ch] for ch in sentence[1:]]                \n",
        "\n",
        "            loss, dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby, cprev, hprev = get_derivative_LSTM (params, inputs, targets, cprev, hprev)\n",
        "\n",
        "            loss_total += loss\n",
        "\n",
        "\n",
        "            # perform parameter update with Adagrad\n",
        "            for param, dparam, mem in zip(params,\n",
        "                        [dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby],\n",
        "                        mems):\n",
        "                mem += dparam * dparam\n",
        "                param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "            loss_trace.append(loss_total)\n",
        "\n",
        "            if (n % 50 == 0):\n",
        "                import matplotlib.pyplot as plt\n",
        "                from IPython import display\n",
        "\n",
        "                display.clear_output(wait=True)\n",
        "                # plt.ylim((0,4000))\n",
        "                plt.plot(loss_trace)\n",
        "                plt.ylabel('cost')\n",
        "                plt.xlabel('iterations (per hundreds)')\n",
        "                plt.show()\n",
        "\n",
        "                print ('iter %d, loss: %f \\nguess_sentences:' % (n, loss_total)) # print progress\n",
        "                for i in range(1):\n",
        "                    print(guess_sentence_LSTM(params, 'T', len(sentence)))\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            break\n",
        "\n",
        "    return params, loss_trace\n",
        "\n",
        "iteration = 801\n",
        "hidden_size = 50\n",
        "params, loss_trace = optimize(iteration, hidden_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "vSjoGUUOZ8oF",
        "outputId": "24f40f18-02f4-4866-dc22-61e28a4d2725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEJCAYAAABlmAtYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU5bnA8d+TyQ5ZIYRAwIBsgiJoAFGraFXQLi5Vr7Z16bWX2tpqa29bve29tra29qrV22s3rbhf19pK3RCVFjeEgIDsRHZISCBhSUK2yXP/OO8kk2SyQSYzSZ7v5zOfnHnPmTPPJJN55l3O+4qqYowxxrQnJtIBGGOMiX6WLIwxxnTIkoUxxpgOWbIwxhjTIUsWxhhjOmTJwhhjTIfCnixExCciH4vIK+7+KBH5SEQKReQ5EYl35QnufqHbnxd0jttd+UYRmR3umI0xxjTXEzWLW4D1Qfd/DdyvqmOAcuAGV34DUO7K73fHISITgauAScAc4Pci4uuBuI0xxjgSzovyRCQXeBy4C7gV+AJQCgxV1XoRmQn8VFVni8gCt/2hiMQCxUAWcBuAqv7KnbPxuLaed/DgwZqXlxe212WMMX3R8uXL96lqVqh9sWF+7geAHwIp7v4g4ICq1rv7u4Dhbns4sBPAJZKD7vjhwJKgcwY/JqS8vDwKCgq65QUYY0x/ISLb29oXtmYoEfk8UKKqy8P1HC2eb66IFIhIQWlpaU88pTHG9Bvh7LM4A/iiiGwDngXOBf4HSHfNTAC5wG63vRsYAeD2pwH7g8tDPKaRqj6kqvmqmp+VFbIWZYwx5iiFLVmo6u2qmquqeXgd1O+o6leARcDl7rDrgJfd9nx3H7f/HfU6VOYDV7nRUqOAscDScMVtjDGmtXD3WYTyI+BZEfkF8DHwiCt/BHhSRAqBMrwEg6quFZHngXVAPXCTqvp7PmxjjOm/wjoaKlLy8/PVOriNMaZrRGS5quaH2mdXcBtjjOmQJQtjjDEdsmTRQnWdnxcKdtIXm+eMMeZoRaKDO6rdv3ATf1q8hfTkeM6fmB3pcIwxJipYzaKFfRW1AByoqo1wJMYYEz0sWbQQGyMA1DdYM5QxxgRYsmjB57NkYYwxLVmyaCHO1Sz8/oYIR2KMMdHDkkULvhjvV2I1C2OMaWLJooVY1wzlt2RhjDGNLFm0ECPWZ2GMMS1ZsmghMBrKahbGGNPEkkULPhs6a4wxrViyaCGusc/CRkMZY0yAJYsWbDSUMca0ZsmihcY+C78lC2OMCbBk0UKM9VkYY0wrYUsWIpIoIktFZJWIrBWRn7nyx0Rkq4isdLcprlxE5LciUigiq0XklKBzXScim93turaeszvVW5+FMcY0CucU5TXAuapaISJxwHsi8rrb9wNVfbHF8RcCY91tBvAHYIaIZAJ3APmAAstFZL6qlocj6MA6FvXWDGWMMY3CVrNQT4W7G+du7X0CXww84R63BEgXkRxgNrBQVctcglgIzAlX3A0uWTTY4kfGGNMorH0WIuITkZVACd4H/kdu112uqel+EUlwZcOBnUEP3+XK2ioPi0BXheUKY4xpEtZkoap+VZ0C5ALTReRE4HZgAjANyAR+1B3PJSJzRaRARApKS0uP+jyBGoXlCmOMadIjo6FU9QCwCJijqkWuqakGeBSY7g7bDYwIeliuK2urvOVzPKSq+aqan5WVdQyxNv9pjDEmvKOhskQk3W0nAecDG1w/BCIiwCXAGveQ+cC1blTUacBBVS0CFgAXiEiGiGQAF7iysGhoCNQsLFsYY0xAOEdD5QCPi4gPLyk9r6qviMg7IpIFCLASuNEd/xpwEVAIVAFfA1DVMhH5ObDMHXenqpaFK+jGyyssVxhjTKOwJQtVXQ1MDVF+bhvHK3BTG/vmAfO6NcA2WJ+FMca0ZldwtxBIFjZFuTHGNLFk0UJjsrAebmOMaWTJooVAhaLBahbGGNPIkkUQVeVIrR+wZihjjAlmySLIvopaHvtgG2DTfRhjTDBLFkECa1mATVFujDHBLFkEifU1JQtrhjLGmCaWLILE+Zp+HdYMZYwxTSxZBAluhrKahTHGNLFkEcQXlCxsoTxjjGliySKIN7ehxy7KM8aYJpYs2mDNUMYY08SSRRusg9sYY5pYsmiD1SyMMaaJJYs2WLIwxpgmlizaYM1QxhjTxJJFG6xmYYwxTcK5BneiiCwVkVUislZEfubKR4nIRyJSKCLPiUi8K09w9wvd/rygc93uyjeKyOxwxRzMcoUxxjQJZ82iBjhXVU8GpgBzROQ04NfA/ao6BigHbnDH3wCUu/L73XGIyETgKmASMAf4vVvXO6zq7ao8Y4xpFLZkoZ4KdzfO3RQ4F3jRlT8OXOK2L3b3cfs/K95VchcDz6pqjapuBQqB6eGKO8ByhTHGNAlrn4WI+ERkJVACLAQ+BQ6oar07ZBcw3G0PB3YCuP0HgUHB5SEeEzbWZ2GMMU3CmixU1a+qU4BcvNrAhHA9l4jMFZECESkoLS095vPZdB/GGNOkR0ZDqeoBYBEwE0gXkVi3KxfY7bZ3AyMA3P40YH9weYjHBD/HQ6qar6r5WVlZxxxz6eEa9lfUHPN5jDGmLwjnaKgsEUl320nA+cB6vKRxuTvsOuBltz3f3cftf0dV1ZVf5UZLjQLGAkvDFXewBxcV9sTTGGNM1Ivt+JCjlgM87kYuxQDPq+orIrIOeFZEfgF8DDzijn8EeFJECoEyvBFQqOpaEXkeWAfUAzepqj+McTeqrrNebmOMARDtg23z+fn5WlBQcFSP3Vh8mLV7DnLr86sAeO3mzzBxWGp3hmeMMVFJRJaran6ofXYFdwvjh6Zw2Sm5/GiO1xf/yHtbIxyRMcZEniWLNtx49miyUxPYUVYZ6VCMMSbiLFm0QUSYNW4IW/dVRToUY4yJOEsW7chJT2RfRQ019T3Sn26MMVHLkkU7hqUlAbCl1JqijDH9myWLdkwekQbAEx9ui2gcxhgTaZYs2jFhaCqDBybwzNKdLNpYEulwjDEmYixZdGCKq10sWFMc4UiMMSZyLFl04L4rpwCwdGsZffECRmOM6QxLFh1IS4rjiycPY8u+SjYUH450OMYYExGWLDrhW+ccD8CmvZYsjDH9kyWLThg9eCBxPmFd0aFIh2KMMRFhyaIT4mNjOHF4Gsu3lUc6FGOMiQhLFp2Uf1wGq3cdpLrOruY2xvQ/liw6KT8vk1p/A2t2H4x0KMaYEJZuLeP83/yTI7X2hS4cLFl0Uv5xGQAss6YoY6LST+evZXNJBZ+WVkQ6lD7JkkUnDRqYQN6gZFbuLOdHL67m7fV7Ix2SMSZIfYO3sqUvRiIcSd8UzjW4R4jIIhFZJyJrReQWV/5TEdktIivd7aKgx9wuIoUislFEZgeVz3FlhSJyW7hi7sgJOamsLzrMcwU7ueHxo1uJzxgTHvV+76JZSxbhEc41uOuB76vqChFJAZaLyEK3735VvTf4YBGZiLfu9iRgGPCWiIxzu38HnA/sApaJyHxVXRfG2EMal53C6zbthzFRqb7BSxZ1/oYIR9I3hS1ZqGoRUOS2D4vIemB4Ow+5GHhWVWuArSJSCEx3+wpVdQuAiDzrju3xZDF9VGZPP6UxppPqXZKo89u0POHQI30WIpIHTAU+ckXfFpHVIjJPRDJc2XBgZ9DDdrmytspbPsdcESkQkYLS0tJufgWe/LyMjg8yxkREndUswirsyUJEBgJ/Ab6rqoeAPwDHA1Pwah73dcfzqOpDqpqvqvlZWVndccpWEmJ9XD19RFjObYw5No01i3pLFuEQ1mQhInF4ieJpVX0JQFX3qqpfVRuAh2lqatoNBH8S57qytsoj4mdfPJEhKQkMHpgQqRCMMSEEOrhrrWYRFuEcDSXAI8B6Vf1NUHlO0GGXAmvc9nzgKhFJEJFRwFhgKbAMGCsio0QkHq8TfH644u5IfGwMV+TnUl5VS0ODtY0aEy0CHdxvry+xST/DIJyjoc4ArgE+EZGVruw/gKtFZAqgwDbgGwCqulZEnsfruK4HblJVP4CIfBtYAPiAeaq6Noxxd2jwwAT8DcqBI3VkDoiPZCjGmBaeXLKdJ5dsZ9vdn4t0KH1KOEdDvQeEGvD8WjuPuQu4K0T5a+09rqcFmqDeXr+XK/KtD8OYaGDXV4SXXcF9FNKT4wD4wYurrSnKmCghlivCypLFUZicm964XXK4JoKRGGMCYixbhJUli6OQlhTHn645FYB9FZYsjIkGbTVDPfb+VvYequ7haPoeSxZHafBAr2PbkoUx0SFUrthz4Ag//fs6vm5zuR0zSxZHKdDJva+iNsKRGGMgdDNUoKjooNUsjpUli6M0JCWROJ+wfHtZpEMxxhA6WTy0eAsANbbC5TGzZHGUkuJ9XDxlOK+sKmqcZsAYEzkN2npk4qPvbwOgxqYAOWaWLI7BOeOHcLimntW21KoxEedvZxi7TQFy7CxZHIOZxw8C4N1N+yIciTHGH6JmYbqPJYtjkDkgnlOPy+D+tzaxofhQpMMxpl/z2zoWYWXJ4hh959wxALy0ImIT4RpjaJpI0ISHJYtjNGv8EE4/fhCLN4VnwSVjTOdYM1R4WbLoBmeNy2JD8WG7StSYCGrZwW3ztnUvSxbd4Kyx3sp8/7TahTERoaqtkkXLEVC1Nnz2mFiy6AYn5KQwIjOJl1bsinQoxvRLoSoRLZNFRU19D0XTN1my6AYiwtXTR7JkSxmfllZEOhxj+p1Q11i0rElUVFuyOBadShYickVnyvqzL52SC8Aba4ojHIkx/U9nksXuA0cotSUFjlpnaxa3d7KskYiMEJFFIrJORNaKyC2uPFNEForIZvczw5WLiPxWRApFZLWInBJ0ruvc8ZtF5LrOvrielJ2ayPjsFJZvL490KMb0O/UNrfsjWiaLqx9ewrS73uJgVV1PhdWntLusqohcCFwEDBeR3wbtSsVbJ7s99cD3VXWFiKQAy0VkIXA98Laq3i0itwG3AT8CLgTGutsM4A/ADBHJBO4A8vHW7V4uIvNVNeo+lcdmD2T1Lpv6w5ieFiJXNOuzODk3jVXuf7Po0BHS3GqXpvM6qlnsAQqAamB50G0+MLu9B6pqkaqucNuHgfXAcOBi4HF32OPAJW77YuAJ9SwB0kUkxz3PQlUtcwliITCnS6+yh4zPTmFneRXllTZtuTE9qaOaxeljBjdu77dlBY5Ku8lCVVep6uPAGFV93G3PBwq78s1eRPKAqcBHQLaqFrldxUC22x4O7Ax62C5X1lZ5y+eYKyIFIlJQWhqZIaznTBiCKsxftSciz29MfxXqgryaem9a8pvPHcOoQQMay7/y5496LK6+pLN9FgtFJNU1Ca0AHhaR+zvzQBEZCPwF+K6qNptASVUVr2npmKnqQ6qar6r5WVlZ3XHKLps0LJVTRqbzx39+2vhGNcaEX6gO7ooa73/w7PFZnHJcerN9ald7d1lnk0Wa+6C/DK+paAbw2Y4eJCJxeIniaVV9yRXvdc1LuJ8lrnw3MCLo4bmurK3yqCMifHPWGIoOVrNki7coUp1NjWxM2IVMFm6o7ICEWMYMSWHdnbOJ83kLJO23puIu62yyiHUf7FcCr3TmASIiwCPAelX9TdCu+UBgRNN1wMtB5de6UVGnAQddc9UC4AIRyXAjpy5wZVHpzDGDiffF8O6mUhZtKGHsj19nfZHNSGtMOIVKFpXuIryBCd44nuT4WP50zakAnP3fi6x20UXtjoYKcifeB/T7qrpMREYDmzt4zBnANcAnIrLSlf0HcDfwvIjcAGzHS0AAr+GNvCoEqoCvAahqmYj8HFgWiEVVo3Yt06R4H9NGZfDiil2sc0li+fZyTshJjXBkxvRdgRlnM5LjKHdDYytaJAuAERnJAFTW+qmpbyAxztfDkfZenUoWqvoC8ELQ/S3Alzp4zHtA60VxPa2asFz/xU1tnGseMK8zsUaDa2fm8Y0nl/PBp/uBbuqUMca0KdDc+7UzRvGbhZuApmQxIChZ5LpkAXC4ut6SRRd09gruXBH5q4iUuNtfRCQ33MH1VhdMzG5239boNia8qmq9zuyTctO4/vQ8UhNjqaypJyE2hjhf08dcUnxTcrC5orqms30Wj+L1KQxzt7+7MhOCiPDo9dMa75fbFaPGhFW1SxZJcT5EQBUO19Q3a4IK+PnFkwA4XG3/l13R2WSRpaqPqmq9uz0GRGZ8ai9xzoQhjds2H40x4RWoWSTH+/CJ4FelsqaegYmtk8XY7BTAJhbsqs4mi/0i8lUR8bnbV4H94QysL3jr1rOJ8wmFJYcjHYoxfdqRuqaahS9G8Dd4yWJAfOtkkeISyGFrhuqSziaLf8UbtVQMFAGX483xZNoxZshALpuay7Jt5XxQuC/S4RjTZx0JNEPF+4iJERpUOVwdumaRmujNCxVqWp6SQ9VRf21U3m2vcuIdC1i0oaTjg7tRZ5PFncB1qpqlqkPwksfPwhdW33HDZ0YB8PfVRR0caYw5Gg0NSlWtV0tIivMRI95iSJW1ofsshqUnkRgXw6a9zdee+bS0gum/fJs/v7u1U897sKqOm5/5mJJjWE65sqae6x9d2ulrsapdDaqipp6vPbasg6O7V2evs5gcPBeUu/Zhaphi6lPGZadw6dTh/GXFLhJiY5h5/CBmTxoa6bCM6RMqa+qZdEfTNbrJ8bFen0WDUlFdz6jBrT/ifDHChKGprT6g316/F4AVO8pRVbzritv2/qf7mL9qD/sra5h3/TQSYrs2DFdVG2OP88Xw8LX5HT6m5FDz/s8XCnYS54vhkqmtpsvrdp2tWcQE1p0Ab00KOp9o+r0vnZJLbX0Dj32wjW88uTzS4RjTZxw40nxEU0JsDLFuqOyBI3UhaxYAE4elsq7oULOruIsOejWEhev2cvrd74S8wvtAVS0PvLWJmnp/41Xj7xfu58Q7FnT5ivDg5QwGD4zv1GP2Hm5ei/nBi6v57nMrG2sc4dTZZHEf8KGI/NxdTf0B8N/hC6tvmZCT0ridEqIN1RhzdPz+5h/QMTFCxgDvg/dAVR0DE0J/2z8hJ5WDR+ooDmpC2hu0XXSwmjLXp6GqbN1XCcC185bywFubeWd9CYeDRlPV+bXdJZVVlXc3l9IQNC1J4JyBWDtjz4EjIctveHwZNzy2jB37qzp1nqPRqWShqk/gTSK4190uU9UnwxZVHzN4YELj9pCUhHaONMZ0RXWI2Z2zgv7fhqYlhXzciAyvPPjDd++hGkYPHsDV0715SwM1jUfe28o59/6D5dvLGmsD33x6Rash8V/43/d5Y00Rb63b21hWU+9n94EjLFhbzDWPLOXJJdsb9wUS1dSR6by+pphZ9yzqcJj9i8t3AbDmZ7P58PZzefHGmYBXu3l7Qwln3bOIwpK2k9ax6GzNAlVdp6oPutu6sETTD/hi2m8HNcZ0XmAUVLCslKYmnbxBya32g7cMMngJIuBAVS0n5KRy1bSRQFMi+ecmb32cjcXNP4Tvf2tT81jq/Nz41Aq+/kRBY5PUn9/dyhl3v8MvXl0PwK5y75u/qnL36xsAGJM1EIBt+6t4d3Pba/Es317Gu5v3kZOWyMCEWHLSkpg4LJWzxmWRHrTy36W/f7/NcxyLTicLc2ye+NfpAM2qrsaYzvnBC6v4i/tWHSxUW/2QlMTG7TFDBoY831CXLIoPNjU9Ha6uJyUxlpx0b1/RwWoOV9fx7mZv2HtXhtRucU1MH+/wxgXtKvcST2DqkRfca8lOTSAnrSneW59fxfb9lbS0ofgQX/rDhwDcd+XJjeXJ8bE88a/TWflfF3DVNK9GFBga3N0sWfSQs8Zl8bUz8uyqUWOOwgvLd/H9F1a1Kj8SIlmMyEzm0eun8fotn+G4oBXygqUnxxEfG9Osw7iixksWgwckEOcTtu6r5J4FGxv3d2UmhkByaNkX8ft/fMqba4v54YurGZmZzKJ/n0VOevOmsldCDLPf7voiLps6nNOPH9xqP8C3Zo0B4IwxgzodZ1dYsuhBGcnxHK6pZ3cbnVTGmK6prgv9bf+cCUPaXRZARMhOTWCvq1nU+xuoqvUzMCGOmBihzq889sE2nviwqY/hwUWFzc7xx6+eylM3zADgopOGNuuP3HuomoYGpbC0gjPHNP9wn+tGRP7xq6eSHB/bKs57Fmyk5FA1izeVoqoUH6xuvIDw+7PHt/maRg5K5rm5p3HnxSe2ecyxsGTRgy47ZTi+GOGHL66yhVeM6QbHMmQ0OyWxsc+i0i3BGhitOKeda6HOOyGbP1+bz5wThzLz+EF877xx/OfnJ3LTOWMaj1m35xCvrynmQFUdF54U+lyjs7xaz6RhrZPaD/+ymmvnLeX3//iU0371Nr98zevzyExuf4jtjNGDwjbtuo3j7EG5GclMGZHO+4X7Of/+xbx169mRDsmYqNfeF6tjShZpiazbc4if/X0tH7llkAPTgzxw1RR8McL/fbSDlMRYbn2+qQnszosnMcw1HflihFvOGwvAdafncfmpuUy6YwGPfbCNxz7YxtDURD53Ug5xMTGcclw6ew5Uc+28pZx3Qnbjh3qcL4ZPf3kRx//Ha43PsanYm0/u/z7aAcAh13wdPMV6T7Nk0cP+9+qpnH73OxSWVPDe5n2cOTZ0+6MxxhNqydSAUH0WnTVpWCqvri5qdr1DhvvmHvggv+70PLc8a1OySEtquwN5QEIsQ1ISKHH9Gw9+eSrpyfFc6Tqfj88ayD/+fRYjM5uP0vLFCI9+bRoCXP/oMva45rFoarIOWzOUiMxzCyWtCSr7qYjsFpGV7nZR0L7bRaRQRDaKyOyg8jmurFBEbgtXvD1lWHpSY9vmVx/5KMLRGBP96vxtJ4vKoJlj2xom25ZLQ0yRMXVkequypBbNOgPauCo84IUbZzIwIZavnjaS/LzMZvtEhLzBA4gJMYT+nPFDOGtsVsjh9ZNz01jxn+e3+7zhFs6axWPAg8ATLcrvV9V7gwtEZCJwFTAJb3Glt0RknNv9O+B8YBewTETm9/brPO678mSueWRppMMwpleoDTFkVVXZtLeC9UWHSYyL4Y1bzmq8cruzctKS+M65YxiZmczIzGSWbStrdgFtQKgP9vYcN2gAa342u+MDQ4hx06uD12n+2ifFAMz/9plHdb7uFLaahaouBso6efjFwLOqWqOqW4FCYLq7FarqFlWtBZ51x/ZqZ44ZzInDU0mK81lHtzEdCHV9w5NLtjP7gcW8+kkRqYlx5A0e0G7zUFu+f8F4rsgfwYzRg/j2uWPbPO5P15za5XMfrc9PzgHgF5ec1GPP2RmRGA31bRFZ7ZqpApMTDgd2Bh2zy5W1Vd6KiMwVkQIRKSgtbfsqyGggIvzLtJEcqfOzdGtn86kx/VOoZLFsW+Mk2MR0MDtsd5g9aSiP/+t0/vLN08P+XPdecTLLfnwema6mdPKI1k1jkdDTyeIPwPHAFLxFlO7rrhOr6kOqmq+q+VlZ0b/i66VTh5OTlsidr/TqFjVjwq6uvnXtOymu6aOrp9bSPntcFqcel9HxgccoMc5HluvXXP6T83j2304L+3N2Ro8mC1Xdq6p+VW0AHsZrZgLYDYwIOjTXlbVV3usNTIjlS6fksnbPISpseUdj2hTcZ1FT37R8akB9O6OlertBAxMiOlw2WI8mCxHJCbp7KRAYKTUfuEpEEkRkFDAWWAosA8aKyCgRicfrBJ/fkzGH0/RR3kiJBxZu6uBIY/qfipp6bnxyOTvLmqbdHv+TN6iu85MQlCx+eWl0te33VWEbDSUizwCzgMEisgu4A5glIlMABbYB3wBQ1bUi8jywDqgHblJVvzvPt4EFgA+Yp6prwxVzTztrXBbZqQlsCzFxmDH93aur9/DG2uJm10EA3P36Bha72Vkf/PJUPj95WCTC63fClixU9eoQxY+0c/xdwF0hyl8DXmv9iL5hXHYKb60voabe3+VlGY3pywKtS3UNzTu4H/tgG+DN2GqJoufY3FARFriSc/xP3rC+C2OCBEaVbykNXfOOjbGPr55kv+0I+9GFExq3r35oSQQjMSZ6LNtWxn/89ZN2j4mmqTD6A0sWERa8UMknuw+2c6Qx/cOh6jrmPlHQqvyOL0xs3B6YENu42I/pGZYsosDSH3+W60/PA5qGBhrTH1XV1jPtF29RXtX62onguZxW33EBd39pck+G1u9ZsogCQ1ISOSEnBWi+zKMx/c3CdXupqQ+9oFFaUhzD0hK5+7KTujxfkzl2liyixIgMr6P7N3bNhenH3lhTzMAWs7oeNyiZv37rdESED27/LFdNHxmh6Po3W88iSswYPYhRgwdYv4Xpl6be+SafGZvFgrXFfP0zo3lo8ZbGff/8wTkRjMwEWM0iSvhihCvzR7CltJL9FZ1fGN6Y3sTfoNS3mBhQVSmvqmP+qj00KFwwMZuffO4EgFaLBJnIsZpFFJk+ypukbOnWMi48KaeDo43pfb7xZAHLt5fz8X9d0FhWWdt8UMeEnFTy8zLJz8tkREZST4do2mA1iygyOTedtKQ4/rR4i42KMn1OdZ2ft9aXUF5VR21QJ3ZwTTolIbaxz2LKiHQGhViMyESGJYsoEueL4ebPjmXlzgPMe29bpMMx5qh9+Ol+3lhT3Kzs4x0HGrcDo/78Dcr6osON5UPTEnsmQNNlliyizA1njuLkEen8+o0NdoWq6bWufngJNz61vFnZhuJDjdtLt5XR0KD87zubmx03LN2anaKVJYsodOkUb3K0e97YEOFIjOk+e4K+/Pz7C6v49YINLNrYfFXLEZmWLKKVJYsoFBhH/reVezhQVRvhaIw5eoeqvf6JW59bycPvbmX04AGN+/70zy0kB61Lcev547j1/PGRCNN0giWLKJQY9A+0I2jhF2Oi0Ztriyk9HHq49yW/e59572/lpY+9BS79qsT7mj52Ptyyv3H75s+ObVx32kQfSxZR6j8/702atn2/JQsTvarr/Mx9cjnXzlsacv+W0krufr2pOfW2ORP4v3+bwexJ2fhsyo5exa6ziFJfmTGSX7+xgZdX7mHOiUOJ81leN9Hn0BFvwr/1RYc6OBJevHEm+XneUsL5eZm8urqIgu1lnDN+CPUNoeeDMtEjbJ9AIjJPREpEZKZRgTsAABlFSURBVE1QWaaILBSRze5nhisXEfmtiBSKyGoROSXoMde54zeLyHXhijfaJMb5mD1pKG+t38v9Nl+UiUKHq+u44IHFrco1sGpRkNsvnNCYKAI+NzmHO74wibPGZXHuhOywxWm6Rzi/rj4GzGlRdhvwtqqOBd529wEuBMa621zgD+AlF7y1u2cA04E7AgmmP7j3islkpybw9Ec78De0/gc0JpK276/iQIipxF8o2NXs/s3njmHuWaN7KiwTJmFLFqq6GChrUXwx8Ljbfhy4JKj8CfUsAdJFJAeYDSxU1TJVLQcW0joB9VkJsT7OnTCEg0fqmHLnm1TasqsmioRaBvjt9Xv54V9WA/DZCUMAqPE3IGL9E71dTzeEZ6tqkdsuBgJ1z+HAzqDjdrmytspbEZG5IlIgIgWlpaWhDumVvnfeOE7OTeNwdT3rOtEubExPqahunSxueNxb4W5oaiK//+opfHPW8XzjrON7OjQTBhHrNVWvYbPb2lZU9SFVzVfV/KysrO46bcQNSU3kD189FYAbHlvGoerW1X5jIqGytu2arohXM/7RnAk2HLaP6Olksdc1L+F+lrjy3UDwgrq5rqyt8n4lJy2RL5w8jEPV9Uz+6Zts2nu44wcZE2aBZqhrTjsOgF3lTcO8Y6zZqc/p6WQxHwiMaLoOeDmo/Fo3Kuo04KBrrloAXCAiGa5j+wJX1q+ICA/8y5TG+/cs2BjBaIzxBJqhApP/fbTF66LMTk3gwS9PjVhcJjzCdp2FiDwDzAIGi8guvFFNdwPPi8gNwHbgSnf4a8BFQCFQBXwNQFXLROTnwDJ33J2q2rLTvF/wxQi5GUnsKj9CWlJcpMMxpnHARVaKN4340q3ev+bCW88mNdHeo31N2JKFql7dxq7PhjhWgZvaOM88YF43htZrPXXDDGbd+4+QHYvG9LSyqlrSkuIaE8PizaWMzEy2RNFH2WXBvUje4AFcdNJQ3t6wl8KSikiHY/q53eVHGJ6eRGqS952z6GA1XzjZVnjsqyxZ9DLfPHsMvhjhe8+tjHQopp/6x8YSVu48wO4DRxiekcTQ1KYFi75z7tgIRmbCyeaG6mVOyk3jxrOP54G3NlNYcpgxQ1IiHZLpJ47U+lGU6x/1uhAHxPs4/fjB5KQ1rUERPGOy6VusZtELTcxJBeC83yymwaYBMT1k2l1vceIdTYMRK2v95GYkkRRvCaI/sGTRC501LotRbhGZVz4p6uBoY7pHRU09Lb+bDHfLoD50zam88p0zIxCV6SmWLHqhxDgfb996NolxMazccSDS4Zh+bFSW96XlgklDOXF4WoSjMeFkyaKXiokRxgwZyLz3t/LGGqtdmPCqqfe3KrvwxKFMGJoagWhMJFiy6MUumeLNqXjjUysoOngkwtGYvqzkUPNlU4/PGtA4Z5npHyxZ9GJfmXFc4/bMX73Dt55eHnLhGWOOVUmLNba/eHLIyZ9NH2bJohdLivfx6s1NnYqvfVLMxzutD8N0v5JD1c3ux8faR0d/Y3/xXm7SsDReuHEml031vumt2F4e4YhMX7S3RbJIsGTR79hFeX3AtLxMpuVl8sGn+1m3xxZIMt1vz0GrWfR39hfvQ0ZkJrHrgHV0m+63bV8lKYlN3y2tZtH/2F+8D8lOTWTp1jKeXLKdg0dsRT3Tfbbvr2LGqEwmDfOGyvpibHGj/saSRR8yLS8TgP/82xp+/NdPePCdzRS3aD4wpqtUle1llRw3aABnjBkMwK5yq8H2N5Ys+pBrZx5HwU/OY/qoTF5ZXcS9b27i9pdWRzos08uVHK6huq6BvEHJfGXGSBJiY5g9aWikwzI9LCLJQkS2icgnIrJSRApcWaaILBSRze5nhisXEfmtiBSKyGoROSUSMfcGIsLggQl8ZcbIxrJFG0vZsb+qnUcZ077t7v0zctAAjhs0gI2/uJDxQ2224/4mkjWLc1R1iqrmu/u3AW+r6ljgbXcf4EJgrLvNBf7Q45H2Ml+YPIx7Lp/ceH/FDhtOa47evgrvgrwhbvlU0z9FUzPUxcDjbvtx4JKg8ifUswRIFxFbjqsdMTHCFfkjGi/Y++5zK5m/ak+EozK91X6XLAYNjI9wJCaSIpUsFHhTRJaLyFxXlq2qgRnxioFstz0c2Bn02F2uzHRg0rA0vnH2aABufuZjlmzZH+GITG+0v7IWgIxkSxb9WaSSxZmqegpeE9NNInJW8E71Jjjq0iRHIjJXRApEpKC0tLQbQ+3dzjshu3H7qoeW8MzSHRGMxvRG+ytqSU+OI84XTQ0RpqdF5K+vqrvdzxLgr8B0YG+gecn9LHGH7wZGBD0815W1POdDqpqvqvlZWVnhDL9XmZaXyT/+fVZjp/edf1/HxuLDEY7K9CZllbUMGmC1iv6ux5OFiAwQkZTANnABsAaYD1znDrsOeNltzweudaOiTgMOBjVXmU7IGzyAuy49iXd/eA5H6vzMfmAx2/ZVRjos00vsq6hh0ADr3O7vIlGzyAbeE5FVwFLgVVV9A7gbOF9ENgPnufsArwFbgELgYeBbPR9y3zAiM5kZo7wL92bd+w+bztx0yv7KWuvcNj0/kaCqbgFODlG+H/hsiHIFbuqB0PqF574xk0t//z4f7zjALc+u5DdXnkystUWbdpRZsjBE19BZ00Me+9p0zhwzmPmr9vD5/32PN9YUU3LYpgUxrfkblPKqWjKtGarfsynK+6G0pDie+voMXv+kiB//bQ03PrUcgM+MHcy1M/M4f2J2B2cw/UV5VS2qMNhqFv2e1Sz6sQtPyuGPQesov7t5H//2REEEIzLRZn+Fd42FdXAbSxb93PRRmbxw48xmZX+3q72NE3gvZNrQ2X7PkoVhWl4mW391EX/91ukAfOeZj3lqyXYbLdXPbSw+zIOLCgFrhjKWLIwjIkwdmdFYy/jJ39Yw6vbXOOmOBby5tjjC0ZlIKK+qbdwenpEUwUhMNLBkYZqZlpfJQ9c09WMcrqln7pPLuePlNRGMykRCmZsT6gezx5Mcb2Nh+jtLFqaVCyYN5c3vncXlp+Y2lj3+4XbybnuVs+9ZxIK1xdz16jqWbSuLYJQm3AITCAa/D0z/ZcnChDQuO4V7rziZrb+6iNdv+Uxj+fb9VXzjyeU8/O5W/uvltRGM0IRbuc02a4JY3dK0S0Q4ISeV9350DrvKj3DtvKXU1jcAsL7oEHm3vUpyvI+8QQPIz8vgzotPjHDEpjvsLKviNws3ARAfa98pjSUL00m5GcnkZiSz6RcXsrOsivKqWm55diVb91VSVetnXdEh1hUd4u31JUwZkc4dX5jIln2VnDZ6UKRDN11UXefnM/+9KNJhmChjycJ02YjMZEZkJvPc3NNoUG/UzLubS9lQdJiXPt7N7gNHePUTb2LgH190Ap+bnMOwdBtN01ts228zEpvWLFmYozYkNRGAoWmJnJCTCsD1Z+RxxR8/pMY1Vd312nruem09E3NSOW30IFKTYhmR4SWbaXkZVNc1kBTvi9hrMK1tKfWSRVKcj6f/bUaEozHRwpKF6VaTc9P54LZzeWrJDqblZVBWVcvDi7ewvugw64oOhXzMO98/m9FZA3s4UhNKyaFqnlm6A1+MsPw/z7Mhs6aRvRNMtxs0MIFbzhvbeP/zk4dRXefn0fe38bePdzM4JZ73C5vWAz/3vn+SkRzH4IEJbC6pYHh6EnPPGs2Hn+5nbPZALj81l+zUROoblIEJ4XnLqioPLd7CuROGMDY7JSzP0Rt8/YkCVu86yHGDki1RmGakL07pkJ+frwUFNiFeNPM3KNV1fp5asp1fvb6BAfE+auobqG9o//140UlDOWf8EDYUH+YrM0YeU42kus5PvC+GmBihvLKWqT9fCMAP54znW7PGHPV5g20praCmvqGxmS6aFR08wsxfvQPAPZdP5or8ER08wvQ1IrJcVfND7rNkYaKFv0E5Uudn275Klm0rIzcjmYqaOsoq63h55W5W7zrY6jG5GUkMS09iREYyw9ITGZGRTHKCj3q/cqi6jqQ4H8cNGsCJw1PZUlrJicPTAO9DfM4D73LRSUN54KqpfLyjnEt//0HjeR//1+nMHD3omIeN5t32KgDb7v7cMZ2nLarK8u3ljBuaQmpi3FGf55mlO7j9pU8A+NxJOfzuK6d0V4imF2kvWfSaeqaIzAH+B/ABf1bVuzt4iOllfDHCwIRYThye1vihHnDDmaNQVfZV1FKwrYz3CvexvugQ+ytrOVhVx8c7dlPnb/uLT5xPqPMrY4cMZERmMu9sKAHgbyv3sHjzPmJEALhgYjaFpRVcN28pALPGZ3Fl/giOzxpIdmoCCbE+ausbqGtooORQDQAThqYgAjvLjlDX0MDxIWo7H366n+o6P7PGZyHuucD7sH/6ox0s3VrGfVeeTFwXVy18fU0x33p6BVdPH8GvLpvcpccGu99dU3HZ1OH85PMTj/o8pu/qFTULEfEBm4DzgV3AMuBqVV0X6nirWfQ/df4GyqtqWbKljHhfDMdnDcAXI1TV+nlnQwmfllag6jW1bN1Xxb6KGq7MzyVzQAKb9h5mc8lhZo4exK+/NJnt+6v47dubWbC2mMpaf6eeP94XQ63fGwE2NDWR9OQ46vwNfFrafBjq5Nw0xgwZyKhBA6hrUD7asp+PtnrTpkwYmkLeoAEUllZw4YlDOXF4GnE+ITHWR0Kcj3hfDIpS36DU1DUgAve9uZFl28oR8WoEF0wayoShKaQnxZGWHEdCbMcjzWrrGzjxjgV8ecZIfvrFSV38zZu+pNc3Q4nITOCnqjrb3b8dQFV/Fep4SxamO9TWN1Df0MCW0krWFR2iorqe6no/FdX1JMT6qKn3Ex8bQ71fqfU34IsRjtT6qaipp+jgEfYcqOb8idlcMmU4r68p4qUVu8lKSaD4YDXFh7xlbH0xws3njiUlMZb7F24ifUAcOWlJLN3a+Xm3vnraSOr9yutrijl4pK7ZvqQ4H4lxMTQoNKiiIX76VfE3KA9+eSqfnzysW3+HpnfpC8nicmCOqn7d3b8GmKGq3w51vCULE+0Ou/4Uv2rjt39VbWyi2nPgCGWVtdS7gQA19Q3U1jcQI16CifPF4G9QxmYPJCfNu+Cx3t/Asm3l7K+s4UBVHQeP1FFeWUutv4EYEUTwfgIxMU33YwSS42O54cxRJMbZNS/9WZ/os+iIiMwF5gKMHDkywtEY074U1xkd/A8Y3JcxLD2py1e9x/pimHm8Ta9iwqO3zBC2Gwgex5fryhqp6kOqmq+q+VlZWT0anDHG9HW9JVksA8aKyCgRiQeuAuZHOCZjjOk3ekUzlKrWi8i3gQV4Q2fnqaotpmCMMT2kVyQLAFV9DXgt0nEYY0x/1FuaoYwxxkSQJQtjjDEdsmRhjDGmQ5YsjDHGdKhXXMHdVSJSCmw/hlMMBvZ1UzjdyeLqGourayyurumLcR2nqiEvVOuTyeJYiUhBW5e8R5LF1TUWV9dYXF3T3+KyZihjjDEdsmRhjDGmQ5YsQnso0gG0weLqGourayyurulXcVmfhTHGmA5ZzcIYY0yHLFkEEZE5IrJRRApF5LYefu55IlIiImuCyjJFZKGIbHY/M1y5iMhvXZyrReSUMMY1QkQWicg6EVkrIrdEQ2wikigiS0VklYvrZ658lIh85J7/OTdLMSKS4O4Xuv154YgrKD6fiHwsIq9ES1wisk1EPhGRlSJS4Mqi4T2WLiIvisgGEVkvIjMjHZeIjHe/p8DtkIh8N9Jxuef6nnvPrxGRZ9z/QvjfX6pqN68pzgd8CowG4oFVwMQefP6zgFOANUFl/w3c5rZvA37tti8CXgcEOA34KIxx5QCnuO0UvLXQJ0Y6Nnf+gW47DvjIPd/zwFWu/I/AN932t4A/uu2rgOfC/Pe8Ffg/4BV3P+JxAduAwS3KouE99jjwdbcdD6RHQ1xB8fmAYuC4SMcFDAe2AklB76vre+L9FdZfcm+6ATOBBUH3bwdu7+EY8mieLDYCOW47B9jotv8EXB3quB6I8WXg/GiKDUgGVgAz8C5Gim35N8Wb3n6m2451x0mY4skF3gbOBV5xHyDRENc2WieLiP4dgTT34SfRFFeLWC4A3o+GuPCSxU4g071fXgFm98T7y5qhmgT+CAG7XFkkZatqkdsuBrLddkRidVXYqXjf4iMem2vqWQmUAAvxaoYHVLU+xHM3xuX2HwTCtQbpA8APgQZ3f1CUxKXAmyKyXLxliCHyf8dRQCnwqGu2+7OIDIiCuIJdBTzjtiMal6ruBu4FdgBFeO+X5fTA+8uSRS+h3leDiA1dE5GBwF+A76rqoeB9kYpNVf2qOgXvm/x0YEJPx9CSiHweKFHV5ZGOJYQzVfUU4ELgJhE5K3hnhP6OsXjNr39Q1alAJV7zTqTjAsC1/X8ReKHlvkjE5fpILsZLssOAAcCcnnhuSxZNOlznOwL2ikgOgPtZ4sp7NFYRicNLFE+r6kvRFBuAqh4AFuFVv9NFJLCoV/BzN8bl9qcB+8MQzhnAF0VkG/AsXlPU/0RBXIFvpahqCfBXvAQb6b/jLmCXqn7k7r+IlzwiHVfAhcAKVd3r7kc6rvOArapaqqp1wEt477mwv78sWTSJxnW+5wPXue3r8PoLAuXXuhEYpwEHg6rG3UpEBHgEWK+qv4mW2EQkS0TS3XYSXj/KerykcXkbcQXivRx4x30z7Faqeruq5qpqHt576B1V/Uqk4xKRASKSEtjGa4dfQ4T/jqpaDOwUkfGu6LPAukjHFeRqmpqgAs8fybh2AKeJSLL73wz8vsL//gpnx1Bvu+GNaNiE1/b94x5+7mfw2iDr8L5t3YDXtvg2sBl4C8h0xwrwOxfnJ0B+GOM6E6+qvRpY6W4XRTo2YDLwsYtrDfBfrnw0sBQoxGs6SHDlie5+ods/ugf+prNoGg0V0bjc869yt7WB93ek/47uuaYABe5v+TcgI0riGoD3LTwtqCwa4voZsMG9758EEnri/WVXcBtjjOmQNUMZY4zpkCULY4wxHbJkYYwxpkOWLIwxxnTIkoUxxpgOWbIwESMiH7ifeSLy5W4+93+Eeq5wEZFLROS/wnTuijCdd5a4WXGP4RzbRGRwO/ufFZGxx/IcJjpYsjARo6qnu808oEvJIuhq1bY0SxZBzxUuPwR+f6wn6cTrCrtujuEPeL8b08tZsjARE/SN+W7gM27dgO+5CQLvEZFlbm2Ab7jjZ4nIuyIyH++qVUTkb25ivLWByfFE5G4gyZ3v6eDnclfY3uPWAvhERP4l6Nz/kKZ1FZ52V8giIneLt57HahG5N8TrGAfUqOo+d/8xEfmjiBSIyCY3X1Rg4sNOva4Qz3GXeGt3LBGR7KDnuTzomIqg87X1Wua4shXAZUGP/amIPCki7wNPuivk/+JiXSYiZ7jjBonIm+73/We8i9ECV4i/6mJcE/i9Au8C50VDEjTHKFxXGdrNbh3dgAr3cxbuSmd3fy7wE7edgHd17yh3XCUwKujYwBW0SXhXtA4KPneI5/oS3gy1PrwZQ3fgTTU9C29Gzly8L1Ef4l29PghvuunABazpIV7H14D7gu4/BrzhzjMW74r8xK68rhbnV+ALbvu/g87xGHB5G7/PUK8lEW8G0rF4H/LP03SF+U/xZi8NrJPwf3gTDwKMxJvuBeC3NF0t/zkX22D3e304KJbgq54XAqdG+v1mt2O7Wc3CRKML8ObZWYk3HfogvA84gKWqujXo2JtFZBWwBG/CtI7ax88EnlFvxtq9wD+BaUHn3qWqDXjTmuThfehWA4+IyGVAVYhz5uBNsx3seVVtUNXNwBa8GXG78rqC1eKtWwDeB3peB6+xrdcyAW8Sus3qfYo/1eIx81X1iNs+D3jQxTofSBVv5uGzAo9T1VeBcnf8J8D5IvJrEfmMqh4MOm8J3gypphezqqGJRgJ8R1UXNCsUmYX3DTz4/nl4i7tUicg/8L49H62aoG0/3mIy9SIyHW/CtsuBb+PNJBvsCN5snsFazqOjdPJ1hVDnPtwb43Lb9bimZBGJwVtlrs3X0s75A4JjiAFOU9XqFrGGfKCqbhJvKdGLgF+IyNuqeqfbnYj3OzK9mNUsTDQ4jLdka8AC4JviTY2OiIwTb6bUltKAcpcoJuAtZxlQF3h8C+8C/+L6D7LwvikvbSsw9206TVVfA74HnBzisPXAmBZlV4hIjIgcjzfJ28YuvK7O2gac6ra/iLe8bHs2AHkuJvBmVG3Lm8B3AndEZIrbXIwbjCAiF+JN+oeIDAOqVPUp4B68acYDxuE1EZpezGoWJhqsBvyuOekxvPUf8oAVrmO2FLgkxOPeAG4UkfV4H8ZLgvY9BKwWkRXqTREe8Fe8dS9W4X3b/6GqFrtkE0oK8LKIJOLVDG4Nccxi4D4RkaAawA68JJQK3Kiq1a5DuDOvq7MedrGtwvtdtFc7wcUwF3hVRKrwEmdKG4ffDPxORFbjfU4sBm7Em/H0GRFZC3zgXifAScA9ItKAN3PyNwFcZ/wR9aYiN72YzTprTDcQkf8B/q6qb4nIY3gdxy9GOKyIE5HvAYdU9ZFIx2KOjTVDGdM9fgkkRzqIKHQAeDzSQZhjZzULY4wxHbKahTHGmA5ZsjDGGNMhSxbGGGM6ZMnCGGNMhyxZGGOM6ZAlC2OMMR36fwzIWnPq188UAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 800, loss: 1686.837779 \n",
            "guess_sentences:\n",
            "Th aive tho ov opspr vowspet ofwed aing hor, wha wt af wh theed aed condeuntherar.\n",
            "Mapand and ound forf lorghe owfy m tl-alded nerepivistraf ioc fa crn forr hhor pprthagigl\n",
            "nd sire wshern id foed und od wemtaf wlf sor ty cerspaved bsiv on wfersingsoun.\n",
            "piaglyer ao ling tho wonfey knwir ware le,t erser ond Thoed no oll-eas ant af ollye heedersairsspers noms htracd of hhroum whepr or thu ar-ed ulyeseponictagtlemngirc.\n",
            "pons aored se lhac fongule ad a ralvas whe thfe leemees;\n",
            "upwith ongitha kioh porinha nircidirpppirhitagtrturit semmatlesnoc keowee i;erth yees obles nom sund tho fortated bernderwiande theren ond bors Tat uand ounlerea nderes ledeperspir tho akr of y aicrey onlmrdnd ofllevact aftwir whes ka cedthove msero l.\n",
            "he ansnant atrcepwpenwon wo-mot foed bond pongreat iand tunirlend an ,oshorand cotthe lapt the of of a the consesteniw fon.ppot itrearniac a lyas encertend bo nghe deeslingy koncomb tungd ofe for fommodwersein the herrer \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import signal\n",
        "learning_rate = 1e-1\n",
        "\n",
        "def optimize(iteration = 10000, hidden_size = 8, T_steps =  100) :\n",
        "\n",
        "    n, pointer =  0, 0\n",
        "    smooth_loss = -np.log(1.0 / vocab_size) * T_steps\n",
        "    loss_trace = []\n",
        "\n",
        "    params = make_LSTM_parameters(hidden_size, vocab_size)\n",
        "    mems = []\n",
        "    for param in params:\n",
        "        mems.append(np.zeros_like(param))\n",
        "\n",
        "    for n in range(iteration):\n",
        "\n",
        "        try:\n",
        "            if pointer + T_steps >= len(data) or n == 0:\n",
        "                hprev, cprev = np.zeros((hidden_size,1)), np.zeros((hidden_size,1))\n",
        "                pointer = 0\n",
        "\n",
        "            inputs = ([char_to_idx[ch]\n",
        "                       for ch in data[pointer: pointer + T_steps]])\n",
        "            targets = ([char_to_idx[ch]\n",
        "                        for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
        "\n",
        "\n",
        "            loss, dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby, cprev, hprev = get_derivative_LSTM (params, inputs, targets, cprev, hprev)\n",
        "\n",
        "            # perform parameter update with Adagrad\n",
        "            for param, dparam, mem in zip(params,\n",
        "                        [dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby],\n",
        "                        mems):\n",
        "\n",
        "                mem += dparam * dparam\n",
        "                param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "\n",
        "            loss_trace.append(smooth_loss)\n",
        "\n",
        "            if (n % 100 == 0):\n",
        "                import matplotlib.pyplot as plt\n",
        "                from IPython import display\n",
        "\n",
        "                display.clear_output(wait=True)\n",
        "\n",
        "                plt.plot(loss_trace)\n",
        "                plt.ylabel('cost')\n",
        "                plt.xlabel('iterations (per hundreds)')\n",
        "                plt.show()\n",
        "\n",
        "                print ('iter %d, loss: %f \\nguess_sentences:' % (n, smooth_loss)) # print progress\n",
        "                for i in range(1):\n",
        "                    print(guess_sentence_LSTM(params, 'T', len(data)))\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            break\n",
        "\n",
        "    return params, loss_trace\n",
        "\n",
        "iteration = 801\n",
        "hidden_size = 50\n",
        "params, loss_trace = optimize(iteration, hidden_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "b3XfFExaaA86",
        "outputId": "5922e710-e48d-491a-d6bf-4a1d2762ae6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 800, loss: 159.539874 \n",
            "guess_sentences:\n",
            "That Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many Thrat Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many Thrat Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many Tho thee  hundred years ago, was certaisly not the last of her kind.\n",
            "Many Thrat Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many The three hundred years ago, was certainly not the last of her kind.\n",
            "Many Thret Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many Thh threinh horred wof her kind.\n",
            "Many Tha therdsh kind.\n",
            "Many Thr theee hundred years ago, was certainly not the last of her kind.\n",
            "Many Tho thee hundred years ago, was certainly not the last of her kind.\n",
            "Many Tho the last of her kind.\n",
            "Many Thrat Spaniihh what Sainly not the last of her kind.\n",
            "Many Tho liv\n"
          ]
        }
      ]
    }
  ]
}