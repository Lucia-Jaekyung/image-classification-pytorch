{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = open('input2.txt', 'r', encoding='UTF-8').read()\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMMUqN0_ZJFq",
        "outputId": "83a7eabb-cc1d-4866-dd00-c2b260a25d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "That Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many Theresas have been born who found for themselves no epic life wherein there was a constant unfolding of far-resonant action;\n",
            "perhaps only a life of mistakes, the offspring of a certain spiritual grandeur ill-matched with the meanness of opportunity;\n",
            "perhaps a tragic failure which found no sacred poet and sank unwept into oblivion.\n",
            "With dim lights and tangled circumstance they tried to shape their thought and deed in noble agreement;\n",
            "but after all, to common eyes their struggles seemed mere inconsistency and formlessness;\n",
            "for these later-born Theresas were helped by no coherent social faith and order which could perform the function of knowledge for the ardently willing soul.\n",
            "Their ardor alternated between a vague ideal and the common yearning of womanhood; so that the one was disapproved as extravagance, and the other condemned as a lapse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(\"data has %d characters, %d unique\" % (data_size, vocab_size))\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "print('char_to_idx', char_to_idx)\n",
        "print('idx_to_char', idx_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgZB7DJPZT3a",
        "outputId": "bbb268ec-19f4-404a-a2fb-792175d10023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 950 characters, 33 unique\n",
            "char_to_idx {'T': 0, 'l': 1, 'y': 2, 'k': 3, ' ': 4, ';': 5, 's': 6, 't': 7, 'S': 8, 'W': 9, 'm': 10, ',': 11, 'v': 12, '\\n': 13, 'x': 14, 'u': 15, 'i': 16, '.': 17, 'a': 18, 'g': 19, 'c': 20, 'b': 21, 'w': 22, 'f': 23, 'M': 24, 'd': 25, '-': 26, 'o': 27, 'h': 28, 'e': 29, 'r': 30, 'p': 31, 'n': 32}\n",
            "idx_to_char {0: 'T', 1: 'l', 2: 'y', 3: 'k', 4: ' ', 5: ';', 6: 's', 7: 't', 8: 'S', 9: 'W', 10: 'm', 11: ',', 12: 'v', 13: '\\n', 14: 'x', 15: 'u', 16: 'i', 17: '.', 18: 'a', 19: 'g', 20: 'c', 21: 'b', 22: 'w', 23: 'f', 24: 'M', 25: 'd', 26: '-', 27: 'o', 28: 'h', 29: 'e', 30: 'r', 31: 'p', 32: 'n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_LSTM_parameters (hidden_size, vocab_size):\n",
        "\n",
        "    Wf = np.random.randn(hidden_size, vocab_size+hidden_size) * 0.1\n",
        "    Wi = np.random.randn(hidden_size, vocab_size+hidden_size) * 0.1\n",
        "    Wg = np.random.randn(hidden_size, vocab_size+hidden_size) * 0.1\n",
        "    Wo = np.random.randn(hidden_size, vocab_size+hidden_size) * 0.1\n",
        "\n",
        "    bf = np.random.randn(hidden_size, 1) * 0.1 + 0.5\n",
        "    bi = np.random.randn(hidden_size, 1) * 0.1 + 0.5\n",
        "    bg = np.random.randn(hidden_size, 1) * 0.1\n",
        "    bo = np.random.randn(hidden_size, 1) * 0.1 + 0.5\n",
        "\n",
        "    # hidden -> output을 위한 Parameter\n",
        "\n",
        "    Wy = np.random.randn(vocab_size, hidden_size)  * 0.1\n",
        "    by = np.random.randn(vocab_size, 1)  * 0.1\n",
        "\n",
        "    return Wf, Wi, Wg, Wo, bf, bi, bg, bo, Wy, by\n",
        "\n",
        "\n",
        "def sigmoid (v):\n",
        "    return 1./(1. + np.exp(-v))"
      ],
      "metadata": {
        "id": "z5tQAb7mbwgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init Parameters\n",
        "hidden_size = 6\n",
        "Wf, Wi, Wg, Wo, bf, bi, bg, bo, Wy, by = make_LSTM_parameters(hidden_size, vocab_size)\n",
        "\n",
        "# forward pass\n",
        "# input_text = 'In mathematics and physics, a vector is an element of a vector space.'\n",
        "input_text = 'That Spanish woman who lived three hundred years ago,'\n",
        "xs, hs, zs, cs, ys, ps = {}, {}, {}, {}, {}, {}\n",
        "hs[-1] = hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "cs[-1] = cprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
        "\n",
        "print('-----------------------------')\n",
        "for t in range(len(input_text)):\n",
        "    ch = input_text[t]\n",
        "    print(str(t) +'th character is ' + ch + '\\n-----------------------------')\n",
        "\n",
        "    xs[t] = np.zeros((vocab_size, 1))\n",
        "    xs[t][char_to_idx[ch]] = 1\n",
        "    print('\\t' + ch +'\\'s one-hot encoding is')\n",
        "    print('\\t'+str(xs[t].T))      \n",
        "    print('\\t Let us denote this vector by xs['+ str(t) + ']\\n' )\n",
        "\n",
        "    print('\\t zs['+ str(t) + '] < - Concatinate (xs['+ str(t) + '], h['+ str(t-1) + '] ):' )\n",
        "    zs[t] = np.concatenate((xs[t], hs[t-1]), axis=0)\n",
        "    print('\\t'+str(zs[t].T))       \n",
        "\n",
        "    f_raw = np.dot(Wf, zs[t]) + bf\n",
        "    i_raw = np.dot(Wi, zs[t]) + bi\n",
        "    g_raw = np.dot(Wg, zs[t]) + bg\n",
        "    o_raw = np.dot(Wo, zs[t]) + bo\n",
        "\n",
        "    f = sigmoid(f_raw)\n",
        "    i = sigmoid(i_raw)\n",
        "    g = np.tanh(g_raw)\n",
        "    o = sigmoid(i_raw)\n",
        "\n",
        "\n",
        "    cs[t] = f*cs[t-1] + i*g\n",
        "    hs[t] = o*np.tanh(cs[t])\n",
        "\n",
        "    ys[t] = np.dot(Wy, hs[t]) + by\n",
        "    print('\\t softmax of (ys['+ str(t) + ']) =')\n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "    print('\\t\\t'+str(ps[t].T) + '\\n')    \n",
        "\n",
        "    print('-----------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDBeaPXxcJzC",
        "outputId": "a304cbe2-fd95-44c6-b9b6-cd2a7c3c3113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "0th character is T\n",
            "-----------------------------\n",
            "\tT's one-hot encoding is\n",
            "\t[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[0]\n",
            "\n",
            "\t zs[0] < - Concatinate (xs[0], h[-1] ):\n",
            "\t[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t softmax of (ys[0]) =\n",
            "\t\t[[0.027843   0.03387965 0.03230799 0.02600186 0.02970198 0.03080868\n",
            "  0.03238236 0.03176233 0.02766051 0.03024863 0.03006616 0.02959527\n",
            "  0.03437871 0.03499112 0.03581522 0.02985335 0.02601842 0.02955141\n",
            "  0.03165055 0.02550027 0.03255083 0.03006258 0.03269984 0.02891663\n",
            "  0.03311597 0.02576293 0.02602184 0.02796628 0.03032848 0.02960942\n",
            "  0.0320077  0.03298615 0.02795389]]\n",
            "\n",
            "-----------------------------\n",
            "1th character is h\n",
            "-----------------------------\n",
            "\th's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[1]\n",
            "\n",
            "\t zs[1] < - Concatinate (xs[1], h[0] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.07629211  0.0426529   0.08339242\n",
            "   0.08179285 -0.0167482   0.00342301]]\n",
            "\t softmax of (ys[1]) =\n",
            "\t\t[[0.02792678 0.03386057 0.03226638 0.02601127 0.02955429 0.03058724\n",
            "  0.03233243 0.03190283 0.02771926 0.0303651  0.0302454  0.02979094\n",
            "  0.0346473  0.0352559  0.03582158 0.02964687 0.0261164  0.02963936\n",
            "  0.03169821 0.02548632 0.03236664 0.03000918 0.03280716 0.02906492\n",
            "  0.0330106  0.02560436 0.02576239 0.02808042 0.03005655 0.02959937\n",
            "  0.03173606 0.03304917 0.02797876]]\n",
            "\n",
            "-----------------------------\n",
            "2th character is a\n",
            "-----------------------------\n",
            "\ta's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[2]\n",
            "\n",
            "\t zs[2] < - Concatinate (xs[2], h[1] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   1.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.07171446  0.03185055  0.07174784\n",
            "   0.07279602  0.0227237  -0.0194629 ]]\n",
            "\t softmax of (ys[2]) =\n",
            "\t\t[[0.02764462 0.03410539 0.03284936 0.02607857 0.02960323 0.03113335\n",
            "  0.03263759 0.03168462 0.02768808 0.03055884 0.03009711 0.02968653\n",
            "  0.03414896 0.03484348 0.03599545 0.03013845 0.02601139 0.02955249\n",
            "  0.03157222 0.025619   0.03278264 0.02990386 0.03245748 0.02843177\n",
            "  0.03303415 0.02546946 0.02601373 0.02751986 0.03002137 0.02964587\n",
            "  0.03222384 0.03291371 0.02793353]]\n",
            "\n",
            "-----------------------------\n",
            "3th character is t\n",
            "-----------------------------\n",
            "\tt's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[3]\n",
            "\n",
            "\t zs[3] < - Concatinate (xs[3], h[2] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          1.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.0816711  -0.0042859   0.11939964\n",
            "   0.08189059  0.01301851  0.03794348]]\n",
            "\t softmax of (ys[3]) =\n",
            "\t\t[[0.02735312 0.03407779 0.03259499 0.02596035 0.02918644 0.03056425\n",
            "  0.03240739 0.03223702 0.02769445 0.03012384 0.03019901 0.03028019\n",
            "  0.03385044 0.03488231 0.0360636  0.02992893 0.02620888 0.03013258\n",
            "  0.03144239 0.02573438 0.03241138 0.02977126 0.03300008 0.02863373\n",
            "  0.03332097 0.02541991 0.02571025 0.02760502 0.03009432 0.02971422\n",
            "  0.0322781  0.03275991 0.02835848]]\n",
            "\n",
            "-----------------------------\n",
            "4th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[4]\n",
            "\n",
            "\t zs[4] < - Concatinate (xs[4], h[3] ):\n",
            "\t[[0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.11408934 0.03624149 0.05505723\n",
            "  0.13349313 0.04394053 0.03821756]]\n",
            "\t softmax of (ys[4]) =\n",
            "\t\t[[0.02742343 0.03391836 0.03226025 0.0258282  0.02939994 0.03038873\n",
            "  0.03217328 0.03256619 0.02748156 0.02986705 0.02988788 0.03020172\n",
            "  0.0340949  0.03483774 0.03590332 0.02959177 0.02624226 0.03010537\n",
            "  0.03167486 0.02578702 0.03218042 0.02980012 0.03346544 0.02911835\n",
            "  0.03315466 0.02552554 0.02562058 0.02797333 0.03030765 0.02976386\n",
            "  0.03221112 0.03290243 0.02834265]]\n",
            "\n",
            "-----------------------------\n",
            "5th character is S\n",
            "-----------------------------\n",
            "\tS's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[5]\n",
            "\n",
            "\t zs[5] < - Concatinate (xs[5], h[4] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.12192105 0.09939048 0.05887901\n",
            "  0.14522655 0.03052819 0.00171748]]\n",
            "\t softmax of (ys[5]) =\n",
            "\t\t[[0.02721815 0.03428689 0.03258949 0.02607602 0.02924643 0.03094894\n",
            "  0.03249758 0.0319278  0.02779742 0.03031984 0.03002529 0.03031349\n",
            "  0.03368408 0.03441146 0.03600585 0.03000329 0.02614492 0.0301094\n",
            "  0.03139876 0.02603165 0.03259435 0.02992581 0.03297492 0.02846902\n",
            "  0.03326174 0.02530356 0.02603976 0.0273424  0.02999687 0.02966301\n",
            "  0.03236697 0.03271342 0.02831141]]\n",
            "\n",
            "-----------------------------\n",
            "6th character is p\n",
            "-----------------------------\n",
            "\tp's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n",
            "\t Let us denote this vector by xs[6]\n",
            "\n",
            "\t zs[6] < - Concatinate (xs[6], h[5] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.08237807 0.04387913 0.08735232\n",
            "  0.15023512 0.04387272 0.07147859]]\n",
            "\t softmax of (ys[6]) =\n",
            "\t\t[[0.027268   0.03401277 0.03239457 0.02587816 0.02921092 0.03038491\n",
            "  0.03243697 0.03237844 0.02776738 0.030137   0.0300653  0.0308067\n",
            "  0.0339988  0.0347561  0.03589073 0.02941151 0.02622569 0.0304024\n",
            "  0.03163913 0.0257625  0.03233279 0.02996227 0.03310612 0.02889897\n",
            "  0.03312407 0.02531642 0.02546166 0.02778217 0.02992793 0.02980681\n",
            "  0.03207128 0.03321773 0.02816381]]\n",
            "\n",
            "-----------------------------\n",
            "7th character is a\n",
            "-----------------------------\n",
            "\ta's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[7]\n",
            "\n",
            "\t zs[7] < - Concatinate (xs[7], h[6] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   1.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.0906107   0.04709443  0.05715573\n",
            "   0.18421615  0.06518378 -0.00620194]]\n",
            "\t softmax of (ys[7]) =\n",
            "\t\t[[0.02722585 0.03418973 0.03294445 0.02597923 0.02943621 0.03102906\n",
            "  0.03273423 0.03197768 0.02770951 0.03042921 0.02993311 0.0303444\n",
            "  0.03376308 0.03449468 0.03601383 0.02993939 0.02606167 0.03002442\n",
            "  0.0315793  0.02578165 0.03279159 0.02990353 0.03261283 0.02833928\n",
            "  0.03305741 0.02528696 0.02580613 0.0273479  0.02993571 0.02979744\n",
            "  0.03243981 0.03310609 0.02798462]]\n",
            "\n",
            "-----------------------------\n",
            "8th character is n\n",
            "-----------------------------\n",
            "\tn's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "\t Let us denote this vector by xs[8]\n",
            "\n",
            "\t zs[8] < - Concatinate (xs[8], h[7] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.09070339 0.0044591  0.11693894\n",
            "  0.15775222 0.03769085 0.04011742]]\n",
            "\t softmax of (ys[8]) =\n",
            "\t\t[[0.02740738 0.03413986 0.03272503 0.02597867 0.02906946 0.03045772\n",
            "  0.03235271 0.03251424 0.02764687 0.03036028 0.03027907 0.03041598\n",
            "  0.03419919 0.03510696 0.03612139 0.0297477  0.02635143 0.03014604\n",
            "  0.03156034 0.02586071 0.03219606 0.02958217 0.0332626  0.02875208\n",
            "  0.03303633 0.02508578 0.02539717 0.0276353  0.0296735  0.02972062\n",
            "  0.03204247 0.03276829 0.0284066 ]]\n",
            "\n",
            "-----------------------------\n",
            "9th character is i\n",
            "-----------------------------\n",
            "\ti's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[9]\n",
            "\n",
            "\t zs[9] < - Concatinate (xs[9], h[8] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.11724724 0.02934987 0.07053948\n",
            "  0.11447974 0.1052761  0.02007549]]\n",
            "\t softmax of (ys[9]) =\n",
            "\t\t[[0.02766431 0.03393072 0.0330073  0.02598776 0.02923145 0.03058059\n",
            "  0.03265499 0.03195226 0.02775588 0.03034486 0.03060684 0.02992982\n",
            "  0.03404173 0.03548685 0.03619378 0.03022178 0.02610617 0.02981101\n",
            "  0.03139619 0.0251965  0.03264177 0.02969168 0.03223556 0.02832643\n",
            "  0.03336521 0.02556617 0.02557809 0.02761113 0.0300397  0.02969862\n",
            "  0.03217786 0.03286309 0.0281039 ]]\n",
            "\n",
            "-----------------------------\n",
            "10th character is s\n",
            "-----------------------------\n",
            "\ts's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[10]\n",
            "\n",
            "\t zs[10] < - Concatinate (xs[10], h[9] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   1.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.1230002  -0.05351915  0.04989546\n",
            "   0.07414996  0.0264788   0.02129967]]\n",
            "\t softmax of (ys[10]) =\n",
            "\t\t[[0.02761713 0.03399384 0.03266232 0.02611192 0.02899812 0.03052921\n",
            "  0.03262661 0.03154163 0.02807357 0.03024846 0.03087546 0.03006792\n",
            "  0.03372004 0.03536119 0.03615869 0.03036142 0.026069   0.02999584\n",
            "  0.03108394 0.02521949 0.03269677 0.02992966 0.03207815 0.02822221\n",
            "  0.03379912 0.02575195 0.02591632 0.02750852 0.03018178 0.02956255\n",
            "  0.03213045 0.03263376 0.028273  ]]\n",
            "\n",
            "-----------------------------\n",
            "11th character is h\n",
            "-----------------------------\n",
            "\th's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[11]\n",
            "\n",
            "\t zs[11] < - Concatinate (xs[11], h[10] ):\n",
            "\t[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  9.54999599e-02 -5.54445655e-02 -4.46799043e-04\n",
            "   8.69803038e-02  5.75042877e-03  5.69406204e-02]]\n",
            "\t softmax of (ys[11]) =\n",
            "\t\t[[0.02783351 0.03390687 0.03248274 0.02607637 0.02912157 0.03038348\n",
            "  0.03246161 0.03178363 0.02795785 0.03035452 0.03077984 0.03002431\n",
            "  0.03428373 0.03556834 0.03604399 0.02997275 0.02615513 0.02987872\n",
            "  0.03134818 0.02527608 0.03243105 0.02990174 0.03242266 0.02865538\n",
            "  0.0334351  0.02561943 0.02567894 0.02783382 0.0299814  0.0295632\n",
            "  0.03178886 0.032808   0.02818717]]\n",
            "\n",
            "-----------------------------\n",
            "12th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[12]\n",
            "\n",
            "\t zs[12] < - Concatinate (xs[12], h[11] ):\n",
            "\t[[ 0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.08714179 -0.02973918  0.01676046\n",
            "   0.06739416  0.03562024  0.01084649]]\n",
            "\t softmax of (ys[12]) =\n",
            "\t\t[[0.02777573 0.03380697 0.03213099 0.02591907 0.0293699  0.03027804\n",
            "  0.03217328 0.03223815 0.02765378 0.03001612 0.0302651  0.02997407\n",
            "  0.03441889 0.03529108 0.0358767  0.02962481 0.02620899 0.02990278\n",
            "  0.03160894 0.02550449 0.03217166 0.02989822 0.03311846 0.02917377\n",
            "  0.03323225 0.02567546 0.02564455 0.02815178 0.03025094 0.02964088\n",
            "  0.03186145 0.03289439 0.0282483 ]]\n",
            "\n",
            "-----------------------------\n",
            "13th character is w\n",
            "-----------------------------\n",
            "\tw's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[13]\n",
            "\n",
            "\t zs[13] < - Concatinate (xs[13], h[12] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.10149335  0.06196938  0.03289367\n",
            "   0.09586613  0.02289561 -0.01484646]]\n",
            "\t softmax of (ys[13]) =\n",
            "\t\t[[0.02761339 0.03384273 0.03291338 0.02584452 0.02952464 0.03062834\n",
            "  0.03248792 0.03228765 0.02742476 0.03004437 0.03014267 0.0296747\n",
            "  0.03403973 0.03525567 0.0361221  0.03015037 0.0260967  0.02970021\n",
            "  0.03158907 0.02530287 0.03255755 0.02961655 0.03264502 0.0285776\n",
            "  0.03323406 0.02568542 0.02561952 0.02778087 0.03038574 0.0297839\n",
            "  0.03242582 0.03287935 0.02812282]]\n",
            "\n",
            "-----------------------------\n",
            "14th character is o\n",
            "-----------------------------\n",
            "\to's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[14]\n",
            "\n",
            "\t zs[14] < - Concatinate (xs[14], h[13] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          1.          0.          0.\n",
            "   0.          0.          0.          0.14968646  0.01054353  0.07443755\n",
            "   0.08649989 -0.00388043  0.01181134]]\n",
            "\t softmax of (ys[14]) =\n",
            "\t\t[[0.02775918 0.03400825 0.03242099 0.0260438  0.02928005 0.03053875\n",
            "  0.03220671 0.03215158 0.02763616 0.03025119 0.0303454  0.02975107\n",
            "  0.03436832 0.03527461 0.03603688 0.02996387 0.02624803 0.0297451\n",
            "  0.03149325 0.02568967 0.03222435 0.02970627 0.0331293  0.02889385\n",
            "  0.03319965 0.0254731  0.02577686 0.02785294 0.03005314 0.0295693\n",
            "  0.03195766 0.03255499 0.02839572]]\n",
            "\n",
            "-----------------------------\n",
            "15th character is m\n",
            "-----------------------------\n",
            "\tm's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[15]\n",
            "\n",
            "\t zs[15] < - Concatinate (xs[15], h[14] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.10769134 0.04544368 0.05597954\n",
            "  0.06070838 0.04607097 0.02629236]]\n",
            "\t softmax of (ys[15]) =\n",
            "\t\t[[0.02778734 0.03425592 0.03215023 0.02632146 0.02886854 0.03056382\n",
            "  0.03214413 0.03167982 0.02805459 0.03051599 0.03080293 0.02991443\n",
            "  0.03429101 0.03525783 0.03606739 0.03011374 0.02631851 0.02987634\n",
            "  0.03114014 0.02594464 0.03215846 0.02984472 0.0331121  0.02875612\n",
            "  0.03346841 0.02533211 0.02608961 0.0276171  0.02974739 0.02933024\n",
            "  0.03166171 0.03213368 0.02867956]]\n",
            "\n",
            "-----------------------------\n",
            "16th character is a\n",
            "-----------------------------\n",
            "\ta's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[16]\n",
            "\n",
            "\t zs[16] < - Concatinate (xs[16], h[15] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.06241642 0.02924927 0.01884345\n",
            "  0.04102614 0.08124237 0.07596322]]\n",
            "\t softmax of (ys[16]) =\n",
            "\t\t[[0.0276143  0.03431839 0.03273957 0.02625751 0.02922907 0.03110689\n",
            "  0.03247401 0.03158281 0.02784405 0.03062522 0.03040561 0.02966336\n",
            "  0.0340086  0.03488541 0.03613318 0.03041741 0.02613817 0.02963405\n",
            "  0.03126345 0.0258917  0.03261521 0.02978632 0.03270143 0.02831729\n",
            "  0.03328457 0.02533269 0.02621824 0.02730565 0.0298731  0.02947404\n",
            "  0.03215916 0.03233707 0.02836247]]\n",
            "\n",
            "-----------------------------\n",
            "17th character is n\n",
            "-----------------------------\n",
            "\tn's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "\t Let us denote this vector by xs[17]\n",
            "\n",
            "\t zs[17] < - Concatinate (xs[17], h[16] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.         0.07889413 0.00236865 0.08878727\n",
            "  0.05413595 0.04485658 0.09184565]]\n",
            "\t softmax of (ys[17]) =\n",
            "\t\t[[0.02772791 0.03420444 0.03255558 0.02616495 0.02893523 0.03047317\n",
            "  0.03211725 0.03228477 0.02769716 0.03046092 0.03061867 0.02984695\n",
            "  0.03442989 0.03545608 0.03621262 0.03009187 0.02642236 0.02982555\n",
            "  0.03134677 0.02592725 0.03202039 0.02945917 0.03339562 0.02879251\n",
            "  0.0331931  0.02513918 0.02566933 0.02765819 0.02966352 0.02948331\n",
            "  0.03183272 0.03218029 0.02871327]]\n",
            "\n",
            "-----------------------------\n",
            "18th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[18]\n",
            "\n",
            "\t zs[18] < - Concatinate (xs[18], h[17] ):\n",
            "\t[[0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.11486325 0.0344816  0.04819886\n",
            "  0.0319804  0.10909119 0.05440889]]\n",
            "\t softmax of (ys[18]) =\n",
            "\t\t[[0.02770596 0.03400297 0.03221314 0.02597378 0.02925601 0.03035609\n",
            "  0.03194428 0.032584   0.02745498 0.03006851 0.03015146 0.02979716\n",
            "  0.03449224 0.03522356 0.03600734 0.02975615 0.02638254 0.02983788\n",
            "  0.03159824 0.02593034 0.03191092 0.02957399 0.03377009 0.02923891\n",
            "  0.0330855  0.02536983 0.02565348 0.02801829 0.03007061 0.0295884\n",
            "  0.03193278 0.03243814 0.02861242]]\n",
            "\n",
            "-----------------------------\n",
            "19th character is w\n",
            "-----------------------------\n",
            "\tw's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[19]\n",
            "\n",
            "\t zs[19] < - Concatinate (xs[19], h[18] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.12490255 0.10474769 0.05502629\n",
            "  0.06845157 0.06799715 0.01769421]]\n",
            "\t softmax of (ys[19]) =\n",
            "\t\t[[0.02758824 0.03396222 0.03297605 0.02587634 0.02945477 0.03066839\n",
            "  0.03231883 0.03254251 0.02727291 0.03007126 0.03007229 0.02951929\n",
            "  0.03411435 0.03524885 0.03621769 0.03024928 0.0262178  0.02963438\n",
            "  0.03158852 0.02557155 0.03236479 0.02937953 0.03308799 0.0286347\n",
            "  0.03313052 0.02548744 0.02561275 0.02771188 0.03027402 0.02974774\n",
            "  0.03247091 0.03255895 0.02837325]]\n",
            "\n",
            "-----------------------------\n",
            "20th character is h\n",
            "-----------------------------\n",
            "\th's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[20]\n",
            "\n",
            "\t zs[20] < - Concatinate (xs[20], h[19] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.16839782 0.03989371 0.08895983\n",
            "  0.06350135 0.02599666 0.03222344]]\n",
            "\t softmax of (ys[20]) =\n",
            "\t\t[[0.02779763 0.03392422 0.03270533 0.02594359 0.0294375  0.03054636\n",
            "  0.03229477 0.03238378 0.02745793 0.03031286 0.03022427 0.02969586\n",
            "  0.03455458 0.03542001 0.03606631 0.02988134 0.02624175 0.02963614\n",
            "  0.03169482 0.02555446 0.03225123 0.0295723  0.03306408 0.02890173\n",
            "  0.03295048 0.02539754 0.02550902 0.02792338 0.02997637 0.0296807\n",
            "  0.0319953  0.03279231 0.02821204]]\n",
            "\n",
            "-----------------------------\n",
            "21th character is o\n",
            "-----------------------------\n",
            "\to's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[21]\n",
            "\n",
            "\t zs[21] < - Concatinate (xs[21], h[20] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          1.          0.          0.\n",
            "   0.          0.          0.          0.12668764  0.03001421  0.08456049\n",
            "   0.05511398  0.05329414 -0.00348718]]\n",
            "\t softmax of (ys[21]) =\n",
            "\t\t[[0.02791089 0.03404475 0.03227488 0.02610832 0.02923405 0.03048055\n",
            "  0.03205349 0.03220964 0.02763437 0.03039006 0.03040967 0.02967141\n",
            "  0.03470499 0.03541859 0.03601085 0.02984207 0.02633982 0.02965818\n",
            "  0.03154348 0.02583358 0.03201633 0.02965585 0.0334119  0.02911166\n",
            "  0.03304722 0.02533073 0.02573512 0.02796323 0.02984653 0.02948837\n",
            "  0.03169709 0.03243994 0.02848239]]\n",
            "\n",
            "-----------------------------\n",
            "22th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[22]\n",
            "\n",
            "\t zs[22] < - Concatinate (xs[22], h[21] ):\n",
            "\t[[0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.0970636  0.0609072  0.05945958\n",
            "  0.03237943 0.0768179  0.01913969]]\n",
            "\t softmax of (ys[22]) =\n",
            "\t\t[[0.02779115 0.03390479 0.03207674 0.02592893 0.02943814 0.03036137\n",
            "  0.03193289 0.03254664 0.02741889 0.03002635 0.03001855 0.02973189\n",
            "  0.03463137 0.03518696 0.03588875 0.02959823 0.02632625 0.02975923\n",
            "  0.0317237  0.02585865 0.03192959 0.02969523 0.03374613 0.0294085\n",
            "  0.03299388 0.02547978 0.02566987 0.02818904 0.03017612 0.02960807\n",
            "  0.03186997 0.03263309 0.02845122]]\n",
            "\n",
            "-----------------------------\n",
            "23th character is l\n",
            "-----------------------------\n",
            "\tl's one-hot encoding is\n",
            "\t[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[23]\n",
            "\n",
            "\t zs[23] < - Concatinate (xs[23], h[22] ):\n",
            "\t[[ 0.          1.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.11495038  0.11704005  0.06294177\n",
            "   0.07371997  0.04865252 -0.00502048]]\n",
            "\t softmax of (ys[23]) =\n",
            "\t\t[[0.02732934 0.03410183 0.03245176 0.02604487 0.02967752 0.03124729\n",
            "  0.03272491 0.03132147 0.02788627 0.0301995  0.02984759 0.03000496\n",
            "  0.03349684 0.03415855 0.0357985  0.03009976 0.02583164 0.02986528\n",
            "  0.03141937 0.02566503 0.03306557 0.03036108 0.0322683  0.02834902\n",
            "  0.03342009 0.02582551 0.02646568 0.02742242 0.03051548 0.02967156\n",
            "  0.03252679 0.03307276 0.02786348]]\n",
            "\n",
            "-----------------------------\n",
            "24th character is i\n",
            "-----------------------------\n",
            "\ti's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[24]\n",
            "\n",
            "\t zs[24] < - Concatinate (xs[24], h[23] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.05278002  0.02750151  0.09477388\n",
            "   0.16674305 -0.0561518   0.06248929]]\n",
            "\t softmax of (ys[24]) =\n",
            "\t\t[[0.02759462 0.0339361  0.03279238 0.02605643 0.02952947 0.03103866\n",
            "  0.03289399 0.03117384 0.02798258 0.03028388 0.03040402 0.02979047\n",
            "  0.03359063 0.0348809  0.03598694 0.03040844 0.02579903 0.02970898\n",
            "  0.03126924 0.0251054  0.03317105 0.03020939 0.0316235  0.0280676\n",
            "  0.03363361 0.02599045 0.02623746 0.02745077 0.03048934 0.02964857\n",
            "  0.03241655 0.03304593 0.02778978]]\n",
            "\n",
            "-----------------------------\n",
            "25th character is v\n",
            "-----------------------------\n",
            "\tv's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[25]\n",
            "\n",
            "\t zs[25] < - Concatinate (xs[25], h[24] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   1.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.07487379 -0.05932001  0.057862\n",
            "   0.11137121 -0.06452148  0.05005575]]\n",
            "\t softmax of (ys[25]) =\n",
            "\t\t[[0.02774875 0.03377761 0.03268306 0.02588114 0.02981596 0.03087364\n",
            "  0.03252259 0.03185367 0.02747559 0.03001093 0.02999662 0.02935727\n",
            "  0.03402685 0.0350401  0.03595661 0.03020863 0.02592466 0.02946571\n",
            "  0.03160024 0.02522136 0.0327724  0.02990072 0.03238644 0.02860866\n",
            "  0.03329184 0.02598794 0.02600862 0.0278581  0.03069235 0.02972047\n",
            "  0.03244678 0.0329689  0.0279158 ]]\n",
            "\n",
            "-----------------------------\n",
            "26th character is e\n",
            "-----------------------------\n",
            "\te's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[26]\n",
            "\n",
            "\t zs[26] < - Concatinate (xs[26], h[25] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          1.\n",
            "   0.          0.          0.          0.12146427  0.02119191  0.08425269\n",
            "   0.08267149 -0.06239393  0.01454828]]\n",
            "\t softmax of (ys[26]) =\n",
            "\t\t[[0.02721175 0.03430051 0.03276458 0.02605338 0.02974264 0.03150254\n",
            "  0.03267035 0.03166598 0.0276292  0.03040346 0.02955484 0.02990786\n",
            "  0.03362832 0.03397211 0.03589354 0.03012451 0.02594082 0.02975137\n",
            "  0.03158062 0.02606013 0.03295984 0.03006482 0.03273154 0.02833622\n",
            "  0.03303352 0.02542237 0.02636775 0.02724481 0.03021862 0.02971555\n",
            "  0.03265598 0.03292828 0.02796219]]\n",
            "\n",
            "-----------------------------\n",
            "27th character is d\n",
            "-----------------------------\n",
            "\td's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[27]\n",
            "\n",
            "\t zs[27] < - Concatinate (xs[27], h[26] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          1.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.0707421   0.05283028  0.15470954\n",
            "   0.15516758 -0.00847794  0.07667281]]\n",
            "\t softmax of (ys[27]) =\n",
            "\t\t[[0.02694178 0.03429116 0.03285074 0.02602242 0.02934543 0.03125501\n",
            "  0.03269728 0.031655   0.0277418  0.02994647 0.02988331 0.03004309\n",
            "  0.03284451 0.03402832 0.03612492 0.03063641 0.02593803 0.03009195\n",
            "  0.03112138 0.02585272 0.03305077 0.02995114 0.03249166 0.0279189\n",
            "  0.03376195 0.02571212 0.02648548 0.02699401 0.0306478  0.02971737\n",
            "  0.03309706 0.03251061 0.0283494 ]]\n",
            "\n",
            "-----------------------------\n",
            "28th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[28]\n",
            "\n",
            "\t zs[28] < - Concatinate (xs[28], h[27] ):\n",
            "\t[[ 0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.11391966  0.03293447  0.07804076\n",
            "   0.17769485 -0.04466494  0.13129526]]\n",
            "\t softmax of (ys[28]) =\n",
            "\t\t[[0.02713014 0.03407043 0.03240093 0.02587432 0.02945288 0.03079313\n",
            "  0.03236161 0.03219721 0.02755056 0.02976021 0.02971679 0.03015007\n",
            "  0.03342242 0.03427919 0.03594304 0.03000392 0.0260839  0.03014924\n",
            "  0.0314528  0.02588457 0.03257115 0.02993228 0.03315125 0.02865381\n",
            "  0.03345466 0.02568733 0.02609174 0.02756479 0.03062609 0.02976837\n",
            "  0.03271077 0.03275329 0.0283571 ]]\n",
            "\n",
            "-----------------------------\n",
            "29th character is t\n",
            "-----------------------------\n",
            "\tt's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[29]\n",
            "\n",
            "\t zs[29] < - Concatinate (xs[29], h[28] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          1.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.11899304  0.09631281  0.06870066\n",
            "   0.18022299 -0.01995793  0.06152395]]\n",
            "\t softmax of (ys[29]) =\n",
            "\t\t[[0.02701334 0.03407944 0.03228892 0.0258522  0.02908201 0.03038144\n",
            "  0.03227005 0.03248668 0.0276689  0.02968338 0.02996356 0.03066738\n",
            "  0.03339247 0.034466   0.03600068 0.0297942  0.02624191 0.03054953\n",
            "  0.03136622 0.02592571 0.03231    0.02985284 0.03339213 0.0287593\n",
            "  0.03356899 0.02552554 0.02577595 0.02760631 0.03041111 0.02978471\n",
            "  0.03253974 0.03271601 0.02858336]]\n",
            "\n",
            "-----------------------------\n",
            "30th character is h\n",
            "-----------------------------\n",
            "\th's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[30]\n",
            "\n",
            "\t zs[30] < - Concatinate (xs[30], h[29] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.12634552 0.09495341 0.02521369\n",
            "  0.20276159 0.02706841 0.05243554]]\n",
            "\t softmax of (ys[30]) =\n",
            "\t\t[[0.02736187 0.03399817 0.03220519 0.02591944 0.02917155 0.030334\n",
            "  0.03229194 0.03231342 0.02777401 0.03002611 0.03014762 0.03058598\n",
            "  0.03399718 0.03482911 0.03589488 0.0295297  0.02624284 0.0303283\n",
            "  0.03153494 0.02578087 0.03224382 0.02995713 0.03322225 0.02897933\n",
            "  0.03327948 0.0254434  0.02562155 0.02784674 0.03008249 0.02971933\n",
            "  0.03204496 0.03296746 0.02832494]]\n",
            "\n",
            "-----------------------------\n",
            "31th character is r\n",
            "-----------------------------\n",
            "\tr's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\t Let us denote this vector by xs[31]\n",
            "\n",
            "\t zs[31] < - Concatinate (xs[31], h[30] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.09297455 0.06567163 0.03666912\n",
            "  0.16307398 0.05104776 0.00825886]]\n",
            "\t softmax of (ys[31]) =\n",
            "\t\t[[0.02751587 0.03406737 0.03222483 0.02600456 0.02938392 0.03067682\n",
            "  0.0321806  0.03213767 0.02761288 0.03005372 0.03000122 0.02990099\n",
            "  0.03404733 0.03473707 0.03592436 0.02988839 0.02619401 0.02990067\n",
            "  0.03150193 0.02591573 0.03229953 0.02987147 0.03332704 0.02893637\n",
            "  0.03325868 0.02554481 0.02601056 0.02778291 0.03028454 0.02960987\n",
            "  0.03219153 0.03259966 0.02841308]]\n",
            "\n",
            "-----------------------------\n",
            "32th character is e\n",
            "-----------------------------\n",
            "\te's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[32]\n",
            "\n",
            "\t zs[32] < - Concatinate (xs[32], h[31] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.09879414 0.09125845 0.06520987\n",
            "  0.1120664  0.02082958 0.04272919]]\n",
            "\t softmax of (ys[32]) =\n",
            "\t\t[[0.02710971 0.03447987 0.03237874 0.02615481 0.02949904 0.03139885\n",
            "  0.03244621 0.0317402  0.02776239 0.03045769 0.02955974 0.03023847\n",
            "  0.03368594 0.03375898 0.03582287 0.0298824  0.02608867 0.03000035\n",
            "  0.03152088 0.02651382 0.03267097 0.03012688 0.03329677 0.0285827\n",
            "  0.03300901 0.02517921 0.02643593 0.02723026 0.02996874 0.02961729\n",
            "  0.03242558 0.03272169 0.02823533]]\n",
            "\n",
            "-----------------------------\n",
            "33th character is e\n",
            "-----------------------------\n",
            "\te's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[33]\n",
            "\n",
            "\t zs[33] < - Concatinate (xs[33], h[32] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.04451525 0.0991343  0.14217137\n",
            "  0.17273729 0.03915058 0.09238382]]\n",
            "\t softmax of (ys[33]) =\n",
            "\t\t[[0.02691774 0.03471144 0.03242051 0.02625535 0.02956093 0.03181107\n",
            "  0.03256942 0.03148141 0.02785674 0.03069011 0.02933546 0.03037707\n",
            "  0.03351079 0.0332395  0.03575723 0.0298916  0.02602895 0.03002459\n",
            "  0.03151517 0.02685908 0.03286453 0.0302797  0.03328823 0.02840515\n",
            "  0.03288178 0.02499224 0.0267146  0.02693928 0.02980049 0.02959548\n",
            "  0.03252472 0.03274876 0.02815087]]\n",
            "\n",
            "-----------------------------\n",
            "34th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[34]\n",
            "\n",
            "\t zs[34] < - Concatinate (xs[34], h[33] ):\n",
            "\t[[0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.01146154 0.10566716 0.18294032\n",
            "  0.20046549 0.04781049 0.12204547]]\n",
            "\t softmax of (ys[34]) =\n",
            "\t\t[[0.02721842 0.03430051 0.03201119 0.02604462 0.02959326 0.03109781\n",
            "  0.03220373 0.03202674 0.02763268 0.03019197 0.02943271 0.03021503\n",
            "  0.0339269  0.03386544 0.03569501 0.02957174 0.02614439 0.03002929\n",
            "  0.03167229 0.02648861 0.03240131 0.03015482 0.03369366 0.02904796\n",
            "  0.032954   0.02531059 0.02630593 0.02760996 0.03015593 0.02963727\n",
            "  0.03227878 0.03280986 0.02827757]]\n",
            "\n",
            "-----------------------------\n",
            "35th character is h\n",
            "-----------------------------\n",
            "\th's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[35]\n",
            "\n",
            "\t zs[35] < - Concatinate (xs[35], h[34] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         1.         0.\n",
            "  0.         0.         0.         0.0520352  0.14921146 0.12451138\n",
            "  0.1777691  0.03040424 0.05648173]]\n",
            "\t softmax of (ys[35]) =\n",
            "\t\t[[0.02753444 0.0341369  0.03198166 0.02606005 0.02948859 0.03078089\n",
            "  0.03221112 0.03199275 0.02775528 0.0303426  0.02983718 0.03022411\n",
            "  0.03436556 0.03447444 0.0357017  0.02942231 0.02618586 0.02996215\n",
            "  0.03170501 0.02614083 0.03227662 0.03014659 0.03343844 0.02918689\n",
            "  0.03291952 0.02533328 0.02600023 0.02786764 0.02994644 0.0295962\n",
            "  0.03185769 0.0329575  0.02816952]]\n",
            "\n",
            "-----------------------------\n",
            "36th character is u\n",
            "-----------------------------\n",
            "\tu's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[36]\n",
            "\n",
            "\t zs[36] < - Concatinate (xs[36], h[35] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         1.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.04515454 0.1038101  0.09556902\n",
            "  0.13847282 0.05067381 0.01438177]]\n",
            "\t softmax of (ys[36]) =\n",
            "\t\t[[0.02752103 0.03418797 0.03169333 0.0261865  0.02951776 0.03099616\n",
            "  0.03233847 0.03134343 0.02808333 0.03038653 0.02990307 0.03028642\n",
            "  0.0341009  0.03414417 0.03555158 0.02949053 0.02601868 0.02999531\n",
            "  0.03152129 0.02612271 0.03255743 0.03057551 0.03301044 0.02903497\n",
            "  0.03317933 0.02556487 0.02647111 0.02775209 0.03010431 0.02948954\n",
            "  0.03182733 0.0330113  0.0280326 ]]\n",
            "\n",
            "-----------------------------\n",
            "37th character is n\n",
            "-----------------------------\n",
            "\tn's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
            "\t Let us denote this vector by xs[37]\n",
            "\n",
            "\t zs[37] < - Concatinate (xs[37], h[36] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          1.         -0.00581535  0.09227037  0.08001775\n",
            "   0.16090122  0.0073999   0.03834335]]\n",
            "\t softmax of (ys[37]) =\n",
            "\t\t[[0.02760935 0.0341414  0.03178404 0.02614234 0.02907963 0.0304126\n",
            "  0.03206567 0.03199138 0.0279605  0.03028815 0.03031468 0.03038417\n",
            "  0.03436396 0.03484835 0.03579581 0.02947686 0.02631399 0.03015914\n",
            "  0.0314517  0.0260899  0.03204181 0.03008358 0.03351521 0.02922401\n",
            "  0.03322394 0.02533313 0.02592061 0.02790823 0.02984432 0.02947985\n",
            "  0.03162174 0.03264404 0.02848591]]\n",
            "\n",
            "-----------------------------\n",
            "38th character is d\n",
            "-----------------------------\n",
            "\td's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[38]\n",
            "\n",
            "\t zs[38] < - Concatinate (xs[38], h[37] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.04696456 0.09117143 0.03259261\n",
            "  0.11810629 0.07756076 0.02620699]]\n",
            "\t softmax of (ys[38]) =\n",
            "\t\t[[0.0271864  0.03421278 0.03222464 0.02609428 0.0290053  0.03068262\n",
            "  0.032337   0.03177254 0.02794717 0.02989217 0.03029264 0.03026914\n",
            "  0.0332649  0.03447277 0.03604002 0.03026112 0.02613599 0.03029355\n",
            "  0.0310486  0.02590445 0.03253656 0.03001038 0.03294549 0.02843956\n",
            "  0.03387447 0.02568597 0.0263116  0.02737479 0.03045794 0.02956175\n",
            "  0.03248251 0.0323371  0.02864379]]\n",
            "\n",
            "-----------------------------\n",
            "39th character is r\n",
            "-----------------------------\n",
            "\tr's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\t Let us denote this vector by xs[39]\n",
            "\n",
            "\t zs[39] < - Concatinate (xs[39], h[38] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   1.          0.          0.          0.09342758  0.0609417   0.01077864\n",
            "   0.15519736 -0.00100899  0.10547438]]\n",
            "\t softmax of (ys[39]) =\n",
            "\t\t[[0.02732728 0.03421528 0.03225217 0.02609387 0.02923247 0.03086238\n",
            "  0.03220317 0.03187441 0.0277086  0.0299121  0.03007202 0.02979155\n",
            "  0.03349197 0.03446729 0.03603576 0.03035202 0.02614415 0.02996216\n",
            "  0.03117608 0.02602361 0.03246735 0.02987693 0.03320897 0.02857606\n",
            "  0.03366737 0.02567922 0.02641853 0.02745312 0.0305378  0.02953313\n",
            "  0.03253005 0.03218829 0.02866482]]\n",
            "\n",
            "-----------------------------\n",
            "40th character is e\n",
            "-----------------------------\n",
            "\te's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[40]\n",
            "\n",
            "\t zs[40] < - Concatinate (xs[40], h[39] ):\n",
            "\t[[ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          1.\n",
            "   0.          0.          0.          0.10545316  0.09263214  0.04492264\n",
            "   0.11806788 -0.0090502   0.10772701]]\n",
            "\t softmax of (ys[40]) =\n",
            "\t\t[[0.0270105  0.03456263 0.03236117 0.02621094 0.02938943 0.0314791\n",
            "  0.03244172 0.03158869 0.02783149 0.03036519 0.02962366 0.03018502\n",
            "  0.03337473 0.0336253  0.03588412 0.03013947 0.02606781 0.03004909\n",
            "  0.03131992 0.02657789 0.03274677 0.03013468 0.0332452  0.02839534\n",
            "  0.03326028 0.02526064 0.02667367 0.0270569  0.03011219 0.02956341\n",
            "  0.03259799 0.03246639 0.02839871]]\n",
            "\n",
            "-----------------------------\n",
            "41th character is d\n",
            "-----------------------------\n",
            "\td's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[41]\n",
            "\n",
            "\t zs[41] < - Concatinate (xs[41], h[40] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.04750549 0.10118031 0.12601419\n",
            "  0.17544651 0.02301512 0.12979855]]\n",
            "\t softmax of (ys[41]) =\n",
            "\t\t[[0.02684523 0.0344529  0.03253485 0.02613263 0.02913826 0.03124298\n",
            "  0.03252221 0.03157995 0.02787412 0.02991918 0.02992385 0.03017673\n",
            "  0.03271971 0.03380637 0.03609948 0.03063689 0.02601736 0.03025559\n",
            "  0.03095875 0.02619139 0.03289687 0.03001507 0.03284316 0.0279958\n",
            "  0.03390895 0.02562737 0.02671786 0.02690323 0.03059631 0.02960162\n",
            "  0.03302898 0.03219988 0.02863646]]\n",
            "\n",
            "-----------------------------\n",
            "42th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[42]\n",
            "\n",
            "\t zs[42] < - Concatinate (xs[42], h[41] ):\n",
            "\t[[ 0.          0.          0.          0.          1.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.09589562  0.06894629  0.05893941\n",
            "   0.18679303 -0.0270786   0.16502169]]\n",
            "\t softmax of (ys[42]) =\n",
            "\t\t[[0.02707636 0.03418016 0.03216394 0.02595793 0.0293141  0.03079229\n",
            "  0.03223925 0.03211262 0.02765333 0.02974427 0.02975226 0.03022564\n",
            "  0.03333909 0.03412314 0.03591766 0.03000936 0.02613272 0.03025078\n",
            "  0.03133215 0.02611519 0.03247194 0.02999368 0.03338511 0.02871205\n",
            "  0.03356556 0.02564425 0.02627465 0.02750577 0.03060069 0.02967843\n",
            "  0.03265306 0.03253092 0.02855167]]\n",
            "\n",
            "-----------------------------\n",
            "43th character is y\n",
            "-----------------------------\n",
            "\ty's one-hot encoding is\n",
            "\t[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[43]\n",
            "\n",
            "\t zs[43] < - Concatinate (xs[43], h[42] ):\n",
            "\t[[ 0.          0.          1.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.10419378  0.12132818  0.05451721\n",
            "   0.18505819 -0.01009685  0.08552847]]\n",
            "\t softmax of (ys[43]) =\n",
            "\t\t[[0.02725551 0.03419026 0.03189323 0.02603744 0.02915553 0.03051785\n",
            "  0.03225069 0.03209869 0.02795317 0.03022546 0.02998403 0.03088121\n",
            "  0.03403625 0.03436588 0.03572447 0.02927063 0.02625464 0.03045929\n",
            "  0.03156084 0.0261969  0.03221768 0.03021233 0.03347938 0.02911716\n",
            "  0.03313503 0.02524291 0.02584413 0.02775889 0.02984553 0.02964385\n",
            "  0.03186617 0.03301446 0.02831052]]\n",
            "\n",
            "-----------------------------\n",
            "44th character is e\n",
            "-----------------------------\n",
            "\te's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 1. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[44]\n",
            "\n",
            "\t zs[44] < - Concatinate (xs[44], h[43] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.04206071 0.0974082  0.05615314\n",
            "  0.19371028 0.07583225 0.01999068]]\n",
            "\t softmax of (ys[44]) =\n",
            "\t\t[[0.02699435 0.03454207 0.03215231 0.02618355 0.02939221 0.03132071\n",
            "  0.03248739 0.03166612 0.02797458 0.03057575 0.02955214 0.03077499\n",
            "  0.03371481 0.03355269 0.03568539 0.02951194 0.02610953 0.03029076\n",
            "  0.03156211 0.02666644 0.03263403 0.03035918 0.03335499 0.02870435\n",
            "  0.0329295  0.02502767 0.02636711 0.02723792 0.02972246 0.02962405\n",
            "  0.03220279 0.03297896 0.02814713]]\n",
            "\n",
            "-----------------------------\n",
            "45th character is a\n",
            "-----------------------------\n",
            "\ta's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[45]\n",
            "\n",
            "\t zs[45] < - Concatinate (xs[45], h[44] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.00671729 0.10228982 0.13773596\n",
            "  0.21706731 0.06748679 0.07721403]]\n",
            "\t softmax of (ys[45]) =\n",
            "\t\t[[0.02713328 0.03448344 0.03262691 0.02618399 0.02951792 0.03152117\n",
            "  0.03267356 0.03152163 0.02786515 0.03065105 0.02968172 0.03027499\n",
            "  0.03366114 0.03383106 0.03585795 0.0299706  0.02600973 0.02994671\n",
            "  0.03150418 0.02632825 0.03289126 0.03017657 0.03284125 0.0283346\n",
            "  0.03299304 0.02516803 0.02640271 0.02711057 0.02985253 0.02963837\n",
            "  0.03242248 0.03288953 0.02803461]]\n",
            "\n",
            "-----------------------------\n",
            "46th character is r\n",
            "-----------------------------\n",
            "\tr's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
            "\t Let us denote this vector by xs[46]\n",
            "\n",
            "\t zs[46] < - Concatinate (xs[46], h[45] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.03516548 0.04877137 0.15175837\n",
            "  0.16911138 0.036475   0.08925332]]\n",
            "\t softmax of (ys[46]) =\n",
            "\t\t[[0.0273396  0.03436756 0.03248961 0.02615309 0.02956899 0.03137658\n",
            "  0.03239311 0.03171378 0.02764673 0.03038692 0.02970854 0.02973367\n",
            "  0.03379232 0.03411662 0.03592121 0.03017723 0.02606701 0.02970502\n",
            "  0.03146801 0.02626773 0.0326711  0.02997109 0.03314695 0.02853706\n",
            "  0.03311089 0.02537226 0.026477   0.02732034 0.03016705 0.02956982\n",
            "  0.03246508 0.032515   0.028283  ]]\n",
            "\n",
            "-----------------------------\n",
            "47th character is s\n",
            "-----------------------------\n",
            "\ts's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[47]\n",
            "\n",
            "\t zs[47] < - Concatinate (xs[47], h[46] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  1.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.06973924 0.08539002 0.13225598\n",
            "  0.11895398 0.01290147 0.09539855]]\n",
            "\t softmax of (ys[47]) =\n",
            "\t\t[[0.02742354 0.03426061 0.03226256 0.02622986 0.02916761 0.03097528\n",
            "  0.03246398 0.0313421  0.02807522 0.03028326 0.03036221 0.03003044\n",
            "  0.03357536 0.03451273 0.03596149 0.03027634 0.02604513 0.02998273\n",
            "  0.03110939 0.02587182 0.03270493 0.03016798 0.03261363 0.0283812\n",
            "  0.03367157 0.02564153 0.026483   0.02735114 0.03023846 0.02946833\n",
            "  0.03223883 0.03244995 0.02837781]]\n",
            "\n",
            "-----------------------------\n",
            "48th character is  \n",
            "-----------------------------\n",
            "\t 's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[48]\n",
            "\n",
            "\t zs[48] < - Concatinate (xs[48], h[47] ):\n",
            "\t[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  5.36676313e-02  2.88300274e-02  4.21795254e-02\n",
            "   1.19731394e-01 -9.14975922e-04  1.00118394e-01]]\n",
            "\t softmax of (ys[48]) =\n",
            "\t\t[[0.02749743 0.0340411  0.03197446 0.02602259 0.02936176 0.03063166\n",
            "  0.0321658  0.03195131 0.027747   0.02995407 0.03002911 0.03000739\n",
            "  0.03392799 0.03461744 0.03583345 0.02982876 0.02614758 0.0300031\n",
            "  0.03142589 0.02589713 0.032334   0.03006787 0.03325616 0.02898653\n",
            "  0.0334222  0.02569187 0.02616272 0.02782642 0.03042133 0.02957391\n",
            "  0.03215301 0.03263301 0.02840596]]\n",
            "\n",
            "-----------------------------\n",
            "49th character is a\n",
            "-----------------------------\n",
            "\ta's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[49]\n",
            "\n",
            "\t zs[49] < - Concatinate (xs[49], h[48] ):\n",
            "\t[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 8.05202113e-02 9.99745018e-02 4.36951624e-02\n",
            "  1.31094234e-01 6.63478308e-04 4.56556101e-02]]\n",
            "\t softmax of (ys[49]) =\n",
            "\t\t[[0.0273651  0.03422096 0.03261772 0.02608476 0.0294737  0.03114236\n",
            "  0.0325259  0.03171737 0.02772501 0.03030015 0.02995211 0.02988412\n",
            "  0.03371226 0.03441933 0.03598028 0.0301965  0.02603544 0.02981621\n",
            "  0.03141235 0.02589802 0.0327466  0.02997029 0.03276179 0.02841706\n",
            "  0.03328074 0.02551104 0.02625665 0.02737831 0.03022925 0.02963136\n",
            "  0.0324586  0.03268566 0.02819302]]\n",
            "\n",
            "-----------------------------\n",
            "50th character is g\n",
            "-----------------------------\n",
            "\tg's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[50]\n",
            "\n",
            "\t zs[50] < - Concatinate (xs[50], h[49] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.08289101 0.04148056 0.10068755\n",
            "  0.12438512 0.00204472 0.07588602]]\n",
            "\t softmax of (ys[50]) =\n",
            "\t\t[[0.02711795 0.03419371 0.03306481 0.02595384 0.02947797 0.03114218\n",
            "  0.0328172  0.03190329 0.0276803  0.03030821 0.02986869 0.03026789\n",
            "  0.03347023 0.03435405 0.03605861 0.03014724 0.02599087 0.03002373\n",
            "  0.03149748 0.02572197 0.03295677 0.02990382 0.03244527 0.02814109\n",
            "  0.03320765 0.02541087 0.02593682 0.02721968 0.03014434 0.02982871\n",
            "  0.03270213 0.03305796 0.02798468]]\n",
            "\n",
            "-----------------------------\n",
            "51th character is o\n",
            "-----------------------------\n",
            "\to's one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[51]\n",
            "\n",
            "\t zs[51] < - Concatinate (xs[51], h[50] ):\n",
            "\t[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  1.02671098e-01 -5.38407815e-04  1.16737817e-01\n",
            "   1.69198738e-01  8.85838710e-03  5.93929495e-02]]\n",
            "\t softmax of (ys[51]) =\n",
            "\t\t[[0.02747958 0.0341869  0.03247596 0.02609712 0.02923459 0.03077686\n",
            "  0.03238257 0.03194786 0.02779398 0.03036841 0.03021612 0.03011625\n",
            "  0.03403555 0.03479272 0.03599871 0.02994464 0.02619205 0.02995819\n",
            "  0.03143294 0.02590085 0.03243189 0.02987794 0.03301106 0.02866302\n",
            "  0.03321676 0.02534343 0.02593699 0.02756056 0.02993888 0.02959736\n",
            "  0.03209902 0.03266479 0.02832647]]\n",
            "\n",
            "-----------------------------\n",
            "52th character is ,\n",
            "-----------------------------\n",
            "\t,'s one-hot encoding is\n",
            "\t[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "\t Let us denote this vector by xs[52]\n",
            "\n",
            "\t zs[52] < - Concatinate (xs[52], h[51] ):\n",
            "\t[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.08206162 0.03907386 0.07302251\n",
            "  0.10961134 0.05197957 0.05053571]]\n",
            "\t softmax of (ys[52]) =\n",
            "\t\t[[0.02760467 0.03419951 0.0324228  0.02615103 0.02903646 0.03059983\n",
            "  0.03218797 0.032069   0.02776501 0.03031834 0.03046452 0.02990549\n",
            "  0.03413331 0.03511641 0.03612038 0.03011954 0.02630921 0.02989828\n",
            "  0.03129686 0.02592622 0.03220416 0.0296721  0.03324109 0.02872089\n",
            "  0.03334733 0.02532643 0.02591472 0.02760174 0.02991894 0.0294997\n",
            "  0.03200679 0.03227109 0.0286302 ]]\n",
            "\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_derivative_LSTM (params, inputs, targets, cprev, hprev):\n",
        "\n",
        "    Wf, Wi, Wg, Wo, bf, bi, bg, bo, Wy, by = params\n",
        "\n",
        "    xs, hs, zs, cs, ys, ps = {}, {}, {}, {}, {}, {}\n",
        "    fs, i_s, gs, os, tanhcs = {}, {}, {}, {}, {}\n",
        "    cs[-1] = np.copy(cprev) # reset RNN memory\n",
        "    hs[-1] = np.copy(hprev) # reset RNN memory\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # forward pass\n",
        "    for t in range(len(inputs)):\n",
        "\n",
        "        xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
        "        xs[t][inputs[t]] = 1\n",
        "        zs[t] = np.concatenate((xs[t], hs[t-1]), axis=0)\n",
        "\n",
        "        f_raw = np.dot(Wf, zs[t]) + bf\n",
        "        i_raw = np.dot(Wi, zs[t]) + bi\n",
        "        g_raw = np.dot(Wg, zs[t]) + bg\n",
        "        o_raw = np.dot(Wo, zs[t]) + bo\n",
        "\n",
        "        fs[t] = sigmoid(f_raw)\n",
        "        i_s[t] = sigmoid(i_raw)\n",
        "        gs[t] = np.tanh(g_raw)\n",
        "        os[t] = sigmoid(i_raw)\n",
        "\n",
        "        cs[t] = fs[t]*cs[t-1] + i_s[t]*gs[t]\n",
        "        tanhcs[t] = np.tanh(cs[t])\n",
        "        hs[t] = os[t]*tanhcs[t]\n",
        "        ys[t] = np.dot(Wy, hs[t]) + by\n",
        "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "\n",
        "        loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)\n",
        "\n",
        "    # backward pass: compute gradients going backwards\n",
        "    dWf, dWi, dWg, dWo = np.zeros_like(Wf), np.zeros_like(Wi), np.zeros_like(Wg), np.zeros_like(Wo)\n",
        "    dbf,dbi, dbg, dbo = np.zeros_like(bf), np.zeros_like(bi), np.zeros_like(bg), np.zeros_like(bo)\n",
        "    dWy = np.zeros_like(Wy)\n",
        "    dby = np.zeros_like(by)\n",
        "\n",
        "    dcnext = np.zeros((hidden_size, 1))\n",
        "    dhnext = np.zeros((hidden_size, 1))\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        dy = np.copy(ps[t])\n",
        "        dy[targets[t]] -= 1\n",
        "\n",
        "        # Phase 1\n",
        "        dWy += np.dot(dy, hs[t].T)\n",
        "        dby += dy\n",
        "        dh = np.dot(Wy.T, dy) + dhnext\n",
        "        dc = dh * os[t] * (1-tanhcs[t]*tanhcs[t]) + dcnext\n",
        "\n",
        "        ## Phase 2\n",
        "        df = cs[t-1] * dc\n",
        "        di = gs[t] * dc\n",
        "        dg = i_s[t] * dc\n",
        "        do = tanhcs[t] * dh\n",
        "\n",
        "        ## Phase 3\n",
        "        df_raw = fs[t]*(1-fs[t])*df\n",
        "        di_raw = i_s[t]*(1-i_s[t])*di\n",
        "        dg_raw = (1-gs[t]*gs[t])*dg\n",
        "        do_raw = os[t]*(1-os[t])*do\n",
        "\n",
        "        ## Phase4\n",
        "        dWf += np.dot(df_raw, zs[t].T)\n",
        "        dWi += np.dot(di_raw, zs[t].T)\n",
        "        dWg += np.dot(dg_raw, zs[t].T)\n",
        "        dWo += np.dot(do_raw, zs[t].T)\n",
        "\n",
        "        dbf += df_raw\n",
        "        dbi += di_raw\n",
        "        dbg += dg_raw\n",
        "        dbo += do_raw\n",
        "\n",
        "        ## Phase 5\n",
        "        dcnext = fs[t] * dc\n",
        "        dz = np.dot(Wf.T, df_raw) + np.dot(Wi.T, di_raw) + np.dot(Wg.T, dg_raw) + np.dot(Wo.T, do_raw)\n",
        "        dhnext = dz[vocab_size:]\n",
        "\n",
        "        for dparam in [dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby]:\n",
        "            np.clip(dparam, -5, 5, out=dparam)\n",
        "\n",
        "    return loss, dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby, cs[len(inputs)-1], hs[len(inputs)-1]\n",
        "\n",
        "def guess_sentence_LSTM (params, ch, max_seq = 250):\n",
        "\n",
        "    Wf, Wi, Wg, Wo, bf, bi, bg, bo, Wy, by  = params\n",
        "    initial_char = char_to_idx[ch]\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    x[initial_char] = 1\n",
        "\n",
        "    h = np.zeros((hidden_size, 1))\n",
        "    c = np.zeros((hidden_size, 1))\n",
        "\n",
        "    ixes = [initial_char]\n",
        "\n",
        "    n=0\n",
        "    while True:\n",
        "\n",
        "        z = np.concatenate((x, h), axis=0)\n",
        "\n",
        "        f_raw = np.dot(Wf, z) + bf\n",
        "        i_raw = np.dot(Wi, z) + bi\n",
        "        g_raw = np.dot(Wg, z) + bg\n",
        "        o_raw = np.dot(Wo, z) + bo\n",
        "\n",
        "        f = sigmoid(f_raw)\n",
        "        i = sigmoid(i_raw)\n",
        "        g = np.tanh(g_raw)\n",
        "        o = sigmoid(i_raw)\n",
        "\n",
        "        c = f*c + i*g\n",
        "        h = o*np.tanh(c)\n",
        "        y = np.dot(Wy, h) + by\n",
        "        p = np.exp(y) / np.sum(np.exp(y)) # probabilities for next chars\n",
        "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "        x = np.zeros((vocab_size, 1))\n",
        "        x[ix] = 1\n",
        "\n",
        "        n+=1\n",
        "        if ( n > max_seq):\n",
        "            break\n",
        "\n",
        "        ixes.append(ix)\n",
        "\n",
        "    return ''.join([ idx_to_char[x] for x in ixes ])"
      ],
      "metadata": {
        "id": "ZYWwuEiicTM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import signal\n",
        "learning_rate = 1e-1\n",
        "\n",
        "def optimize(iteration = 10000, hidden_size = 8) :\n",
        "\n",
        "    n  =  0\n",
        "    loss_trace = []\n",
        "\n",
        "    params = make_LSTM_parameters(hidden_size, vocab_size)\n",
        "    mems = []\n",
        "    for param in params:\n",
        "        mems.append(np.zeros_like(param))\n",
        "\n",
        "    for n in range(iteration):\n",
        "        try:\n",
        "\n",
        "            loss_total = 0\n",
        "\n",
        "            sentence = data # Whole BackPropagation Through Time (Not Truncated Version)\n",
        "\n",
        "            loss_sentence = 0\n",
        "            hprev, cprev = np.zeros((hidden_size,1)), np.zeros((hidden_size,1))\n",
        "\n",
        "            inputs = [char_to_idx[ch] for ch in sentence[:-1]]\n",
        "            targets = [char_to_idx[ch] for ch in sentence[1:]]                \n",
        "\n",
        "            loss, dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby, cprev, hprev = get_derivative_LSTM (params, inputs, targets, cprev, hprev)\n",
        "\n",
        "            loss_total += loss\n",
        "\n",
        "\n",
        "            # perform parameter update with Adagrad\n",
        "            for param, dparam, mem in zip(params,\n",
        "                        [dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby],\n",
        "                        mems):\n",
        "                mem += dparam * dparam\n",
        "                param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "            loss_trace.append(loss_total)\n",
        "\n",
        "            if (n % 50 == 0):\n",
        "                import matplotlib.pyplot as plt\n",
        "                from IPython import display\n",
        "\n",
        "                display.clear_output(wait=True)\n",
        "                # plt.ylim((0,4000))\n",
        "                plt.plot(loss_trace)\n",
        "                plt.ylabel('cost')\n",
        "                plt.xlabel('iterations (per hundreds)')\n",
        "                plt.show()\n",
        "\n",
        "                print ('iter %d, loss: %f \\nguess_sentences:' % (n, loss_total)) # print progress\n",
        "                for i in range(1):\n",
        "                    print(guess_sentence_LSTM(params, 'T', len(sentence)))\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            break\n",
        "\n",
        "    return params, loss_trace\n",
        "\n",
        "iteration = 801\n",
        "hidden_size = 50\n",
        "params, loss_trace = optimize(iteration, hidden_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "vSjoGUUOZ8oF",
        "outputId": "24f40f18-02f4-4866-dc22-61e28a4d2725"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 800, loss: 1686.837779 \n",
            "guess_sentences:\n",
            "Th aive tho ov opspr vowspet ofwed aing hor, wha wt af wh theed aed condeuntherar.\n",
            "Mapand and ound forf lorghe owfy m tl-alded nerepivistraf ioc fa crn forr hhor pprthagigl\n",
            "nd sire wshern id foed und od wemtaf wlf sor ty cerspaved bsiv on wfersingsoun.\n",
            "piaglyer ao ling tho wonfey knwir ware le,t erser ond Thoed no oll-eas ant af ollye heedersairsspers noms htracd of hhroum whepr or thu ar-ed ulyeseponictagtlemngirc.\n",
            "pons aored se lhac fongule ad a ralvas whe thfe leemees;\n",
            "upwith ongitha kioh porinha nircidirpppirhitagtrturit semmatlesnoc keowee i;erth yees obles nom sund tho fortated bernderwiande theren ond bors Tat uand ounlerea nderes ledeperspir tho akr of y aicrey onlmrdnd ofllevact aftwir whes ka cedthove msero l.\n",
            "he ansnant atrcepwpenwon wo-mot foed bond pongreat iand tunirlend an ,oshorand cotthe lapt the of of a the consesteniw fon.ppot itrearniac a lyas encertend bo nghe deeslingy koncomb tungd ofe for fommodwersein the herrer \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import signal\n",
        "learning_rate = 1e-1\n",
        "\n",
        "def optimize(iteration = 10000, hidden_size = 8, T_steps =  100) :\n",
        "\n",
        "    n, pointer =  0, 0\n",
        "    smooth_loss = -np.log(1.0 / vocab_size) * T_steps\n",
        "    loss_trace = []\n",
        "\n",
        "    params = make_LSTM_parameters(hidden_size, vocab_size)\n",
        "    mems = []\n",
        "    for param in params:\n",
        "        mems.append(np.zeros_like(param))\n",
        "\n",
        "    for n in range(iteration):\n",
        "\n",
        "        try:\n",
        "            if pointer + T_steps >= len(data) or n == 0:\n",
        "                hprev, cprev = np.zeros((hidden_size,1)), np.zeros((hidden_size,1))\n",
        "                pointer = 0\n",
        "\n",
        "            inputs = ([char_to_idx[ch]\n",
        "                       for ch in data[pointer: pointer + T_steps]])\n",
        "            targets = ([char_to_idx[ch]\n",
        "                        for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
        "\n",
        "\n",
        "            loss, dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby, cprev, hprev = get_derivative_LSTM (params, inputs, targets, cprev, hprev)\n",
        "\n",
        "            # perform parameter update with Adagrad\n",
        "            for param, dparam, mem in zip(params,\n",
        "                        [dWf, dWi, dWg, dWo, dbf, dbi, dbg, dbo, dWy, dby],\n",
        "                        mems):\n",
        "\n",
        "                mem += dparam * dparam\n",
        "                param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
        "\n",
        "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "\n",
        "            loss_trace.append(smooth_loss)\n",
        "\n",
        "            if (n % 100 == 0):\n",
        "                import matplotlib.pyplot as plt\n",
        "                from IPython import display\n",
        "\n",
        "                display.clear_output(wait=True)\n",
        "\n",
        "                plt.plot(loss_trace)\n",
        "                plt.ylabel('cost')\n",
        "                plt.xlabel('iterations (per hundreds)')\n",
        "                plt.show()\n",
        "\n",
        "                print ('iter %d, loss: %f \\nguess_sentences:' % (n, smooth_loss)) # print progress\n",
        "                for i in range(1):\n",
        "                    print(guess_sentence_LSTM(params, 'T', len(data)))\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            break\n",
        "\n",
        "    return params, loss_trace\n",
        "\n",
        "iteration = 801\n",
        "hidden_size = 50\n",
        "params, loss_trace = optimize(iteration, hidden_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "b3XfFExaaA86",
        "outputId": "5922e710-e48d-491a-d6bf-4a1d2762ae6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3gVZd7/8fc3FULoRKQEQhVFqZEOoqKia0GwIIpdREGwr7vP79ld91mftSwqKKJgwYaCiIogKiK9h94JINIhUgJIkXL//phJ9ixPwAA5mZzk87quuZhzz5w5n5Mc8j1T7nvMOYeIiAhAVNABRESk4FBREBGRbCoKIiKSTUVBRESyqSiIiEi2mKADnI0KFSq4lJSUoGOIiESUefPm/eKcS8ppWUQXhZSUFNLS0oKOISISUczs55Mt0+EjERHJpqIgIiLZVBRERCSbioKIiGRTURARkWxhKwpmVszM5pjZIjNbZmbP+u1DzewnM1voT438djOzAWa2xswWm1mTcGUTEZGchfOS1MPAZc65/WYWC0wzs3H+sqeccyNPWP9qoI4/NQcG+f+KiEg+CVtRcN6Y3Pv9h7H+dKpxum8APvCfN8vMyphZJefc1rzOtnP/Yd6d/hPVy5cgpXwJUiokcE7JYnn9MiIiESesndfMLBqYB9QGBjrnZpvZQ8BzZvYXYALwjHPuMFAF2Bjy9E1+29YTttkD6AFQrVq1M8q1fucB3pq8jqPH/12jzq9UihsbV6Zrs2qUKhZ7RtsVEYl0lh832TGzMsAXwCPATmAbEAcMBtY65/5uZmOA551z0/znTAD+6Jw7aZfl1NRUd6Y9mo8eO86WPYdYv/NXVm3bx7ilW5m/YQ+li8fyx4716HpxMlFRdkbbFhEpyMxsnnMuNadl+XL1kXNuDzAR6Oic2+o8h4H3gGb+apuB5JCnVfXbwiImOopq5RNoVzeJB9rVZNTDrRnzSBvqnVuSP3+xhK6DZ7F976FwvbyISIEUzquPkvw9BMysOHAFsNLMKvltBnQClvpPGQ3c6V+F1ALIDMf5hFO5sEppPu3RghdvasDSLZn8YcA0Zq3bmZ8RREQCFc49hUrARDNbDMwFxjvnxgAfm9kSYAlQAfiHv/43wDpgDTAEeDiM2U7KzLglNZmverWmVPEYbn97Np/O2RBEFBGRfJcv5xTC5WzOKeTGvkNH6D1sAZNXZ9Dn8jo81qEO3g6OiEjkCvycQqQqWSyWt+9K5ZbUqgyYkM5TIxdz5NjxoGOJiIRNRN9PIT/ERkfxQpcGVC5TnFd/SCdj32HeuL0JJeL1oxORwkd7CrlgZjzaoS7Pd76IqekZdBsyi537DwcdS0Qkz6konIauzaoxuHsqK7fto8ugGWzYeSDoSCIieUpF4TR1uKAiwx5ozp6DR+g8aAZLN2cGHUlEJM+oKJyBptXLMbJnS+Kija6DZzEt/ZegI4mI5AkVhTNU+5ySjHq4NVXKFOeeoXMYvWhL0JFERM6aisJZOLd0MUb0bEnjamXp88kC3pn2U9CRRETOiorCWSpdPJYP7m1Gx/rn8j9jlvPPb1Zw/HjkdggUkaJNRSEPFIuNZuDtTejeojpvTVnHE58tUic3EYlI6oGVR6KjjL/fUJ+KpeL51/er2fnrbwxSJzcRiTDaU8hDZkbvy+rwYpcGTF/zC7cNmcUv6uQmIhFERSEMbrk4mcHdm7J6u9fJ7eedvwYdSUQkV1QUwuTy8ysy7IEWZB48Qhd1chORCKGiEEZNqpVlZM9WxMdEc+tbM5manhF0JBGRU1JRCLPa5yQy6uFWJJdL4N6hc/lqYdjuMCoictZUFPJBxVLFGP5gS5pUK0vfTxfy9tR1QUcSEclROO/RXMzM5pjZIjNbZmbP+u0fm9kqM1tqZu+aWazf3t7MMs1soT/9JVzZglC6eCzv39uMay46l3+MXcFzY5erk5uIFDjhvIj+MHCZc26//4d/mpmNAz4G7vDXGQbcDwzyH091zl0bxkyBKhYbzWu3NaFC4jKGTP2JjH2HefGmhsTFaIdNRAqGsBUF5938eb//MNafnHPum6x1zGwOUDVcGQqi6Cjj2evrU7FUMV76bpXXye2OpiSqk5uIFABh/YpqZtFmthDYAYx3zs0OWRYLdAe+DXlKS/9w0zgzq3+SbfYwszQzS8vIiMyrecyMXpfW5qWbGjBj7U66Dp5Jxj51chOR4IW1KDjnjjnnGuHtDTQzswtDFr8BTHHOTfUfzweqO+caAq8BX55km4Odc6nOudSkpKRwxg+7m1OTGXJnU9bs2E+XQTNY/4s6uYlIsPLlYLZzbg8wEegIYGZ/BZKAx0PW2euc2+/PfwPEmlmF/MgXpMvqVeSTB1qw75DXyW3+ht1BRxKRIiycVx8lmVkZf744cAWw0szuB64CbnPOHQ9Z/1wzM3++mZ9tZ7jyFSSNq5Xl84dakVgshtsGz2Lckq1BRxKRIiqcewqVgIlmthiYi3dOYQzwJlARmHnCpac3AUvNbBEwAOjqn6wuEmomJTLqoVbUr1yKh4fNZ/CUtRShty8iBYRF8h+e1NRUl5aWFnSMPHXoyDGeGLGIsUu2ckeLavztuvrEROuSVRHJO2Y2zzmXmtMyXQdZwHh9GRqTXC6BNyevZfPug7zWrYkuWRWRfKGvoAVQVJTxzNX1+N8bL2JK+i/c8uZMtmUeCjqWiBQBKgoFWLfm1XjnrlR+3vkrnQZOZ/mWvUFHEpFCTkWhgGt/3jl81rMVADe/OYNJq3YEnEhECjMVhQhwQeVSfNmrNdXLl+C+99MYNntD0JFEpJBSUYgQ55YuxoieLWlbpwJ//mIJz49bqVFWRSTPqShEkMT4GN6+M5Xbm1fjzclr6f3JfA7+dizoWCJSiKgoRJiY6Cj+0elC/uua8xm3dBu3Dp7J9r26MklE8oaKQgQyMx5oV5Mh3VNZu2M/178+jaWbM4OOJSKFgIpCBOtwQUVGPtSKmKgobnpzBt8u1ZhJInJ2VBQi3PmVvCuTzq9Uip4fzWfgxDUaM0lEzpiKQiGQVDKeTx5owfUNK/PSd6t4YsQiDh/VCWgROX0aUKeQKBYbTf+ujah9TiIvj1/Nz7sO8Fb3plRIjA86mohEEO0pFCJmRp/L6/B6t8Ys3ZxJp4HTWbVtX9CxRCSCqCgUQtc2qMyIB1vy29HjdBk0g4krNTSGiOSOikIh1TC5DF/1bk318gnc9/5c3p66TiegReR3qSgUYpVKF+ezni254oKK/GPsCp75fIlOQIvIKYXzHs3FzGyOmS0ys2Vm9qzfXsPMZpvZGjMbbmZxfnu8/3iNvzwlXNmKkoS4GAbd3pTel9ZmeNpGug2ZzY596gEtIjkL557CYeAy51xDoBHQ0cxaAC8ArzjnagO7gfv89e8Ddvvtr/jrSR6IijKevOo8Xu/WmOVb9nLD69NZvGlP0LFEpAAKW1Fwnv3+w1h/csBlwEi//X2gkz9/g/8Yf/nlZmbhylcUXdugMiMfakmUGTe/OZOvFm4OOpKIFDBhPadgZtFmthDYAYwH1gJ7nHNH/VU2AVX8+SrARgB/eSZQPodt9jCzNDNLy8jICGf8Qql+5dKM7t2ahsll6PvpQv45bgXHNAS3iPjCWhScc8ecc42AqkAzoF4ebHOwcy7VOZealJR01hmLovKJ8Xx0X3PuaFGNtyav477355J58EjQsUSkAMiXq4+cc3uAiUBLoIyZZfWkrgpkHcPYDCQD+MtLAzvzI19RFBcTxT86XcRzN17ItPRfuHHgdNZm7P/9J4pIoRbOq4+SzKyMP18cuAJYgVccbvJXuwv4yp8f7T/GX/6j04X1YXd78+oMe6AFmQeP0On16eroJlLEhXNPoRIw0cwWA3OB8c65McAfgcfNbA3eOYN3/PXfAcr77Y8Dz4Qxm4RoVqMcX/VuTXK5BO59fy6DJq1VRzeRIsoi+T9/amqqS0tLCzpGoXHwt2M8NXIRYxZv5fqGlXmhSwOKx0UHHUtE8piZzXPOpea0TD2aJVvxuGheu60xT111Hl8v3kLnQTPYsPNA0LFEJB+pKMh/MDN6XVqbd+++mM27D3Dd69OYtErnGUSKChUFydGl553D14+0oVLpYtwzdC6v/5jOcfVnECn0VBTkpKqXL8Goh1txfcPK/Ov71Tz40Tz2HlJ/BpHCTEVBTikhLoZXb23EX669gB9X7qDT69NJ364b94gUVioK8rvMjHvb1ODj+5uz99AROg2czrglW4OOJSJhoKIgudaiZnm+fqQNdc8tyUMfz+f5cSs1bpJIIaOiIKelUunifNqjBbc3r8abk9dy17tz2PXrb0HHEpE8oqIgpy0+JprnbryIF7s0YM76XVz32jSWbs4MOpaI5AEVBTljt1yczGcPtsQ5R+dBM/h0zgYNjyES4VQU5Kw0TC7D14+0oXmNcjwzaglPfraYg7/pPtAikUpFQc5a+cR4ht7TjL6X12HUgk3c+MZ01mkYbpGIpKIgeSI6ynjsiroMvacZ2/ce4vrXpzN2sS5bFYk0KgqSpy6pm8TYPm2pUzGRXsPm8+zXy/jt6PGgY4lILqkoSJ6rXKY4w3u05J7WKbw3fT23Dp7Jlj0Hg44lIrmgoiBhERcTxV+vq8/Abk1YvW0ffxgwlcmrM4KOJSK/Q0VBwuoPDSox+pE2nFOyGHe/N4dXxq9WL2iRAiyc92hONrOJZrbczJaZWV+/fbiZLfSn9Wa20G9PMbODIcveDFc2yV+1khL5sldrbmxchf4T0rn7vTns3H846FgikoOYMG77KPCEc26+mZUE5pnZeOfcrVkrmFk/ILQr7FrnXKMwZpKAFI+Lpt/NDWmWUo6/jF7GNQOmMqBrY5rXLB90NBEJEbY9BefcVufcfH9+H7ACqJK13MwMuAX4JFwZpGAxM7o2q8aoh1pRPDaa24bM4rUJ6TqcJFKA5Ms5BTNLARoDs0Oa2wLbnXPpIW01zGyBmU02s7Yn2VYPM0szs7SMDJ24jEQXVinNmD5tubZBZfqNX82d785mx75DQccSEfKhKJhZIvA58Khzbm/Iotv4z72ErUA151xj4HFgmJmVOnF7zrnBzrlU51xqUlJSOKNLGCXGx9C/ayNe6HIR837ezTX9pzEt/ZegY4kUeWEtCmYWi1cQPnbOjQppjwE6A8Oz2pxzh51zO/35ecBaoG4480mwzIxbL67GV73aUDYhlu7vzqbf96s4ekyd3USCEs6rjwx4B1jhnHv5hMUdgJXOuU0h6yeZWbQ/XxOoA6wLVz4pOM47tyRf9W7NzU2r8tqPa+g2ZDZbM9XZTSQI4dxTaA10By4Lucz0Gn9ZV/7vCeZ2wGL/EtWRQE/n3K4w5pMCJCEuhhdvasgrtzZk6ZZMruk/lYkrdwQdS6TIsUge/z41NdWlpaUFHUPy2NqM/fQetoAVW/fSo11NnrrqPGKj1c9SJK+Y2TznXGpOy/Q/TQqcWkmJfPFwK7q3qM7gKeu4+c2ZbNx1IOhYIkWCioIUSMVio/mfThfyxu1NWLtjP9cMmMrXi7YEHUuk0FNRkALtmosq8U3fttQ5J5FHPlnAk58tYv/ho0HHEim0VBSkwEsul8CIB1vS57LajJq/iWsHTGXRxj1BxxIplFQUJCLEREfx+JXn8ckDLfjt6HG6DJrBm5PXclxDZIjkKRUFiSjNa5ZnXN92XFm/Is+PW8kd78xm+14NkSGSV1QUJOKUTohlYLcmvNDlIhZs2EPHV6cwfvn2oGOJFAq5KgpmdnNu2kTyS9YQGWP6tKFymeI88EEa//3lUg4dORZ0NJGIlts9hT/lsk0kX9VKSmTUw614oG0NPpz1M9e9No0VW/f+/hNFJEenvMmOmV0NXANUMbMBIYtK4d1ERyRw8THR/NcfLqBtnSQeH7GIGwZO509X1+OulilERVnQ8UQiyu/tKWwB0oBDwLyQaTRwVXijiZyednWT+PbRtrSpXYFnv17OXe/NYVumTkKLnI5cjX1kZrHOuSP+fFkg2Tm3ONzhfo/GPpKcOOcYNmcD/xizgriYKJ678UKubVA56FgiBUZejH003sxKmVk5YD4wxMxeybOEInnIzLi9eXXG9mlDSoUS9B62gMeGLyTz4JGgo4kUeLktCqX9u6Z1Bj5wzjUHLg9fLJGzVzMpkc97tuTRDnUYvWgLV786hZlrdwYdS6RAy21RiDGzSsAtwJgw5hHJUzHRUTzaoS6fP9SK+Nhour09i+fGLufwUV26KpKT3BaFvwPfAWudc3P9O6Olhy+WSN5qlFyGsX3acHvzagyZ+hM3vD5dl66K5EA32ZEiZ+KqHTw9cjGZB47w5FV1ua9NTaJ16aoUIWd9otnMqprZF2a2w58+N7Oqv/OcZDObaGbLzWyZmfX12/9mZptzuEUnZvYnM1tjZqvMTJe8Slhcet45fPdoOy6tl8T/frOSbkNmsWm3buIjArk/fPQeXt+Eyv70td92KkeBJ5xzFwAtgF5mdoG/7BXnXCN/+gbAX9YVqA90BN4ws+jTejciuVSuRBxv3tGUl25qwLIte7n61amMmLuRSN5zFskLuS0KSc6595xzR/1pKJB0qic457Y65+b78/uAFUCVUzzlBuBT59xh59xPwBqgWS7ziZw2M+Pm1GTG9W1L/SqlePrzxdw7dK5GXZUiLbdFYaeZ3WFm0f50B5Dra/vMLAVoDMz2m3qb2WIze9fvDAdewdgY8rRN5FBEzKyHmaWZWVpGRkZuI4icVHK5BIbd34K/XXcBM9ft5MpXpvDlgs3aa5AiKbdF4V68y1G3AVuBm4C7c/NEM0sEPgce9fs6DAJqAY38bfU7ncDOucHOuVTnXGpS0il3VkRyLSrKuLt1Dcb1bUftcxJ5dPhCen40j4x9h4OOJpKvTueS1Lucc0nOuXPwisSzv/ckM4vFKwgfO+dGATjntjvnjjnnjgND+Pchos1AcsjTq/ptIvmmRoUSjHiwJX++ph4TV2Vw5SuTGbt4a9CxRPJNbotCA+fc7qwHzrldeIeDTsrMDHgHWOGcezmkvVLIajcCS/350UBXM4s3sxpAHWBOLvOJ5JnoKKNHu1qMfaQN1col0GvYfHoPm8+uX38LOppI2J1y6OwQUWZWNqsw+GMg/d5zWwPdgSVmttBv+zNwm5k1AhywHngQwDm3zMxGAMvxrlzq5ZxTt1MJTJ2KJfn8oVa8NWUdr/6wmlnrdvG/N17IlfXPDTqaSNjkdpTUO/H+oH/mN90MPOec+zCM2X6XOq9JflmxdS9PjFjE8q176dy4Cn+9rj6lE2KDjiVyRs6685pz7gO8wfC2+1PnoAuCSH46v1IpvuzVmj6X1+GrRVu48tXJTFih+0JL4aNhLkRO05JNmTw1chErt+3jhkaV+et19SlXIi7oWCK5lhf3UxAR30VVSzO6dxse7VCHb5Zs5YqXJzNm8Rb1a5BCQUVB5AzExXhDcn/9SBuqlC1O72ELePDDeexQb2iJcCoKImeh3rmlGPVQK/50dT0mr86gw8uT+SxNYyhJ5FJREDlLMdFRPHhJLcb1bct555bkqZGLueu9uRp5VSKSioJIHqmZlMjwHi159vr6pK3fxVWvTOHDmes5flx7DRI5VBRE8lBUlHFXqxS+e7QdTaqX5b+/WkbXIbP46Zdfg44mkisqCiJhkFwugQ/ubcaLXRqwYuteOr46hbcmr+XoseNBRxM5JRUFkTAxM265OJkfHr+EdnWT+Oe4lVz/+nQWb9oTdDSRk1JREAmziqWKMbh7U968owm/7D9Mp4HT+fvXy/n18NGgo4n8HyoKIvnAzOh4YSV+eOISujWvxrvTf+LKV6bw40oNlSEFi4qCSD4qVSyWf3S6iJE9W5IQF829Q9PoNWw+O/ap05sUDCoKIgFITSnH2D5teeKKuoxftp0O/SbzyZwNunxVAqeiIBKQuJgoHrm8DuMebcv5lUrxp1FL6Dp4Fmt27As6mhRhKgoiAauVlMinPVrwYpcGrNq+j2v6T+PVH1Zz+KjuMSX5T0VBpADIunx1whOX0PHCc3n1h3Su6T+VmWt3Bh1NipiwFQUzSzaziWa23MyWmVlfv/0lM1tpZovN7AszK+O3p5jZQTNb6E9vhiubSEFVITGeAbc1Zug9F3P46HFuGzKLx4YvJGPf4aCjSRERtpvsmFkloJJzbr6ZlQTmAZ2AqsCPzrmjZvYCgHPuj2aWAoxxzl2Y29fQTXakMDv42zEGTlzDW1PWUjw2mqc61qNbs2pER1nQ0STCBXKTHefcVufcfH9+H7ACqOKc+945l9VrZxZekRCRExSPi+bJq85jXN92XFilNP/95VI6vzGdJZsyg44mhVi+nFPw9wIaA7NPWHQvMC7kcQ0zW2Bmk82s7Um21cPM0swsLSMjIyx5RQqS2uck8vH9zenftRGb9xzihoHT+OtXS9l76EjQ0aQQCvs9ms0sEZgMPOecGxXS/l9AKtDZOefMLB5IdM7tNLOmwJdAfefc3pNtW4ePpKjJPHiEl79fxQezfqZ8iXj++9rzub5hZcx0SElyL7B7NJtZLPA58PEJBeFu4FrgdudXJefcYefcTn9+HrAWqBvOfCKRpnTxWJ694UJG92pD5TLF6PvpQu54ZzZrM/YHHU0KiXBefWTAO8AK59zLIe0dgaeB651zB0Lak8ws2p+vCdQB1oUrn0gku6hqab54uDX/0+lCFm/K5OpXp9Lv+1UcOqK+DXJ2wrmn0BroDlwWcpnpNcDrQElg/AmXnrYDFpvZQmAk0NM5tyuM+UQiWnSU0b1FdX58oj1/aFCJ135cQ4eXJ/P9sm26R7ScsbCfUwgnnVMQ+beZa3fy19FLWb19P+3qJvHX6y6gVlJi0LGkAArsnIKI5J+Wtcoztk9b/nLtBSz4eTcdX53CP8etYL/u2yCnQUVBpBCJjY7i3jY1+PHJ9nRqVIW3Jq/j8n6T+GrhZh1SklxRURAphJJKxvPSzQ0Z9XArzinpXaV061uzWLH1pFd4iwAqCiKFWpNqZfmyV2v+2fki0nfs4w8DpvLXr5aSeUAd3yRnKgoihVx0lHFbs2pMfLI9d7SozoezfubSfpP4VDf1kRyoKIgUEWUS4vj7DRcy5pG21EoqwTOjltDpjenM37A76GhSgKgoiBQxF1QuxYgHW/LqrY3YlnmIzm/M4LHhC9maeTDoaFIAqCiIFEFmRqfGVfjxyfb0urQWY5ds5bJ/Tab/D+kc/E29oosyFQWRIiwxPoanrqrHhMcv4bJ65/DKD6u5TJewFmkqCiJCcrkEBt7ehOE9WlCuRBx9P11Il0EzWLhxT9DRJJ+pKIhItuY1yzO6dxtevKkBG3cfpNPA6Tw+fCHbMg8FHU3yiYqCiPyH6CjjltRkJj7Znofb12LMkq1c+q9JDJig8w1FgYqCiOQoMT6Gpzt65xsurZfEy+NXc3m/SYxetEXnGwoxFQUROaXkcgm8cXtTPu3RgjIJcfT5ZAGdB81g3s8a2b4wUlEQkVxpUbM8Xz/Shhe7NGDz7oN0GTSThz6ax/pffg06muShmKADiEjkiI4ybrk4mWsbVmLIlJ94a8paxi/fzh0tqtPn8jqUKxEXdEQ5S7rJjoicsR37DvHK+HSGz91AifgYel1am7tbpVAsNjroaHIKgdxkx8ySzWyimS03s2Vm1tdvL2dm480s3f+3rN9uZjbAzNaY2WIzaxKubCKSN84pWYx/dr6I7x5tR7OUcjw/biWX95vMlws2a7C9CBXOcwpHgSeccxcALYBeZnYB8AwwwTlXB5jgPwa4GqjjTz2AQWHMJiJ5qE7Fkrxz98UMu785ZRJieXT4Qq4fOI0Za38JOpqcprAVBefcVufcfH9+H7ACqALcALzvr/Y+0MmfvwH4wHlmAWXMrFK48olI3mtVuwJf927DK7c2ZNf+3+g2ZDb3DZ3Lmh37go4muZQvVx+ZWQrQGJgNVHTObfUXbQMq+vNVgI0hT9vkt524rR5mlmZmaRkZGWHLLCJnJirKuLFxVX58sj1/7FiPOT/t4qpXp/LnL5awfa96Rhd0YS8KZpYIfA486pz7j3sBOu8s92kdeHTODXbOpTrnUpOSkvIwqYjkpWKx0TzUvhaTn76U7i2q81naRi55aSIvfLuSzIO681tBFdaiYGaxeAXhY+fcKL95e9ZhIf/fHX77ZiA55OlV/TYRiWDlSsTxt+vrM+Hx9lxV/1wGTVpLuxcn8tbktRw6omEzCppwXn1kwDvACufcyyGLRgN3+fN3AV+FtN/pX4XUAsgMOcwkIhGuWvkE+ndtzNg+bWhcrQz/HLeS9i95twU9eux40PHEF7Z+CmbWBpgKLAGyfuN/xjuvMAKoBvwM3OKc2+UXkdeBjsAB4B7n3Ck7IaifgkjkmrVuJy98u5IFG/ZQK6kET111HlfVPxfvT4GE06n6KajzmogExjnH98u389J3q1izYz8Nk8vwx47n0apWhaCjFWqBdF4TEfk9ZsZV9c/l275tefGmBmTsPUS3IbPp/s5slm7ODDpekaQ9BREpMA4dOcZHs37m9Ylr2HPgCNc1rMxjHepQMykx6GiFig4fiUhE2XvoCIMnr+OdaT/x27HjdG5chT6X1yG5XELQ0QoFFQURiUgZ+w4zaNJaPpr9M845br04md6X1uHc0sWCjhbRVBREJKJtzTzIwIlr+HTORqKijO4tqvNQ+1pUSIwPOlpEUlEQkUJh464DDJiQzufzNxEfE809rVPo0a4mZRJ0H4fToaIgIoXK2oz99P8hna8XbyExLob72tbgvjY1KFksNuhoEUFFQUQKpZXb9vLK+NV8t2w7ZRJiebBdLe5qVZ2EON1U8lRUFESkUFu8aQ8vj1/NpFUZVEiM46H2tbm9eTXdAe4kVBREpEhIW7+Lft+vZua6nSSVjOfBdjW5vXl1isepOIRSURCRImXWup30/yGdmet2UiExnp6XqDiEUlEQkSJp9rqd9J+Qzoy1O6mQGEePdjW5o4XOOagoiEiRNnf9Lvr/kM60Nb9QvoRXHLq3LLrFQUVBRATvnEP/CelMTfeKwwPtatK9RXVKxBet4qCiICISYt7Pu+k/IZ0pqzMoVyKO+9vW4IMy5McAAA3eSURBVM6WKSQWkeKgoiAikoP5G3bT/4d0Jq/OoGxCLPe1qUH3limULl64O8GpKIiInMKCDbsZMCGdiasyKBkfw52tqnNv6xqUL6RjKwVSFMzsXeBaYIdz7kK/bThwnr9KGWCPc66RmaUAK4BV/rJZzrmev/caKgoikpeWbs7kjUlrGLd0G/ExUdzWrBo92tWkUuniQUfLU0EVhXbAfuCDrKJwwvJ+QKZz7u9+URiT03qnoqIgIuGwZsc+Bk1ax5cLNxNl0KVJVXpeUouUCiWCjpYnArkdp3NuCrDrJIEMuAX4JFyvLyJypmqfU5J+tzRk0pPt6XpxNUYt2Mxl/SbR99MFrNq2L+h4YRXUPZrbAtudc+khbTXMbIGZTTaztid7opn1MLM0M0vLyMgIf1IRKbKSyyXwP50uZNrTl3J/25qMX76dq16dwgMfpLFw456g44VFWE80n+ywkJkNAtY45/r5j+OBROfcTjNrCnwJ1HfO7T3V9nX4SETy0+5ff2PojPUMnbGezINHaFO7Ar0urU2LmuXwDoBEhkAOH50iTAzQGRie1eacO+yc2+nPzwPWAnXzO5uIyKmULRHHY1fUZfozl/HM1fVYuW0ftw2ZRedBM/hu2TaOH4/cqzmzBHH4qAOw0jm3KavBzJLMLNqfrwnUAdYFkE1E5HclxsfQ85JaTPvjpfz9hvpk7DvMgx/Oo8Mrk/l0zgYOHz0WdMQzFraiYGafADOB88xsk5nd5y/qyv89wdwOWGxmC4GRQE/nXI4nqUVECopisdHc2TKFSU+2Z8BtjSkeG80zo5bQ5oWJDJq0lsyDR4KOeNrUeU1EJI8455i+ZidvTVnL1PRfSIyPoVvzatzbugbnli4WdLxs6tEsIpLPlm7O5K0p6xi7eAvRUUanRlXo0a4mdSqWDDqaioKISFA27jrAkKnrGJG2kUNHjtPh/HN48JJaXJxSLrBMKgoiIgHbuf8wH8z8mQ9mrmf3gSM0rV6WHu1q0uH8ikRH5e/lrCoKIiIFxIHfjvJZ2iaGTF3Hpt0HqV4+gXtb1+CmplXz7b4OKgoiIgXM0WPH+W7ZdoZMXcfCjXsoVSyGbs2rc1er6mEfgE9FQUSkAJv3827embaOb5duI8qMPzSoxP1tanJR1dJheb1TFYWicZshEZECrGn1sjSt3pSNuw7w3vT1DJ+7ga8WbqFZjXLc36YGl+fjeQftKYiIFDB7Dx1hxNyNvDd9PZv3HCSlfAL3tvHOOyTEnf13eR0+EhGJQEePHefbZdsYMvUnFm3cQ+nisdzWrNpZn3dQURARiWDOOeZv2M3bU3/iu2XeeYe7W6Xw/6694Iy2p3MKIiIRzMxoWr0cTauXyz7vULVseK5QUlEQEYkgyeUS+Mt1Z7aHkBtB3XlNREQKIBUFERHJpqIgIiLZVBRERCSbioKIiGRTURARkWwqCiIikk1FQUREskX0MBdmlgH8fBabqAD8kkdx8pJynR7lOj3KdXoKY67qzrmknBZEdFE4W2aWdrLxP4KkXKdHuU6Pcp2eopZLh49ERCSbioKIiGQr6kVhcNABTkK5To9ynR7lOj1FKleRPqcgIiL/qajvKYiISAgVBRERyVYki4KZdTSzVWa2xsyeyefXftfMdpjZ0pC2cmY23szS/X/L+u1mZgP8nIvNrEkYcyWb2UQzW25my8ysb0HIZmbFzGyOmS3ycz3rt9cws9n+6w83szi/Pd5/vMZfnhKOXCH5os1sgZmNKSi5zGy9mS0xs4Vmlua3FYTPWBkzG2lmK81shZm1DDqXmZ3n/5yypr1m9mjQufzXesz/zC81s0/8/wvh/3w554rUBEQDa4GaQBywCLggH1+/HdAEWBrS9iLwjD//DPCCP38NMA4woAUwO4y5KgFN/PmSwGrggqCz+dtP9Odjgdn+640AuvrtbwIP+fMPA2/6812B4WH+fT4ODAPG+I8DzwWsByqc0FYQPmPvA/f783FAmYKQKyRfNLANqB50LqAK8BNQPORzdXd+fL7C+kMuiBPQEvgu5PGfgD/lc4YU/rMorAIq+fOVgFX+/FvAbTmtlw8ZvwKuKEjZgARgPtAcrydnzIm/U+A7oKU/H+OvZ2HKUxWYAFwGjPH/UBSEXOv5v0Uh0N8jUNr/I2cFKdcJWa4EpheEXHhFYSNQzv+8jAGuyo/PV1E8fJT1w86yyW8LUkXn3FZ/fhtQ0Z8PJKu/69kY71t54Nn8QzQLgR3AeLw9vT3OuaM5vHZ2Ln95JlA+HLmAV4GngeP+4/IFJJcDvjezeWbWw28L+vdYA8gA3vMPt71tZiUKQK5QXYFP/PlAcznnNgP/AjYAW/E+L/PIh89XUSwKBZrzSn1g1wmbWSLwOfCoc25v6LKgsjnnjjnnGuF9M28G1MvvDCcys2uBHc65eUFnyUEb51wT4Gqgl5m1C10Y0O8xBu+w6SDnXGPgV7zDMkHnAsA/Nn898NmJy4LI5Z/DuAGvmFYGSgAd8+O1i2JR2Awkhzyu6rcFabuZVQLw/93ht+drVjOLxSsIHzvnRhWkbADOuT3ARLzd5jJmFpPDa2fn8peXBnaGIU5r4HozWw98incIqX8ByJX1LRPn3A7gC7xCGvTvcROwyTk32388Eq9IBJ0ry9XAfOfcdv9x0Lk6AD855zKcc0eAUXifubB/vopiUZgL1PHP4sfh7TKODjjTaOAuf/4uvOP5We13+lc8tAAyQ3Zp85SZGfAOsMI593JByWZmSWZWxp8vjneeYwVecbjpJLmy8t4E/Oh/08tTzrk/OeeqOudS8D5DPzrnbg86l5mVMLOSWfN4x8mXEvDv0Tm3DdhoZuf5TZcDy4POFeI2/n3oKOv1g8y1AWhhZgn+/82sn1f4P1/hPHFTUCe8KwhW4x2b/q98fu1P8I4RHsH79nQf3rG/CUA68ANQzl/XgIF+ziVAahhztcHbRV4MLPSna4LOBjQAFvi5lgJ/8dtrAnOANXi7/PF+ezH/8Rp/ec18+J22599XHwWay3/9Rf60LOvzHfTv0X+tRkCa/7v8EihbQHKVwPtWXTqkrSDkehZY6X/uPwTi8+PzpWEuREQkW1E8fCQiIiehoiAiItlUFEREJJuKgoiIZFNREBGRbCoKEnZmNsP/N8XMuuXxtv+c02uFi5l1MrO/hGnb+8O03fbmj+J6FttYb2YVTrH8UzOrczavIQWDioKEnXOulT+bApxWUQjpvXky/1EUQl4rXJ4G3jjbjeTifYVdHmcYhPezkQinoiBhF/IN+HmgrT9u/WP+QHcvmdlcf2z6B/3125vZVDMbjdeLEzP70h/gbVnWIG9m9jxQ3N/ex6Gv5fc4fckfi36Jmd0asu1J9u9x/T/2e4xiZs+bdz+JxWb2rxzeR13gsHPuF//xUDN708zSzGy1Px5S1gB+uXpfObzGc+bdO2KWmVUMeZ2bQtbZH7K9k72Xjn7bfKBzyHP/ZmYfmtl04EO/x/jnfta5ZtbaX6+8mX3v/7zfxuu0ldVjeqyfcWnWzxWYCnQoCMVOzlK4euNp0pQ1Afv9f9vj9/z1H/cA/p8/H4/X27WGv96vQI2QdbN6lBbH6+FZPnTbObxWF7wRVaPxRrjcgDcEcnu8ESSr4n0pmonXm7s83jDIWR06y+TwPu4B+oU8Hgp862+nDl4P9WKn875O2L4DrvPnXwzZxlDgppP8PHN6L8XwRsysg/fHfAT/7nH9N7zRNrPG6R+GN4AeQDW8YU4ABvDv3uN/8LNV8H+uQ0KyhPYCHg80DfrzpunsJu0pSJCuxBtHZiHeMN3l8f6QAcxxzv0Usm4fM1sEzMIb+Ov3jl+3AT5x3gir24HJwMUh297knDuON5xHCt4f10PAO2bWGTiQwzYr4Q3/HGqEc+64cy4dWIc3guvpvK9Qv+GNmw/eH+6U33mPJ3sv9fAGU0t33l/rj054zmjn3EF/vgPwup91NFDKvJFy22U9zzk3Ftjtr78EuMLMXjCzts65zJDt7sAb0VMimHb1JEgGPOKc++4/Gs3a432jDn3cAe8mIgfMbBLet+EzdThk/hjeTUuOmlkzvIHHbgJ64418Guog3uiToU4cJ8aRy/eVgyP+H/HsXP78UfxDvWYWhXfXspO+l1NsP0tohiighXPu0AlZc3yic261ebegvAb4h5lNcM793V9cDO9nJBFMewqSn/bh3eozy3fAQ+YN2Y2Z1TVvZM8TlQZ2+wWhHt5tELMcyXr+CaYCt/rH95PwvvnOOVkw/9txaefcN8BjQMMcVlsB1D6h7WYzizKzWniDla06jfeVW+uBpv789Xi3JT2VlUCKnwm8EUBP5nvgkawHZtbIn52Cf1GAmV2NN3gdZlYZOOCc+wh4CW/46yx18Q7tSQTTnoLkp8XAMf8w0FC8+w+kAPP9E6QZQKccnvct0NPMVuD90Z0VsmwwsNjM5jtv6OosX+Ddd2ER3rf3p51z2/yikpOSwFdmVgzvm/7jOawzBehnZhbyjX4DXrEpBfR0zh3yT8zm5n3l1hA/2yK8n8Wp9jbwM/QAxprZAbwCWfIkq/cBBprZYry/B1OAnngjdH5iZsuAGf77BLgIeMnMjuON9PsQgH9S/KDzhsiWCKZRUkVOg5n1B752zv1gZkPxTuCODDhW4MzsMWCvc+6doLPI2dHhI5HT879AQtAhCqA9wPtBh5Czpz0FERHJpj0FERHJpqIgIiLZVBRERCSbioKIiGRTURARkWz/H96doUNsL0luAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 800, loss: 159.539874 \n",
            "guess_sentences:\n",
            "That Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many Thrat Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many Thrat Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many Tho thee  hundred years ago, was certaisly not the last of her kind.\n",
            "Many Thrat Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many The three hundred years ago, was certainly not the last of her kind.\n",
            "Many Thret Spanish woman who lived three hundred years ago, was certainly not the last of her kind.\n",
            "Many Thh threinh horred wof her kind.\n",
            "Many Tha therdsh kind.\n",
            "Many Thr theee hundred years ago, was certainly not the last of her kind.\n",
            "Many Tho thee hundred years ago, was certainly not the last of her kind.\n",
            "Many Tho the last of her kind.\n",
            "Many Thrat Spaniihh what Sainly not the last of her kind.\n",
            "Many Tho liv\n"
          ]
        }
      ]
    }
  ]
}